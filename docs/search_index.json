[["index.html", "Telfer 2020 Directed Reading: Natural Language Processing Chapter 1 Preface", " Telfer 2020 Directed Reading: Natural Language Processing Christopher Belanger 2020-12-10 Chapter 1 Preface This is a work product for the Telfer School of Management course MBA6292, Directed Readings in Natural Language Processing with Dr. Peter Rabinovitch. This document is intended as a final-work-in-progress: I will continue to revise and add through it during the semester, with the intention that at the end of the semester it will stand as a final product. This document is written in RMarkdown using RStudio and the bookdown package. For details and to learn how to create your own, see Xie (2020). References "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Placeholder for introduction text. Ill write this last along with the Conclusion. This product uses the bookdown package (Xie 2020), which was built on top of R Markdown and knitr (Xie 2015). References "],["web-scraping-our-data.html", "Chapter 3 Web Scraping Our Data 3.1 Foreword: Ethics 3.2 The General Idea 3.3 Goodreads: CSS Selectors 3.4 Yelp: Embedded JSON 3.5 MEC: Reverse-Engineering Client-Side API Calls 3.6 Summary", " Chapter 3 Web Scraping Our Data In this section I will discuss ethical concerns related to web scraping and describe three different approaches I used to scrape datasets from Yelp, Goodreads, and MEC. 3.1 Foreword: Ethics I this section I will briefly consider some ethical aspects of web scraping. Although its a rich topic, my treatment here will be superficial and my conclusion will be that this project is fine. What is web scraping? As a working definition, lets say that web scrapingwhich can also be called crawling, trawling, indexing, harvesting, or any number of other termsmeans automatically visiting websites to collect and store information. So at one extreme, browsing Facebook at work doesnt count since its not automatic. On the other extreme, automatically sending billions of requests to a server in an attempt to overload (i.e. a DDoS attack) doesnt count either, since nothing is being done with the information the server sends back. Why might web scraping be wrong? Here Ill consider three potential objections based on access, burdening the scrapee, and purpose, and show how Ive designed this project to mitigate those concerns. Web scraping might be wrong if were taking things were not supposed to have access to. For example, if data were held on a password-protected server, one might think it wrong to collect it all automatically and re-create that dataset elsewhere. To mitigate this concern, we will only scrape publicly accessible data. Web scraping might be wrong if it posed an undue burden on the sites were scraping. For example, if we were to scrape millions of pages or records from a single site in a short time, it might overload their servers or disrupt other peoples access. To mitigate this concern, we can scrape a smallish number of pages and spread our requests out so that we dont overload any servers. Web scraping might be wrong if we were to use the data we collect unethically. As an example, one might think it would be unethical to scrape data and use it for political or financial purposes. To mitigate this concern, we will only use the data we collect for non-commercial educational purposes. Note also that web scraping is an extremely common business model. To take an obvious example, Googles entire search business is based on information it has extracted from websitesin other words, web scraping (Google 2020). Beyond Google, news agencies report that between 30% and 50% of all web traffic may be from automated web-scraping bots (Bruell 2018, @lafrance_internet_2017). And programming languages, including R, ship with packages that make web scraping relatively easy (e.g. Wickham 2020). So we can say at least that in some cases, web scraping on a massive scale is a commonly accepted business practice. In summary, in this project Ive made the following choices to mitigate ethical concerns about web scraping: Were only scraping publicly accessible information; Were scraping a reasonably small number of pages/reviews; Were being considerate of their servers by spacing out our requests; and, Were collecting and using the data for educational non-commercial purposes. 3.2 The General Idea Web scraping these sites follows a two-step process: Get a list of urls for pages you want to scrape (generating an index). Usually well get these urls by first scraping another page. Use a loop to scrape the information from each page (loading the content). Since different sites have different structures, well need custom code for the index and content pages. Also, by random chance these three sites all use different web-design principles, so well also need to use different techniques. 3.3 Goodreads: CSS Selectors Goodreads describes itself as the worlds largest site for readers and book recommendations ((???)). Registered users can leave reviews and ratings for books, any anyone can use the site to browse user-submitted reviews and a variety of information about books. Goodreads pages are standard html, so we can use css selectors to isolate the exact parts of the page were interested in. I used Rs rvest package, and the package documentation has details about the methods and about css selectors in general (???). To find the css selectors I used SelectorGadget, a point-and-click Chrome extension. 3.3.1 Scraping the Index Goodreads assigns books to genres like sci-fi and romance, and curates lists of each genres 100 most-read books in the past week. By scraping these pages, we can get links to content pages for hundreds of books across different genres. Here is a code block to get the links to the 100 most-read books in the classics genre. The code could be functionized or run several times for other genres. library(tidyverse) library(rvest) # choose a genre genre &lt;- &quot;classics&quot; url &lt;- paste0(&quot;https://www.goodreads.com/genres/most_read/&quot;,genre) # read the page page &lt;- read_html(url) # extract the links to each book&#39;s page using css selectors book_links &lt;- page %&gt;% html_nodes(&quot;.coverWrapper&quot;) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% paste0(&quot;https://www.goodreads.com&quot;, .) book_links %&gt;% as_tibble() %&gt;% write_csv(paste0(&quot;book_links_&quot;,genre,&quot;.csv&quot;)) 3.3.2 Scraping the Content The next step is to load each content link and extract the information about the book, its author, and all the reviews. Books can have several pages of reviews, so we need to figure out how mayn pages there are and how to crawl through them. Since not all reviews have both text and star ratings, we also need to be careful to make sure we handle missing data appropriately. #https://www.goodreads.com/genres/most_read/non-fiction links &lt;- book_links # set up empty results tibble results &lt;- tibble() # remove any links we&#39;ve already seen, if we crashed and are resuming links &lt;- links[!links %in% results$url] for (i in 1:length(links)) { # pause briefly pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,length(links),&quot;: &quot;, url)) # choose a random useragent each time we load the page -- anti-anti-scraping measure httr::user_agent(random_useragent()) %&gt;% httr::set_config() # read the url page &lt;- read_html(url) # read the review page&#39;s html reviews_html &lt;- page %&gt;% html_nodes(&quot;.review&quot;) # extract the informaiton we&#39;re interested in book_title &lt;- page %&gt;% html_nodes(&quot;#bookTitle&quot;) %&gt;% html_text() %&gt;% stringr::str_trim() author_name &lt;- page %&gt;% html_nodes(&quot;.authorName span&quot;) %&gt;% html_text() %&gt;% head(1) review_names &lt;- purrr::map_chr(reviews_html, function(x) { html_nodes(x, &quot;.user&quot;) %&gt;% html_text() }) review_dates &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.reviewDate&quot;) %&gt;% html_text()}) review_text &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.readable span&quot;) %&gt;% html_text() %&gt;% paste0(., &quot; &quot;) %&gt;% na_if(y=&quot; &quot;) %&gt;% str_trim() %&gt;% tail(1)}) review_rating &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.staticStars&quot;) %&gt;% html_text() %&gt;% paste0(., &quot; &quot;) %&gt;% na_if(y=&quot; &quot;) %&gt;% str_trim()}) # how many pages of reviews? # there may be an easier way but this should work num_pages &lt;- page %&gt;% html_text() %&gt;% str_extract_all(&quot;(?&lt;=previous).*?(?=next)&quot;) %&gt;% unlist() %&gt;% tail(1) %&gt;% stringr::str_trim() %&gt;% stringr::str_split(&quot; &quot;) %&gt;% unlist() %&gt;% map_dbl(as.double) %&gt;% max() # put it all together page_reviews &lt;- tibble( book_title = book_title, author_name = author_name, comment = review_text, names = review_names, rating = review_rating, dates = lubridate::mdy(review_dates), url = url, num_pages = num_pages ) results &lt;- bind_rows(results, page_reviews) } filename &lt;- paste0(&quot;goodreads_&quot;,genre,&quot;_reviews.csv&quot;) results %&gt;% write_csv(path = filename) 3.4 Yelp: Embedded JSON Yelp, according to its website, connects people with great local businesses (???). Businesses can upload information like their location, hours, and services, and registered users can can leave reviews with text, star ratings, and pictures. Yelps web design includes structured json data within its html. In other words, each Yelp review page has machine-readable data hidden inside it if you know where to look. Well exploit this by using a regular expression to extract the json from the html, then parse the json and work with it directly. 3.4.1 Scraping the Index First well get the urls for each restaurant in Ottawa. We start at the base url for Ottawa restaurants and iterate through all of the pages: we could get the page numbers automatically, but here I just saw that there are 24 and hard-coded that number in. We extract the urls from the json in the page without parsing it using a regex. We could have parsed the json and done it using structured data, but since were only looking for one value type this was faster and worked fine. # the base url for restaurants in Ottawa baseurl &lt;- &quot;https://www.yelp.ca/search?cflt=restaurants&amp;find_loc=Ottawa%2C%20Ontario%2C%20CA&quot; # an empty tibble for our links links &lt;- tibble() # loop through all 24 pages of Ottawa restaurants. (The number 24 was hard-coded to keep things moving.) for (pagenum in 1:24){ Sys.sleep(1) # get the url for the page we&#39;re loading url &lt;- paste0(baseurl, if(pagenum&gt;1){ paste0(&quot;&amp;start=&quot;,(pagenum-1)*10) }) # load the html for the page and print an update message text &lt;- read_html(url) %&gt;% html_text() message(&quot;**PAGE &quot;,pagenum,&quot;: &quot;, url) # extract the urls using a straight regex based on the json value key. we&#39;re not parsing any json here. urls &lt;- text %&gt;% str_extract_all(&#39;(?&lt;=businessUrl&quot;:&quot;)(.*?)(?=&quot;)&#39;) %&gt;% unlist() %&gt;% enframe() %&gt;% select(-name) %&gt;% filter (!str_detect(value, &quot;ad_business_id&quot;)) %&gt;% distinct() %&gt;% transmute(url = paste0(&quot;http://www.yelp.ca&quot;, value)) # add to our results links &lt;- bind_rows(links, urls) } links %&gt;% write_csv(&quot;yelp_ottawa_links.csv&quot;) 3.4.2 Scraping the Content Scraping the content has two steps. First, now that we have a list of content urls, we can load each in turn and extract the reviews and the links for any additional review pages for this business. In the second step well load these new links and get those reviews. This function loads a single review page, extracts the machine-readable json using a regex, parses the json, and extracts the information were interested in. It then returns that information in a tibble. get_review &lt;- function(page, url) { # get the html text &lt;- page %&gt;% html_text() # extract the json with the review data json_text &lt;- text %&gt;% str_extract(&#39;(?&lt;=&quot;reviewFeedQueryProps&quot;:)(.*)(&quot;query&quot;:&quot;&quot;\\\\}\\\\})&#39;) # set our review_page results variable to NA, in case we don&#39;t get a results review_page &lt;- NA # make sure we have valid json text before we try to parse it if (!is.na(json_text)){ # parse the json json_parse &lt;- json_text %&gt;% jsonlite::fromJSON() # pull out the variables we&#39;re interested in review_text &lt;- json_parse$reviews$comment$text review_rating &lt;- json_parse$reviews$rating review_name &lt;- json_parse$reviews$user$markupDisplayName review_date &lt;- json_parse$reviews$localizedDate review_business &lt;- json_parse$reviews$business$name review_url &lt;- rep(url, length(review_text)) # put them all into a tibble review_page &lt;- tibble(business = review_business, name = review_name, date = review_date, comment = review_text, rating = review_rating, url = review_url) } # return either NA or a results tibble return (review_page) } # simple function to pause for a random period of time pause &lt;- function(min_wait = 1, max_wait = 3){ runif(n=1, min=min_wait, max = max_wait) %&gt;% Sys.sleep() } We then proceed with step one, loading the initial list of links, extracting the reviews there, and collecting any more links to more reviews: # load our set of restaurant page links base_links &lt;- read_csv(&quot;yelp_ottawa_links.csv&quot;) # set up an empty tibble for our reviews reviews &lt;- tibble() # set up an empty tibble for the links we&#39;re going to visit later more_links &lt;- tribble(~links) # now we&#39;re going to visit each page, extract the reviews from it, and find out how many *more* pages there are for this restaurant. # we&#39;ll keep track of those other pages and visit them later in a random order. for (i in 1:nrow(base_links)) { # pause briefly pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,nrow(base_links),&quot;: &quot;, url)) # read the url page &lt;- read_html(url) # extract the reviews from the page review_page &lt;- get_review(page, url) # add these reviews to our list of reviews reviews &lt;- bind_rows(reviews, review_page) # now find out how many other pages there are for this restaurant # we&#39;ll regex to find the second half of &quot;dd of dd&quot;, where d is a digit (and it could be either one or two digits--see the regex below) num_pages &lt;- page %&gt;% html_node((&quot;.text-align--center__373c0__2n2yQ .text-align--left__373c0__2XGa-&quot;)) %&gt;% html_text() %&gt;% str_extract(&quot;(?&lt;=of )(\\\\d\\\\d?)&quot;) %&gt;% as.integer() # make sure we don&#39;t get an NA if (is.na(num_pages)) num_pages &lt;- 1 # if there&#39;s more than one page, construct the links and add them to our list of links to read next if (num_pages &gt; 1) { more_links &lt;- more_links %&gt;% add_row(links = paste0(url, &quot;?start=&quot;,(1:(num_pages-1))*20) ) } } # end for i in 1:nrow(base_links) # save our results reviews %&gt;% write_csv(&quot;data/ottawa-reviews-1.csv&quot;) more_links %&gt;% write_csv(&quot;data/ottawa_more_links.csv&quot;) In step two, well repeat the process for the new links we collected: Now lets do the same thing for the extra links we got: note its stopping me every 136 or so and giving a 503 error, so im either rebooting my modem to get a new ip address or tethering to my phone for a bit links &lt;- more_links for (i in 1:length(links)) { # pause briefly for random interval pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,length(links),&quot;: &quot;, url)) message(&quot; Loading page.&quot;) # read the url page &lt;- read_html(url) message(&quot; Parsing review.&quot;) # extract the reviews from the page review_page &lt;- get_review(page, url) if (!is.na(review_page)){ message (&quot; Adding to inventory.&quot;) # add these reviews to our list of reviews reviews &lt;- bind_rows(reviews, review_page) } else { message (&quot; No valid json found.&quot;) } } # end for i in 1:nrow(base_links) reviews %&gt;% write_csv(&quot;ottawa-reviews-2.csv&quot;) 3.5 MEC: Reverse-Engineering Client-Side API Calls MECs website uses a completely different design principle that makes it seem more difficult to extract information. If you inspect the html for one of MECs product pages, youll find that the review information simply isnt there! Its quite mysterious. The secret is that MECs site uses client-side API calls to download the data which is then displayed locally. To solve this puzzle, I needed to use Chromes developer console (opened with Control-Shift-J) to see the network activity (under the Network tab) happening each time I loaded a new product page. I discovered that my browser was making API calls to a specific server, and by comparing the calls for a few products I found that the main difference was the product ID. This let me reverse-engineer the syntax just enough to be able to call it myself and get reviews for any product based on its ID. I also found that there was one API call for the first page of reviews and a different one for loading more reviews, so I built functions for both of them. As a result, the index in this case is a list of product IDs rather than urls, and the content is the result of API calls rather than web pages. However, the principles remain the same. 3.5.1 Scraping the Index This code block collects product IDs for mittens and gloves. Each product category has a different catalogue page, so I modified the code to load a few different kinds of products. We load the first page, use a regex to figure out how many pages there are, then use css selectors to extract the IDs for products with reviews. # enter the base url by hand base_url &lt;- &quot;https://www.mec.ca/en/products/clothing/clothing-accessories/gloves-and-mittens/c/987&quot; # enter the product type by hand product_type &lt;- &quot;gloves-and-mittens&quot; # read the page page &lt;- read_html(base_url) # get the number of items using a CSS selector and a regex # we expect to find between one and three digits num_items &lt;- page %&gt;% html_nodes(&quot;.qa-filter-group__count&quot;) %&gt;% html_text() %&gt;% str_extract(&quot;(\\\\d\\\\d?\\\\d?)&quot;) %&gt;% as.integer() # there are at most 36 items per page num_pages &lt;- (num_items / 36) %&gt;% ceiling() # first let&#39;s do the items on this page # find each link to a product, filter out any that don&#39;t have reviews yet, extract the product ids product_ids &lt;- page %&gt;% html_nodes(&quot;.rating__count__link&quot;) %&gt;% html_attrs() %&gt;% enframe() %&gt;% unnest_wider(value) %&gt;% filter(!str_detect(title, &quot;No reviews yet&quot;)) %&gt;% mutate(product_id = str_extract(href, &quot;\\\\d\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d&quot;)) %&gt;% select(-name, -class) # now we load the extra pages, if there are any if (num_pages &gt; 1) { # we iterate from 1 to num_pages-1, because MEC calls the first extra page page 1 for (i in 1:(num_pages-1)){ # send an update to the console message(paste0(i,&quot;/&quot;,(num_pages-1))) # wait a little bit patience(min_wait = 3, max_wait = 10) # get the new url for the next page url &lt;- paste0(base_url,&quot;?page=&quot;,i) # load the next page page &lt;- read_html(base_url) # find each link to a product, filter out any that don&#39;t have reviews yet, extract the product ids new_product_ids &lt;- page %&gt;% html_nodes(&quot;.rating__count__link&quot;) %&gt;% html_attrs() %&gt;% enframe() %&gt;% unnest_wider(value) %&gt;% filter(!str_detect(title, &quot;No reviews yet&quot;)) %&gt;% mutate(product_id = str_extract(href, &quot;\\\\d\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d&quot;)) %&gt;% select(-name, -class) # add it to our list product_ids &lt;- bind_rows(product_ids, new_product_ids) } # end for (i in 1:(num_pages-1)) } # end if (num_pages &gt;1) product_ids %&gt;% write_csv(paste0(&quot;data/product_ids_&quot;,product_type,&quot;.csv&quot;)) 3.5.2 Functions for API Calls Next, I defined functions to make the API calls and to process their results. The API calls are quite uglyI could have spent more time figuring out exactly how they worked and slimmed them down, but this worked. get_first_api_url &lt;- function(product_code){ api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=products&amp;filter.q0=id%3Aeq%3A&quot;,product_code,&quot;&amp;stats.q0=questions%2Creviews&amp;filteredstats.q0=questions%2Creviews&amp;filter_questions.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_answers.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;resource.q1=questions&amp;filter.q1=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q1=totalanswercount%3Adesc&amp;stats.q1=questions&amp;filteredstats.q1=questions&amp;include.q1=authors%2Cproducts%2Canswers&amp;filter_questions.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_answers.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q1=10&amp;offset.q1=0&amp;limit_answers.q1=10&amp;resource.q2=reviews&amp;filter.q2=isratingsonly%3Aeq%3Afalse&amp;filter.q2=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q2=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q2=reviews&amp;filteredstats.q2=reviews&amp;include.q2=authors%2Cproducts%2Ccomments&amp;filter_reviews.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q2=8&amp;offset.q2=0&amp;limit_comments.q2=3&amp;resource.q3=reviews&amp;filter.q3=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q3=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q3=1&amp;resource.q4=reviews&amp;filter.q4=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q4=isratingsonly%3Aeq%3Afalse&amp;filter.q4=issyndicated%3Aeq%3Afalse&amp;filter.q4=rating%3Agt%3A3&amp;filter.q4=totalpositivefeedbackcount%3Agte%3A3&amp;filter.q4=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q4=totalpositivefeedbackcount%3Adesc&amp;include.q4=authors%2Creviews%2Cproducts&amp;filter_reviews.q4=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q4=1&amp;resource.q5=reviews&amp;filter.q5=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q5=isratingsonly%3Aeq%3Afalse&amp;filter.q5=issyndicated%3Aeq%3Afalse&amp;filter.q5=rating%3Alte%3A3&amp;filter.q5=totalpositivefeedbackcount%3Agte%3A3&amp;filter.q5=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q5=totalpositivefeedbackcount%3Adesc&amp;include.q5=authors%2Creviews%2Cproducts&amp;filter_reviews.q5=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q5=1&amp;callback=BV._internal.dataHandler0&quot;) return(api_url) } get_second_api_url &lt;- function(product_code){ api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=reviews&amp;filter.q0=isratingsonly%3Aeq%3Afalse&amp;filter.q0=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q0=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q0=reviews&amp;filteredstats.q0=reviews&amp;include.q0=authors%2Cproducts%2Ccomments&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q0=30&amp;offset.q0=8&amp;limit_comments.q0=3&amp;callback=bv_351_44883&quot;) #api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=reviews&amp;filter.q0=isratingsonly%3Aeq%3Afalse&amp;filter.q0=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q0=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q0=reviews&amp;filteredstats.q0=reviews&amp;include.q0=authors%2Cproducts%2Ccomments&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q0=500&amp;offset.q0=0&amp;limit_comments.q0=3&amp;callback=bv_351_44883&quot;) return(api_url) } # function to go through the list and extract meaningful results get_review &lt;- function(x){ product_id &lt;- ifelse(!is.null(x$ProductId), x$ProductId, &quot;&quot;) user_name &lt;- ifelse(!is.null(x$UserNickname), x$UserNickname, &quot;&quot;) rating &lt;- ifelse(!is.null(x$Rating), x$Rating, 0) review_date &lt;- ifelse(!is.null(x$SubmissionTime), x$SubmissionTime, &quot;&quot;) review_text &lt;- ifelse(!is.null(x$ReviewText), x$ReviewText, &quot;&quot;) review_title &lt;- ifelse(!is.null(x$Title), x$Title, &quot;&quot;) # message (&quot;sofa sogood&quot;) results &lt;- tibble( product_id = product_id, user_name= user_name, rating_num = rating, review_date =review_date, review_text =review_text , review_title = review_title ) return(results) } 3.5.3 Scraping the Content Now that we have the product ids, we can loop through them and call the API to get the reviews. # set up our results tibble all_reviews &lt;- tibble() # #20 seems to have no reviews, json_results didn&#39;t have SubmissionTime, so added that to conditions for (i in 1:nrow(product_ids)){ # print an update message and wait nicely product_id &lt;- product_ids$product_id[[i]] message(paste0(&quot;Product #&quot;,i,&quot;/&quot;,nrow(product_ids),&quot;: &quot;,product_id)) Sys.sleep(2) api_url &lt;- get_first_api_url(product_id) # call the API text &lt;- GET(api_url) %&gt;% content(&quot;text&quot;) # parse the returned text into json json_parsed &lt;- text %&gt;% str_extract(&quot;\\\\{(.*)\\\\}&quot;) %&gt;% #str_extract(&quot;(?&lt;=BV._internal.dataHandler0\\\\()(.*)&quot;)#(?=\\\\))&quot;) %&gt;% jsonlite::parse_json() # get the product information product &lt;- json_parsed$BatchedResults$q0$Results[[1]] product_name &lt;- product$Name product_brand &lt;- product$Brand$Name reviews &lt;- json_parsed$BatchedResults$q2$Results # use purrr::map to apply get_review() to each individual review reviews1 &lt;- tibble( x = purrr::map(reviews, get_review) ) %&gt;% unnest(cols = &quot;x&quot;) message (&quot; First API call done and processed.&quot;) ####################################3 # SECOND API CALL. Try to load additional reviews: api_url &lt;- get_second_api_url(product_id) # test &lt;- read_html(api_url) # text &lt;- test %&gt;% html_text() text &lt;- GET(api_url) %&gt;% content(&quot;text&quot;) json_parsed &lt;- text %&gt;% str_extract(&quot;(?&lt;=\\\\()(.*)(?=\\\\))&quot;) %&gt;% jsonlite::fromJSON() json_results &lt;- json_parsed$BatchedResults$q0$Results # set our second set of reviews to NULL in case we don&#39;t find any reviews2 &lt;- NULL # if we do find some, set them to that! if (!is.null(json_results) &amp; length(json_results)&gt;0) { if (any(str_detect(names(json_results), &quot;SubmissionTime&quot;))){ reviews2 &lt;- json_results %&gt;% as_tibble() %&gt;% select(review_date = SubmissionTime, user_name = UserNickname, review_title = Title, review_text = ReviewText, rating_num = Rating ) %&gt;% mutate(product_id = product_code) } } message (&quot; Second API call done and processed.&quot;) # put the new reviews together: new_reviews &lt;- bind_rows(reviews1, reviews2) %&gt;% mutate(product_name = product_name, product_brand= product_brand) all_reviews &lt;- bind_rows(all_reviews, new_reviews) } # end (for i) all_reviews %&gt;% distinct() %&gt;% write_csv(paste0(&quot;reviews-&quot;,product_type,&quot;.csv&quot;)) 3.6 Summary References "],["data-summary-eda-initial-model-attempts.html", "Chapter 4 Data Summary, EDA, &amp; Initial Model Attempts 4.1 Introduction 4.2 Goodreads 4.3 Yelp 4.4 Mountain Equipment Co-op (MEC) 4.5 Comparing Goodreads, MEC, and Yelp 4.6 Reviews Over Time 4.7 Proposed Next Steps 4.8 SessionInfo", " Chapter 4 Data Summary, EDA, &amp; Initial Model Attempts 4.1 Introduction I have three original datasets for analysis, both of which were collected from public websites between October 21 and 27, 2020. Yelp Reviews: 9,402 reviews for restaurants in Ottawa, which I believe includes all reviews available as of October 21. Each review includes: Business Name: The name the business is listed as operating under on Yelp. (Character) Reviewer Name: The screen name of the user who wrote the review. (Character) Review Date: The date the review was posted. (Character in mm/dd/yyyy format) Review Text: The full text of the review. (Character) Star Rating: The number of stars associated with the review (Integer from 1 to 5) Review URL: The URL from which the review was downloaded for traceability. (Character) Goodreads Reviews: 17,091 book reviews, culled from the first-page reviews of the 100 most-read books in a number of genres. Each review includes: Book Title: The title of the book. (Character) Book Genre: The Goodreads-assigned genre of the book, e.g. scifi or romance. (Character) Book Author: The author of the book. (Character) Reviewer Name: The screen name of the user who wrote the review. (Character) Review Date: The date the review was posted. (Character in yyyy-mm-dd format) Review Text: The full text of the review. (Character) Star Text: Goodreads text equivalent for star ratings. (Character) Star Rating: The number of stars associated with the review (Integer from 1 to 5) Review URL: The URL from which the review was downloaded for traceability. (Character) Mountain Equipment Co-op (MEC) Reviews: 2,392 reviews for products for sale from MEC. Each review includes: Product Type: MECs categorization for the product (e.g. mittens, bicycle components.) (Character) Product Brand: The brand under which the product is marketed on MECs website. (Character) Product Name: The name of the product. (Character) Product ID: MECs internal product ID, used to call the API. (Character) Reviewer Name: The username of the review writer. (Character) Review Date: The date the review was left. (Character) Review Title: The title of the review. (Character) Review Text: The complete text of the review. (Character) Star Rating: The number of stars associated with the review. (Integer from 1 to 5) In this section, Ill take a look at these two datasets to get a feel for the star ratings and review text. I will consider each dataset in turn. reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% rename(rating_num = rating) reviews_gr &lt;- read_csv(&quot;../tests/data/goodreads_all.csv&quot;) reviews_mec &lt;- read_csv(&quot;../tests/data/mec-reviews.csv&quot;) %&gt;% rename(comment = review_text, date = review_date) 4.2 Goodreads 4.2.1 Star Ratings The following histogram shows the overall distribution of star ratings. Reviews are overwhelmingly positive: there are move 5-star reviews than there are 1-, 2-, and 3-star reviews combined. This may make modeling more difficult, since there will be fewer low-star ratings to train our models. reviews_gr %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Goodreads Ratings: Rating Count, Overall&quot;, x=&quot;Star Rating&quot;, y=NULL) The next histogram shows that the pattern is broadly consistent across genres. There are some minor differences: for example, graphic-novel and mystery reviews have nearly the same number of 4- and 5-star ratings, whereas nonfiction and romance novels show markedly more 5-star reviews than 4-star reviews. But for present purposes the overall pattern looks largely the samefor example, there are no U-shaped distributions, or exponential-type distributions with the opposite skew. reviews_gr %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Goodreads Ratings: Rating Count by Genre&quot;, x = &quot;Star Rating&quot;, y=NULL) + facet_wrap(facets = vars(genre)) However, if we look at the level of individual books, the distributions look a bit more interesting. All the histograms are unimodal, but some of them peak at 3 or 4. (Poor Brian K. Vaughan.) top_6_books &lt;- reviews_gr %&gt;% group_by(book_title) %&gt;% summarise(n = n()) %&gt;% slice_max(n=6, order_by=n, with_ties=FALSE) %&gt;% pull(book_title) ## `summarise()` ungrouping output (override with `.groups` argument) reviews_gr %&gt;% filter(book_title %in% top_6_books) %&gt;% ggplot(aes(x = rating_num)) + geom_histogram( binwidth=1, boundary=0.5, bins=5) + facet_wrap(facets = vars(book_title)) + theme_grey() + labs(title = &quot;Star Ratings for 6 of the Most-Reviewed Books&quot;, subtitle = &quot;Sampled randomly from across all genres.&quot;, x = &quot;Star Rating&quot;, y = &quot;# of Ratings&quot;) 4.2.2 Word Count Turning to word count, the following graph shows the cumulative density of word counts in our review dataset. In other words, as word count increases on the x-axis, the y-axis shows us how many reviews have at most that many words. I have counted words here using unnest_tokens() from the tidytext package (as per Tidy Text Mining). There may be an easier way, but this worked! We find that most reviews are very short: about 15,000 are below 500 words, and they go as short as one word. Some reviews are quite long, and one stretches out past 3,500 words. wordcounts_gr &lt;- reviews_gr %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_gr %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;Goodreads Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) This distribution may also make our modeling task more difficult. With so many short reviews its unlikely that they will have many words in common, and so a lasso regression at the word level may not work very well. However, short reviews may still be useful for sentiment analysis. The following table shows the five shortest reviews, since I wanted to check and make sure it wasnt a data error. One reviewer left a single word: SUCKS. Concise and informative. wordcounts_gr %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_gr, .) %&gt;% select(book_title,author_name, rating_num, comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Book Title&quot;, &quot;Book Author&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Book Title Book Author Stars Review The Alchemist Paulo Coelho 1 SUCKS. The Mysterious Affair at Styles Agatha Christie 5 Classic Siddhartha Hermann Hesse 2 Eh. Treasure Island Robert Louis Stevenson 5 ARRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR Logan Likes Mary Anne! Gale Galligan 4 cool # # %&gt;% # kableExtra::column_spec(column = 1:4, # width = c(&quot;15cm&quot;,&quot;10cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.2.3 Reviewers The following histogram shows that while most Goodreads users posted only a handful of reviews in our dataset, some posted over 50. reviewers_gr &lt;- reviews_gr %&gt;% group_by(names) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_gr %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;Goodreads: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looking at the following table, we can see that the top 10 reviewers all posted over 50 reviews, and one posted 95. reviewers_gr %&gt;% top_n(10, wt = n) ## # A tibble: 11 x 2 ## names n ## &lt;chr&gt; &lt;int&gt; ## 1 Ahmad Sharabiani 95 ## 2 Lisa 89 ## 3 Matthew 63 ## 4 jessica 61 ## 5 Sean Barrs 59 ## 6 Emily May 56 ## 7 Michelle 53 ## 8 Jennifer 52 ## 9 Elyse Walters 51 ## 10 Melissa &lt;U+2665&gt; Dog/Wolf Lover &lt;U+2665&gt; Martin 50 ## 11 Nilufer Ozmekik 50 Out of curiosity (and as a check on our data quality), lets investigate the 95 reviews from our top poster, Ahmad Sharabiani: reviews_gr %&gt;% filter(names == &quot;Ahmad Sharabiani&quot;) %&gt;% select(book_title, author_name, rating_num, comment) %&gt;% mutate (comment = str_trunc(comment, 80)) %&gt;% arrange(desc(author_name)) %&gt;% slice_head(n=10) %&gt;% knitr::kable(col.names = c(&quot;Book Title&quot;, &quot;Book Author&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Book Title Book Author Stars Review Romeo and Juliet William Shakespeare 5 Romeo and Juliet = The Tragedy of Romeo and Juliet, William ShakespeareRomeo  Othello William Shakespeare 4 Othello = The Tragedy of Othello, William ShakespeareOthello (The Tragedy of  Othello William Shakespeare 5 The Tragedy of Othello, The Moor of Venice, William ShakespeareOthello is a t Lord of the Flies William Golding 4 Lord of the flies, William GoldingLord of the Flies is a 1954 novel by N A Room of Ones Own Virginia Woolf 4 A Room of Ones Own, Virginia WoolfA Room of Ones Own is an extended essay b Beowulf Unknown 5 Beowulf, Anonymous Anglo-Saxon poetBeowulf is an Old English epic poem consis In Cold Blood Truman Capote 4 In Cold Blood, Truman CapoteThis article is about the book by Truman Capote.  The Bluest Eye Toni Morrison 4 The Bluest Eye, Toni MorrisonThe Bluest Eye is a novel written by Toni M The Bell Jar Sylvia Plath 4 Victoria Lucas = The Bell Jar, Sylvia PlathThe Bell Jar is the only nove The Shining Stephen King 4 The Shining (The Shining #1), Stephen KingThe Shining is a horror novel by Am # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Something looks a bit suspicious here. First, many books have more than one review (for example, Othello has 2 and The Catcher in the Rye has 3). Second, the reviews all seem to begin with the title of the book and a factual summary without much personality. If we do a Google search for the opening text of Ahmads review for Farenheit 451, Fahrenheit 451 is a dystopian novel by American, we find that exact text in the first line of the books Wikipedia page. Google also suggests we look at Farenheit 451s Goodreads page, which includes Ahmads review. If we look at Ahmads review more closely, we see that it includes an English-language summary and then a lot of text in a non-Latin alphabet. reviews_gr %&gt;% filter(names == &quot;Ahmad Sharabiani&quot; &amp; book_title == &quot;Fahrenheit 451&quot;) %&gt;% pull(comment) %&gt;% str_trunc(700) ## [1] &quot;Fahrenheit 451, Ray BradburyFahrenheit 451 is a dystopian novel by American writer Ray Bradbury, published in 1953. Fahrenheit 451 is set in an unspecified city at an unspecified time in the future after the year 1960.Guy Montag is a \\&quot;fireman\\&quot; employed to burn houses containing outlawed books. He is married but has no children. One fall night while returning from work, he meets his new neighbor, a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and he calls for medical attention. ...&lt;U+062A&gt;&lt;U+0627&gt;&lt;U+0631&gt;&lt;U+06CC&gt;&lt;U+062E&gt; &lt;U+0646&gt;&lt;U+062E&gt;&lt;U+0633&gt;&lt;U+062A&gt;&lt;U+06CC&gt;&lt;U+0646&gt; &lt;U+062E&gt;&lt;U+0648&gt;&lt;U+0627&gt;&lt;U+0646&gt;&lt;U+0634&gt;: &lt;U+0631&gt;&lt;U+0648&gt;...&quot; Google Translate tells me the language is Persian, and the translated text includes a brief noteDate of first reading: The third day of February 1984and then another summary of the book written in Persian. The text does not seem to have any actual review or opinion in it. Im not sure whats going on here, but we have learned that: * Some users post a large number of reviews; * Some users post useless/non-review reviews, e.g. copy/pasting text from Wikipedia; and, * At least one super-poster posts such reviews. This bears looking into more, since reviews that are copy/pasted from Wikipedia are unlikely to have any predictive value at all and may need to be identified and filtered out in pre-processing. These users may even be bots, especially given the short timeframe for the Goodreads dataset (see below). 4.3 Yelp 4.3.1 Star Ratings Repeating the process for Yelp, this histogram shows the distribution of star ratings. Reviews are again very positive and show a similar distribution. reviews_yelp %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Yelp Ratings by Star&quot;, x=&quot;Star Rating&quot;, y=NULL) The Yelp data didnt include restaurant type, so we cant do a genre-specific investigation as we did for Goodreads. However, we can repeat the analysis where we look at star distributions for the top 6 businesses. Overall the distributions look the same, but here, finally, we get the first hint of bimodality in our distributions. Two restaurants, Sansotei Ramen and Shawarma Palace, have slight second peaks at 1 star. However, the overall story is the same and this could arguably be random fluctuations. top_6_restos &lt;- reviews_yelp %&gt;% group_by(business) %&gt;% summarise(n = n()) %&gt;% slice_max(n=6, order_by=n, with_ties=FALSE) %&gt;% pull(business) ## `summarise()` ungrouping output (override with `.groups` argument) reviews_yelp %&gt;% filter(business %in% top_6_restos) %&gt;% ggplot(aes(x = rating_num)) + geom_histogram( binwidth=1, boundary=0.5, bins=5) + facet_wrap(facets = vars(business)) + theme_grey() + labs(title = &quot;Star Ratings for 6 of the Most-Reviewed Restaurants&quot;, x = &quot;Star Rating&quot;, y = &quot;# of Ratings&quot;) 4.3.2 Word Count As with the Goodreads data, most Yelp reviews are very short. wordcounts_yelp &lt;- reviews_yelp %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_yelp %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;Yelp Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) And again, lets review the five shortest Yelp reviews in the table below. They seem to be genuine good-faith reviews that include helpful words, and so may be workable for our models. wordcounts_yelp %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_yelp, .) %&gt;% select(business,rating_num,comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Business Stars Review Kallisto Greek Restaurant 4 Great takeout, service, ambiance and  Bite Burger House 4 Delicious, juicy, interesting burgers BeaverTails 4 BeaverTails pastry..no words needed.. Saigon Boy Noodle House 3 Very decent pho shop, well priced. Supreme Kabob House 5 Excellent Afghani Food and Good Space # %&gt;% # kableExtra::column_spec(column = 1:3, # width = c(&quot;5cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling()%&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.3.3 Reviewers The following histogram shows how many reviews were posted be users. Its distribution is similar to the one we found for Goodreads: most users posted only a few times, but some posted over 50. reviewers_yelp &lt;- reviews_yelp %&gt;% group_by(name) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_yelp %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;Yelp: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looking at our top-10 Yelp reviewers, the drop-off is quite a bit sharper than it was for Goodreads. reviewers_yelp %&gt;% top_n(10, wt = n) %&gt;% knitr::kable(col.names = c(&quot;Name&quot;, &quot;# Reviews&quot;), align = c(&quot;l&quot;,&quot;c&quot;)) Name # Reviews Jennifer P. 78 Amelia J. 77 Dawn M. 51 Samantha M. 44 Eric B. 41 Amanda B. 35 Coy W. 34 Drew K. 27 Spike D. 25 Amy B. 23 # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) The following table shows the first 10 reviews by our top reviewer, Jennifer P., in chronological order. reviews_yelp %&gt;% filter(name == &quot;Jennifer P.&quot;) %&gt;% select(date, business, rating_num, comment) %&gt;% mutate(date = lubridate::mdy(date), comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% slice_head(n=10) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Business Stars Review 2012-06-22 Mr Bs-March House Restaurant 4 I never would have tried this restaurant had it not been recommende 2012-07-04 Alirang Restaurant 3 I was here last week with my husband, my brother and his girlfriend 2013-01-24 Corazón De Maíz 4 I can&amp;amp;#39;t believe that I walk by this place all the time, but 2013-06-24 222 Lyon Tapas Bar 5 This place is absolutely delicious, but man is it ever expensive!  2013-09-02 Bennys Bistro 5 I was visiting from out of town for my best friend&amp;amp;#39;s weddin 2013-09-20 Art Is In Bakery 4 My husband and I were here for their Sunday Brunch recently with an 2013-11-22 Gezellig 3 Sorry, I&amp;amp;#39;m going to have to downgrade this place to 3 stars 2013-12-30 Thai Coconut 4 I went here with my husband today for the lunch buffet. It was gre 2014-05-07 Bite Burger House 4 I had an early dinner here with my husband recently. Bite Burger H 2014-07-07 Pookies Thai 4 I went here for dinner recently with my husband on a whim. We&amp;amp; # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) These all seem to be good-faith restaurant reviews. And since this user has been active since 2012, to write 78 reviews they would have to write fewer than one per month. From this brief glance, we have no reason to think that Yelp users are posting insincere reviews. However, I note that the reviews have some html junk in them: &amp;amp;#39; instead of an apostrophe, for example. These will need to be cleaned up before we use the data. 4.4 Mountain Equipment Co-op (MEC) 4.4.1 Star Ratings This histogram shows the distribution of star ratings for MEC reviews. Its broadly similar to the Yelp and Goodreads reviews, except there is a small second peak at 1 star. reviews_mec %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count, Overall&quot;, x=&quot;Star Rating&quot;, y=NULL) If we break out the reviews by category, we can see that they all follow the same kind of exponential distribution except bicycle components. reviews_mec %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count by Product Category&quot;, x=&quot;Star Rating&quot;, y=NULL) + facet_wrap(~product_type) We can break the bicycle compoenents category down further by individual product. The facet wrap is messy, but we can clearly see that there are a few produts with anomalous spikes in 1-star ratings, and that ecah of these products has the word tube in the title. reviews_mec %&gt;% filter(product_type==&quot;bike-components&quot;) %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count by Product&quot;, subtitle = &quot;Bicycle Components&quot;, x=&quot;Star Rating&quot;, y=NULL) + facet_wrap(~product_name) We can conclude that MECs reviews follow the same pattern as Yelp and Goodreads overall, except for bicycle inner tubes which have unusually high numbers of 1-star reviews. We should keep this in mind when modeling using the MEC data. 4.4.2 Word Counts Most MEC reviews are very short. They look to be shortest of all three datasets, both in terms of the shape of the dsitribution and the maximum review lengths. We will see this below in a later section when we plot all three distributions at once. wordcounts_mec &lt;- reviews_mec %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_mec %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;MEC Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) If we look at the five shortest reviews, they all seem to be short but legitimate so we can be comfortable with our data quality. wordcounts_mec %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_mec, .) %&gt;% select(product_name,rating_num,comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Business Stars Review MEC 700 x 23-25C Tube (48mm Presta Va 2 Lasted 1 season basically disposable Smartwool Liner Gloves - Unisex 5 Love smartwool products, the gloves a Scarpa Moraine Mid Gore-Tex Light Tra 5 light, comfortable and good looking!  Scarpa Moraine Mid Gore-Tex Light Tra 4 good pair of shoes. lightweight but  La Sportiva TC Pro Rock Shoes - Unisex 5 Flat stiff shoe. Perfect for vertical # %&gt;% # kableExtra::column_spec(column = 1:3, # width = c(&quot;5cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling()%&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.4.3 Reviewers As with the other datasets, it first appears that most users leave only a few reviews but there are some super-users who leave quite a few. reviewers_mec &lt;- reviews_mec %&gt;% group_by(user_name) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_mec %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;MEC: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Upon closer inspection, however, we see that our largest user is NA, suggesting that most users leave a smallish number of reviews but that some leave reviews anonymously. reviewers_mec %&gt;% top_n(10, wt = n) %&gt;% knitr::kable(col.names = c(&quot;Name&quot;, &quot;# Reviews&quot;), align = c(&quot;l&quot;,&quot;c&quot;))%&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Name # Reviews NA 76 Matt 9 Chris 8 Mike 8 Ryan 7 John 6 Mark 6 VicCyclist40 6 Dave 5 Paul 5 Steph 5 The following table shows all 9 reviews by our top reviewer, Matt, in chronological order. reviews_mec %&gt;% filter(user_name == &quot;Matt&quot;) %&gt;% select(date, product_name, rating_num, comment) %&gt;% mutate(comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Product&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Product Stars Review 2016-02-29 04:59:40 Black Diamond Mercury Mitts - Mens 3 I bought these to replace the BD mercury mitts I purchased 6 years  2017-08-23 20:16:05 MEC Mallard -5C Down Sleeping Bag - Unisex 5 I usually use a bag with a hood but found my self feeling confined  2017-11-14 03:28:48 Oboz Bridger Mid Bdry Hiking Shoes - Mens 5 Bought these boots two years ago. Hiked up Sulphur Skyline in Jaspe 2018-01-20 21:55:36 MEC Goto Fleece Gloves - Unisex 5 These are casual use gloves for me and I wear them around town when 2018-02-01 11:13:17 Black Diamond Guide Gloves - Mens 2 Very warm, but not very durable. Considering the cost, these gloves 2018-07-24 20:03:37 MEC Creekside 0C Sleeping Bag - Unisex 1 Ive used this bag twice and froze both times at temperatures betwe 2019-05-15 04:00:46 Scarpa Kailash Trek Gore-Tex Hiking Boots - Mens 5 While i have so far only logged one day of hiking in my new Scarpas 2019-06-11 00:19:21 La Sportiva Finale Rock Shoes - Mens 5 Im relatively new to the sport and decided to go with these as my  2019-08-25 17:47:30 MEC Reactor 10 Double Sleeping Pad - Unisex 5 I recently bought this mattress for car camping and it is incredibl # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) This looks like a legit usage pattern with real reviews. However, we should also spot-check some reviews assigned to NA: reviews_mec %&gt;% filter(is.na(user_name)) %&gt;% select(date, product_name, rating_num, comment) %&gt;% slice_head(n=10) %&gt;% mutate(comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Business Stars Review 2012-08-05 02:01:07 SRAM PC-971 9 Speed Chain 3 I used this chain on both my road and mountain bikes. Its done fi 2013-09-23 01:57:46 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 Dont waste your time with these. Ive been through 4 this season a 2014-05-12 17:00:57 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 This is the third of these tubes I have had split down the seam. Th 2014-06-25 03:13:43 MEC 700 x 23-25C Tube (60mm Presta Valve) 1 Im afraid that I have to add my voice to the chorus of negative re 2014-10-04 02:21:33 MEC 700 x 23-25C Tube (48mm Presta Valve) 4 Im not sure where this chorus of negative reviews is coming from.  2014-10-25 21:28:29 MEC 700X32-35C (27\"x1 1/4) Tube Schrader Valve 1 Ive bought two of these tubes and had to return both of them, I wi 2015-08-06 21:25:01 MEC 700X32-35C (27\"x1 1/4) Tube Schrader Valve 3 Have used three of these over a couple of years and the only one th 2015-11-16 19:45:33 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 I ride ten kilometres to work and ten kilometres home from work eve 2016-02-27 13:47:26 SRAM PC-971 9 Speed Chain 4 I have been using PC971 chains for many years. Currently I do most  2016-06-22 21:38:52 SRAM PC-971 9 Speed Chain 4 The chain that I have purchased from MEC is really a good chain. I  # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) These also look like legitimate reviews, so its possible that these were legitimately left anonymously or that there was a data-parsing issue with the API. 4.5 Comparing Goodreads, MEC, and Yelp 4.5.1 Star Ratings When we compare Yelp and Goodreads reviews by the number of star ratings, the distributions look very similar. There are fewer Yelp reviews, but the shape of the distribution looks like a scaled-down version of the Goodreads distribution. There are far fewer MEC reviews, and it looks like the distribution has a slight second peak at 1 star. gr &lt;- reviews_gr %&gt;% group_by(rating_num) %&gt;% summarise(gr = n()) ## `summarise()` ungrouping output (override with `.groups` argument) yp &lt;- reviews_yelp %&gt;% group_by(rating_num) %&gt;% summarise(yp = n()) ## `summarise()` ungrouping output (override with `.groups` argument) mc &lt;- reviews_mec %&gt;% group_by(rating_num) %&gt;% summarise(mc = n()) ## `summarise()` ungrouping output (override with `.groups` argument) compare &lt;- left_join(gr, yp) %&gt;% left_join(mc) ## Joining, by = &quot;rating_num&quot; ## Joining, by = &quot;rating_num&quot; compare_long &lt;- compare %&gt;% pivot_longer(cols = c(&quot;gr&quot;, &quot;yp&quot;,&quot;mc&quot;), names_to = &quot;source&quot;, values_to = &quot;num&quot;) compare_long %&gt;% ggplot() + geom_col(aes(x=rating_num, y=num, group=source, fill=source), position = &quot;dodge&quot;) + theme_minimal() + labs(title = &quot;Goodreads, MEC, and Yelp Reviews: Total Counts by Rating&quot;, x = &quot;Star Rating&quot;, y = &quot;n&quot;, fill = &quot;Source&quot;) + scale_fill_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) To get a better feel for how the distributions vary, we can plot the proportional breakdown of star reviews for each source. The following plot shows that the Goodreads and Yelp distributions track each other somewhat closely but the MEC reviews are quite different. compare_long %&gt;% group_by(source) %&gt;% mutate(prop = num / sum(num)) %&gt;% ggplot() + geom_col(aes(x=rating_num, y=prop, group=source, fill=source), position = &quot;dodge&quot;) + theme_minimal() + labs(title = &quot;Goodreads, MEC, and Yelp Reviews: Proportion of Counts by Rating&quot;, x = &quot;Star Rating&quot;, y = &quot;Proportion&quot;, fill = &quot;Source&quot;) + scale_fill_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) We can use a standard Pearsons Chi-squared test to see if the Goodreads and Yelp distributions differ meaningfully. t &lt;- chisq.test(compare$gr, compare$yp) ## Warning in chisq.test(compare$gr, compare$yp): Chi-squared approximation may be incorrect tt &lt;- chisq.test(matrix(c(compare$gr, compare$yp), ncol=5)) tt ## ## Pearson&#39;s Chi-squared test ## ## data: matrix(c(compare$gr, compare$yp), ncol = 5) ## X-squared = 8357.3, df = 4, p-value &lt; 2.2e-16 We find that yes, we can reject the null hypothesis that there is no difference between the two distributions with a large amount of confidence. However, the two review distributions are still qualitatively similar, its not clear that the difference between them is large or meaningfulwe could look into that later. 4.5.2 Word Counts Out of interest, lets also check the differences in word-count distributions between the three datasets. From the figure below, we can see that Yelp reviews tend to be much shorter than Goodreads reviews. Just by visual inspection, we can estimate that the 80th percentile Goodreads review is about 500 words, whereas the 80th percentile Yelp review is only about half of that. The MEC reviews are shortest of all. wordcounts_all &lt;- wordcounts_gr %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;goodreads&quot;) %&gt;% bind_rows( wordcounts_yelp %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;yelp&quot;)) %&gt;% bind_rows( wordcounts_mec %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;mec&quot;)) wordcounts_all %&gt;% group_by(source) %&gt;% mutate (prop = cumdist / max(cumdist)) %&gt;% ggplot() + geom_point(aes(y=prop, x=n, colour = source)) + labs(title = &quot;Cumulative Distribution of Word Lengths&quot;, subtitle = &quot;Comparing Goodreads, MEC, and Yelp&quot;, x = &quot;Word Length&quot;, y = &quot;Cumulative Probability&quot;, colour = &quot;Source&quot;) + scale_color_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) + theme_minimal() To test for difference, we can confirm do a non-parametric Kolmogorov-Smirnov test to see if the Goodreads and Yelp distributions differ. # pull the word lengths for goodreads into a vector grd &lt;- wordcounts_all %&gt;% filter(source == &quot;goodreads&quot;) %&gt;% pull(n) # pull the word lengths for yelp into a vector ypd &lt;- wordcounts_all %&gt;% filter(source == &quot;yelp&quot;) %&gt;% pull(n) # run KS test comparing the two vectors ks.test(grd, ypd) ## Warning in ks.test(grd, ypd): p-value will be approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: grd and ypd ## D = 0.24968, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided # remove the vectors to keep environment clean rm(grd, ypd) We can again reject the null hypothesis that there is no difference between the two distributions. We can hypothesize about why there might be a difference: Goodreads reviewers are writing about books, and so might be expected to be interested in expressing themselves through writing. Yelp reviewers, by and large, are interested in restaurants, and so may not put as much effort into writing full reports. We might expect the difference in distributions to have an effect on our future modeling, since shorter reviews may contain less information. 4.6 Reviews Over Time This section looks at how our review datasets change over time, to see how recent reviews are and if there are any trends in volume. 4.6.1 Goodreads The following chart shows the monthly volume of reviews in the Goodreads dataset. reviews_gr %&gt;% mutate(dates = lubridate::ymd(dates) %&gt;% lubridate::floor_date(&quot;months&quot;)) %&gt;% group_by(dates) %&gt;% summarise(n = n()) %&gt;% ggplot(aes(x=dates,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;Goodreads Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) While some reviews date from as far back as 2005, most reviews are from 2020 and the majority are from the past few months. However, its unlikely that this distribution represents an actual exponential growth in the number of reviews posted. Instead, recall that I collected reviews for the 100 most-read books in the past week across a few genres. In other words, I collected reviews from books that were being reviewed a lot at that moment in time, so my data collection is heavily biased towards more recent reviews. There may a trend in usagefor example, home-bound readers may be posting more reviews during COVID-19but we cant draw any conclusions from this distribution. 4.6.2 Yelp The following chart shows the monthly volume of reviews in the Yelp dataset. reviews_yelp %&gt;% mutate(date = lubridate::mdy(date) %&gt;% lubridate::floor_date(&quot;months&quot;)) %&gt;% group_by(date) %&gt;% summarise(n = n()) %&gt;% ggplot(aes(x=date,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;Yelp Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) Since I collected all Yelp reviews for restaurants in Ottawa, we can use this dataset to make statements about how review volumes have changed over time. We can see a steep decline in the early months of 2020, coinciding with the start of the COVID-19 pandemic and worldwide lockdowns. However, the volumes also tell an interesting story pre-COVID. From 2010 to 2015 we can see what looks like slow but steady growth, and then after 2015 usage increases dramatically. From 2015-2020 we can see what look like seasonal trends, but it looks like overall volumes stopped growing and may have started declining. In other words, Yelp may have been in trouble before the pandemic hit. For our purposes, we can be satisfied that our restaurant review dataset spans a long period of time both pre- and post-COVID. 4.6.3 MEC The following chart shows the monthly volume of reviews in the MEC dataset for each complete month. The data was collected in the first few days of November, so I have left November out. reviews_mec %&gt;% mutate(date = lubridate::floor_date(date, &quot;months&quot;)) %&gt;% group_by(date) %&gt;% summarise(n = n()) %&gt;% slice_head(n = nrow(.)-1) %&gt;% ggplot(aes(x=date,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;MEC Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) We can expect several biases in the MEC data, so we will need to be cautious about making inferences from this time series. First, I collected MEC data from only a few product categories which may have seasonal trends (e.g. biking in the summer, snowshoeing in the winter). Second, MEC only lists products on its website if theyre currently for sale, so the maximum review age is limited by the longevity of MECs product lines. So we should expect to see a decay in review volume as we go further back in time caused by MEC naturally rotating its product line. That said, we can still see a big dip in early 2020 and then a big spike in summer 2020. This could correspond to a big drop in sales with the COVID lockdown and associated uncertainty, and then a bike spike in outdoor sporting goods as people tried to find socially distanced ways of entertaining themselves over the summer. Out of curiosity, here are the 10 oldest reviews in our dataset: reviews_mec %&gt;% arrange(date) %&gt;% slice_head(n=10) %&gt;% select(date, product_name, review_title) ## # A tibble: 10 x 3 ## date product_name review_title ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2007-05-22 00:00:00 MEC V-Brake Cartridge Brake Pads I need more!!! ## 2 2007-08-12 00:00:00 MEC V-Brake Shoe/Pad Assembly Buy them once, love them forever ## 3 2007-09-05 00:00:00 SRAM PC-971 9 Speed Chain Great value ## 4 2008-02-28 00:00:00 MEC V-Brake Cartridge Brake Pads Decent brake pads ## 5 2008-11-02 00:00:00 MEC V-Brake Shoe/Pad Assembly Great product. ## 6 2008-12-16 00:00:00 MEC V-Brake Shoe/Pad Assembly Best value in a V-brake pad! ## 7 2008-12-18 00:00:00 Zamberlan Vioz GT Gore-Tex Backpacking Boots - Women&#39;s Mènent au sommet! ## 8 2009-01-15 00:00:00 SRAM PC-971 9 Speed Chain Decent deal on a higher end chain. ## 9 2009-07-21 18:37:26 SRAM PC-830 8 Speed Chain short life ## 10 2009-08-17 19:44:51 SRAM PC-830 8 Speed Chain Not impressed! Not surprisingly, 9 out of 10 are for standard bicycle components that are more about function than fashion: it seems that MEC and SRAM have been offering the same brake pads and chains for more than 10 years. And we can take a look at the first review for the Zamberlan boots: reviews_mec %&gt;% filter(product_name==&quot;Zamberlan Vioz GT Gore-Tex Backpacking Boots - Women&#39;s&quot;) %&gt;% slice_head(n=1) %&gt;% transmute(date = date, comment = str_trunc(comment, 150)) ## # A tibble: 1 x 2 ## date comment ## &lt;dttm&gt; &lt;chr&gt; ## 1 2015-05-26 20:40:15 I&#39;ve been using these Zamberlan Viozes for the past 4 years. I&#39;ve owned 4 pairs in that time and I&#39;m just about to start my 5th. I buy a pair every... These boots seem to have been around for a while (and certainly seem to have committed fans), so we can be confident that these reviews are legit. 4.7 Proposed Next Steps Sentiment analysis Regression models LASSO regression to predict star rating from review text. Potential to use minimum review length as a parameter. Linear regression to predict star rating from review sentiment. Classification models 4.8 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] vip_0.2.2 glmnet_4.0-2 Matrix_1.2-18 ggridges_0.5.2 discrim_0.1.1 tictoc_1.0 textrecipes_0.3.0 lubridate_1.7.9 yardstick_0.0.7 workflows_0.2.0 ## [11] tune_0.1.1 rsample_0.0.8 recipes_0.1.13 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 ## [21] tidytext_0.2.5 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 SnowballC_0.7.0 ## [10] prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 pROC_1.16.2 packrat_0.5.0 ## [19] dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 htmltools_0.5.0 tools_4.0.2 gtable_0.3.0 ## [28] glue_1.4.1 naivebayes_0.9.7 rappdirs_0.3.1 Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 iterators_1.0.12 timeDate_3043.102 ## [37] gower_0.2.2 xfun_0.16 stopwords_2.0 globals_0.13.0 rvest_0.3.6 lifecycle_0.2.0 future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 ## [46] hms_0.5.3 parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 ## [55] textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 lava_1.6.8 rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 ## [64] labeling_0.3 tidyselect_1.1.0 plyr_1.8.6 magrittr_1.5 bookdown_0.20 R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.6 ## [73] haven_2.3.1 withr_2.2.0 survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 modelr_0.1.8 crayon_1.3.4 utf8_1.1.4 rmarkdown_2.3 ## [82] usethis_1.6.1 grid_4.0.2 readxl_1.3.1 blob_1.2.1 reprex_0.3.0 digest_0.6.25 webshot_0.5.2 munsell_0.5.0 GPfit_1.0-8 ## [91] viridisLite_0.3.0 kableExtra_1.1.0 "],["a-first-lasso-attempt.html", "Chapter 5 A First LASSO Attempt 5.1 Introduction 5.2 A First Regression: Yelp Data 5.3 Trying lasso again 5.4 Removing stop words 5.5 Adjusting n-grams 5.6 Full final regression 5.7 Conclusion", " Chapter 5 A First LASSO Attempt 5.1 Introduction This analysis will use regression methods to attemp to predict star ratings from the text and/or titles of the reviews in our Yelp, Goodreads, and MEC datasets. My methods will closely follow those given in Chapter 6 of Supervised Machine Learning for Text Analysis in R (SMLTAR) by Silge and Hvitfeldt (2020). In the first case I will work through an example in detail to describe the steps (and to learn them!!), and in later sections I will move more quickly to try some different variations on the analysis. Im going to use the tidymodels framework as much as possible, both because its the approach used in SMLTAR and because Im a fan of the Tidyverse approach to software design and analysis. 5.2 A First Regression: Yelp Data I will begin with the Yelp data because we have a lot of it, and because based on our EDA it seemed to be cleaner than the Goodreads data which had a lot of duplicate posts, spam posts, plot summaries, etc. reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% mutate(date = lubridate::mdy(date)) %&gt;% rename(text = comment, rating_num = rating) reviews_yelp %&gt;% head(10) %&gt;% mutate(text = stringr::str_trunc(text, 100)) %&gt;% knitr::kable() business name date text rating_num url La Squadra Alain G. 2017-08-21 Confession: I am a foodie and I am a restaurant trained amateur Chef.&lt;br&gt;Been wanting to try t 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Amelia J. 2018-12-19 I came here for a Christmas lunch with coworkers and we tried the set Christmas menu (appetizer + 4 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Michael C. 2017-07-03 Beautiful venue, great service and incredible food. Try Squadra pasta and pizzaand the Aranci 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Cee Y. 2019-03-30 My husband and I stopped into this place as per a recommendation by Yelp. We had a fantastic time 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Luc S. 2018-03-04 Best Italian restaurant in Gatineau ! The food is authentic and fresh with good wine recommendati 5 http://www.yelp.ca/biz/la-squadra-gatineau Kallisto Greek Restaurant Amster S. 2020-04-18 I&amp;#39;ve been here twice. Once with my work friends and second with my family. I will come ba 5 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Reema D. 2020-01-12 Waitress was pretty slow. Didn&amp;#39;t take our dinner orders until after we finished apps and  4 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Jennifer P. 2018-05-21 My husband and I had dinner here recently and overall it was very good 3.75 stars, rounded up to  4 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Teena D. 2018-02-07 I had lunch today at Kallisto Greek Restaurant.&lt;br&gt;&lt;br&gt;I love chicken souvlaki and that&amp;amp 2 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Janie M. 2019-06-26 Find there&amp;#39;s always warm and friendly service. Best Greek food in Ottawa! My son, daught 5 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa 5.2.1 Splitting the data First we will split our data into a training set and a testing set. This is a standard practice, wherein we build a model using the training data but set aside some other data so we can test it later. Otherwise we might have concerns about overfitting or model validity. Im setting the value strata = \"rating_num\" to ensure that our random sampling has about the same distribution of star ratings as our full populationsee the documentation for initial_split(). set.seed(1234) yelp_split &lt;- reviews_yelp %&gt;% initial_split(strata = &quot;rating_num&quot;) yelp_train &lt;- yelp_split %&gt;% training() yelp_test &lt;- yelp_split %&gt;% testing() The next step is to define our preprocessing steps: the stuff well do to the text before we put it into a regression model. In the tidymodels approach we do this by creating a recipe objects and then adding a number of steps to it. We modify the object by using the pipe operator to add a bunch of steps to it using verb functions. This makes it easy to read the step-by-step process and understand whats going on. Ill note, though, that when I follow SMLTARs guide the recipe still includes explicit references to the dataset were analyzing, so its not a completely generic object that could be applied to other datasets: we would need to make other recipes for MEC and Goodreads. There may be more advanced ways to create generic recipes that can be reused. Here, following SMLTAR, we will use a recipe with the following steps: Tokenizing the text, which means breaking it down into constituent bits (words here), Filtering the tokens based on frequency, taking only the 250 most-common tokens, (NOTE this is not many tokens!!) TFIDF, or term frequency inverse document frequency, which weights each token based on both how frequent it is and on how common it is across documents (see step_tfidf()s help page for details), and then Normalizing so our lasso regression will work properly. num_tokens &lt;- 250 yelp_rec1 &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) yelp_rec1 ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Tokenization for text ## Text filtering for text ## Term frequency-inverse document frequency with text ## Centering and scaling for all_predictors() Next, Silge and Hvitfeldt (2020) suggest we create a workflow() object that combines preprocessing steps and models. yelp_wf1 &lt;- workflow() %&gt;% add_recipe(yelp_rec1) yelp_wf1 ## == Workflow ========================================================================================================================================================================================================================= ## Preprocessor: Recipe ## Model: None ## ## -- Preprocessor --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_tokenfilter() ## * step_tfidf() ## * step_normalize() We now define a lasso regression model using parsnip. My understanding is that this acts as a tidy wrapper around other functions/packages, in this case glmnet, that lets you use them in a tidy way. I believe it can also make it easier to swap out models or parameters without having to completely rewrite your codebase. Note that penalty = 0.1 is arbitrary and well look into that parameter more closely later. lasso_model1 &lt;- parsnip::linear_reg(penalty = 0.1, mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) lasso_model1 Now we add the lasso model to the workflow and run the model. This takes about 10 seconds on my machine using only 250 tokens. (I expect well need to use more to get a good result.) lasso_fit1 &lt;- yelp_wf1 %&gt;% add_model(lasso_model1) %&gt;% fit(data = yelp_train) We can look at the terms with the highest coefficients in the model: lasso_fit1 %&gt;% pull_workflow_fit() %&gt;% tidy() %&gt;% arrange(-estimate) ## # A tibble: 251 x 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.16 0.1 ## 2 tfidf_text_amazing 0.0490 0.1 ## 3 tfidf_text_delicious 0.0395 0.1 ## 4 tfidf_text_great 0.0379 0.1 ## 5 tfidf_text_best 0.0307 0.1 ## 6 tfidf_text_and 0.00624 0.1 ## 7 tfidf_text_2 0 0.1 ## 8 tfidf_text_3 0 0.1 ## 9 tfidf_text_34 0 0.1 ## 10 tfidf_text_39 0 0.1 ## # ... with 241 more rows This already doesnt look too promising; only 5 terms have positive coefficients, and the intercept is 4.16. But lets see how it goes. 5.2.2 Evaluating the first model Following Silge and Hvitfeldt (2020), well evaluate the model using cross-fold validation, which is a way of trying to squeeze as much validation as you can out of a finite dataset. We will resample our training dataset to create 10 new datasets, and in each one well use 90% for training and 10% for assessment. set.seed(1234) yelp_folds &lt;- vfold_cv(yelp_train) lasso_rs1 &lt;- fit_resamples( yelp_wf1 %&gt;% add_model(lasso_model1), yelp_folds, control = control_resamples(save_pred = TRUE) ) lasso_rs1 ## # Resampling results ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [6.3K/706]&gt; Fold01 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 4]&gt; ## 2 &lt;split [6.3K/706]&gt; Fold02 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 4]&gt; ## 3 &lt;split [6.3K/706]&gt; Fold03 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 4]&gt; ## 4 &lt;split [6.3K/705]&gt; Fold04 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 5 &lt;split [6.3K/705]&gt; Fold05 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 6 &lt;split [6.3K/705]&gt; Fold06 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 7 &lt;split [6.3K/705]&gt; Fold07 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 8 &lt;split [6.3K/705]&gt; Fold08 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 9 &lt;split [6.3K/705]&gt; Fold09 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; ## 10 &lt;split [6.3K/705]&gt; Fold10 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 4]&gt; Our \\(R^2\\) and RMSEs look really quite terrible: lasso_rs1 %&gt;% collect_metrics() ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.985 10 0.00732 Preprocessor1_Model1 ## 2 rsq standard 0.147 10 0.0122 Preprocessor1_Model1 And when we plot predictions vs. true values, that also looks quite terrible: lasso_rs1 %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred, color = id)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Each cross-validation fold is shown in a different color&quot; ) The model generally predicts that everything will have a star rating of between 3 and 5, and is especially poor at predicting lower values. Were now operating without much of a map, since the example in Silge and Hvitfeldt (2020) worked beautifully (predicting the year a USA Supreme Court decision was written based on its text). However, we can follow one of their last steps by tuning our lasso hyperparameters. 5.2.3 Tuning model parameters We can repeat the process but use model tuning to set the paramters in our lasso regression. Now instead of choosing a random lasso penalty of 0.1, were going to use the tune() function to figure out which penalty gives the best results on our training data. tune_model1 &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) tune_model1 ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet We create a grid of values to try: lambda_grid &lt;- grid_regular(penalty(), levels = 30) And now we use the function tune_grid() to fit our model at many different parameter values to see how they fare on our cross-fold validation set. Note: this takes a long time, 81.5 seconds for the 250-token model on my machine. set.seed(1234) tune_rs2 &lt;- tune_grid( yelp_wf1 %&gt;% add_model(tune_model1), yelp_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) tune_rs2 ## Warning: This tuning result has notes. Example notes on model fitting include: ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [6.3K/706]&gt; Fold01 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 2 &lt;split [6.3K/706]&gt; Fold02 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 3 &lt;split [6.3K/706]&gt; Fold03 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 4 &lt;split [6.3K/705]&gt; Fold04 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 5 &lt;split [6.3K/705]&gt; Fold05 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 6 &lt;split [6.3K/705]&gt; Fold06 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 7 &lt;split [6.3K/705]&gt; Fold07 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 8 &lt;split [6.3K/705]&gt; Fold08 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 9 &lt;split [6.3K/705]&gt; Fold09 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 10 &lt;split [6.3K/705]&gt; Fold10 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; We can visualize our lasso models performance for each parameter value: tune_rs2 %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err ), alpha = 0.5 ) + geom_line(size = 1.5) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10() + theme(legend.position = &quot;none&quot;) + labs( title = &quot;Lasso model performance across regularization penalties&quot;, subtitle = &quot;Performance metrics can be used to identity the best penalty&quot; ) ## Warning: Removed 2 row(s) containing missing values (geom_path). Since we want the best model performance possible, well follow Silge and Hvitfeldt (2020) and choose the value that minimizes our RMSE. tune_rs2 %&gt;% show_best(&quot;rmse&quot;) ## # A tibble: 5 x 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00853 rmse standard 0.903 10 0.0129 Preprocessor1_Model24 ## 2 0.00386 rmse standard 0.904 10 0.0133 Preprocessor1_Model23 ## 3 0.00174 rmse standard 0.905 10 0.0135 Preprocessor1_Model22 ## 4 0.000788 rmse standard 0.906 10 0.0136 Preprocessor1_Model21 ## 5 0.000356 rmse standard 0.907 10 0.0136 Preprocessor1_Model20 And we can extract the penalty that gives us the lowest RMSE using the select_best() function as follows: lowest_rmse &lt;- tune_rs2 %&gt;% select_best(&quot;rmse&quot;) And we can put it all together into a final workflow: final_lasso1 &lt;- finalize_workflow( yelp_wf1 %&gt;% add_model(tune_model1), lowest_rmse ) We can then do a final fit by testing our models predictions against our testing data using the following command. lasso_fit2 &lt;- final_lasso1 %&gt;% last_fit(split = yelp_split) And then we can extract its predictions and plot them against the true values to see how it looks. lasso_fit2 %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot; ) This model looks better in some ways but worse in others. Its better in that it gives lower predictions for in-truth lower reviews; its worse in that it predicts ratings over 5, and even over 6.5. The spread of predictions is also still quite large, but that may be to be expected with an \\(R^2\\) of only about 0.25. 5.3 Trying lasso again 5.3.1 With 1000 tokens Here is the whole process again in a single code block using 1000 tokens. num_tokens &lt;- 1000 set.seed(1234) # do initial split yelp_split &lt;- reviews_yelp %&gt;% initial_split(strata = &quot;rating_num&quot;) yelp_train &lt;- yelp_split %&gt;% training() yelp_test &lt;- yelp_split %&gt;% testing() # set up recipe yelp_rec &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) # set up our lasso model using tuning parameters tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) # create a grid of tuning parameters lambda_grid &lt;- grid_regular(penalty(), levels = 30) # create cross-validation folds set.seed(1234) yelp_folds &lt;- vfold_cv(yelp_train) # fit our model at many different parameter values using the cross-fold validation set set.seed(1234) tune_rs &lt;- tune_grid( yelp_wf %&gt;% add_model(tune_model), yelp_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) # extract penalty that gives us the lowest RMSE lowest_rmse &lt;- tune_rs %&gt;% select_best(&quot;rmse&quot;) # put it into a final workflow final_lasso &lt;- finalize_workflow( yelp_wf %&gt;% add_model(tune_model), lowest_rmse ) # do a last fit lasso_fit3 &lt;- final_lasso %&gt;% last_fit(split = yelp_split) # see the metrics lasso_fit3 %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.868 Preprocessor1_Model1 ## 2 rsq standard 0.373 Preprocessor1_Model1 # and plot it lasso_fit3 %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;All reviews, 1000 tokens&quot; ) This has an \\(R^2\\) of 0.37, which is a big improvement over the 250-token model, but its still nowhere near good enough to use in practice. 5.3.2 Short reviews only: &lt;125 words Lets try only using reviews under 125 words. Its possible that shorter reviews are denser and more to the point, and that longer reviews contain too much noise. This leaves us with 6,323 reviews. To begin with, Im going to define a function to run the lasso regression with different inputs. run_lasso &lt;- function(dataset, num_tokens){ set.seed(1234) data_split &lt;- dataset %&gt;% initial_split(strata = &quot;rating_num&quot;) data_train &lt;- data_split %&gt;% training() data_test &lt;- data_split %&gt;% testing() data_rec &lt;- recipe(rating_num ~ text, data = data_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) data_wf &lt;- workflow() %&gt;% add_recipe(data_rec) # set up our lasso model using tuning parameters tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) # create a grid of tuning parameters lambda_grid &lt;- grid_regular(penalty(), levels = 30) # create cross-validation folds set.seed(1234) data_folds &lt;- vfold_cv(data_train) # fit our model at many different parameter values using the cross-fold validation set set.seed(1234) tic() tune_rs &lt;- tune_grid( data_wf %&gt;% add_model(tune_model), data_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) toc() # extract penalty that gives us the lowest RMSE lowest_rmse &lt;- tune_rs %&gt;% select_best(&quot;rmse&quot;) # put it into a final workflow final_lasso &lt;- finalize_workflow( data_wf %&gt;% add_model(tune_model), lowest_rmse ) # do a last fit lasso_fit &lt;- final_lasso %&gt;% last_fit(split = data_split) return(lasso_fit) } Then we can use this function to easily run lasso regressions on different datasets. max_length &lt;- 125 min_length &lt;- 1 wordcounts_yelp &lt;- reviews_yelp %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% left_join (reviews_yelp %&gt;% rowid_to_column(), by =&quot;rowid&quot;) %&gt;% select(-rowid) reviews_yelp_short &lt;- wordcounts_yelp %&gt;% filter(n &lt;= max_length &amp; n &gt;= min_length ) lasso_results_short &lt;- run_lasso(dataset = reviews_yelp_short, num_tokens = 1000) # see the metrics lasso_results_short %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.821 Preprocessor1_Model1 ## 2 rsq standard 0.391 Preprocessor1_Model1 # and plot it lasso_results_short %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Short Reviews &lt; 125 Words, 1000 Tokens&quot; ) This gives us an \\(R^2\\) of 0.39, slightly better than our full dataset. But looking at the chart, we can see that this wont be useful in practice either. 5.3.3 Longer reviews &gt; 125 words For completeness, well also try only using the long reviews &gt; 125 words. Its possible that these reviews contain more useful information due to their length. This leaves us with 3,104 reviews. max_length &lt;- 10000 min_length &lt;- 125 wordcounts_yelp &lt;- reviews_yelp %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% left_join (reviews_yelp %&gt;% rowid_to_column(), by =&quot;rowid&quot;) %&gt;% select(-rowid) reviews_yelp_long &lt;- wordcounts_yelp %&gt;% filter(n &lt;= max_length &amp; n &gt;= min_length ) lasso_results_long &lt;- run_lasso(dataset = reviews_yelp_long, num_tokens = 1000) # see the metrics lasso_results_long %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.824 Preprocessor1_Model1 ## 2 rsq standard 0.429 Preprocessor1_Model1 # and plot it lasso_results_long %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Long Reviews &gt; 125 Words, 1000 Tokens&quot; ) Now our \\(R^2\\) has gone up to 0.43, so it is possible that the longer reviews do in fact contain more information. And looking at the chart, the cloud of points does creep measurably higher for each true star rating. However, Im still skeptical that this would be useful for predicting anything in practice. 5.4 Removing stop words Stop words are common words that contain little information on their own, like the and to. If using a bag-of-words approach, where youre not looking at the input text in a way that considers syntax (or, really, sentence-wise semantics) then it can be helpful to remove stop words. Here I will follow Silge and Hvitfeldt (2020) s SMLTAR s6.6 to try using three different sets of stopwords, to see which performs best on this dataset. First, they build a wrapper function to make it easy to build recipes with different stopword sets. stopword_rec &lt;- function(stopword_name) { recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text, stopword_source = stopword_name) %&gt;% step_tokenfilter(text, max_tokens = 1000) %&gt;% step_tfidf(text) } Next we set up a workflow that only has a model, using our tunable regularized regression model from before: tunable_wf &lt;- workflow() %&gt;% add_model(tune_model1) tunable_wf ## == Workflow ========================================================================================================================================================================================================================= ## Preprocessor: None ## Model: linear_reg() ## ## -- Model ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Now we will combine our functionized preprocessor with this tunable model and try three different stopword sets: snowball, smart, and stopwords-iso. This takes about 8 minutes on my machine. set.seed(1234) snowball_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;snowball&quot;)), yelp_folds, grid = lambda_grid ) set.seed(1234) smart_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;smart&quot;)), yelp_folds, grid = lambda_grid ) set.seed(1234) stopwords_iso_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;stopwords-iso&quot;)), yelp_folds, grid = lambda_grid ) And we plot their performance, using code straight from SMLTAR: word_counts &lt;- tibble(name = c(&quot;snowball&quot;, &quot;smart&quot;, &quot;stopwords-iso&quot;)) %&gt;% mutate(words = map_int(name, ~ length(stopwords::stopwords(source = .)))) list( snowball = snowball_rs, smart = smart_rs, `stopwords-iso` = stopwords_iso_rs ) %&gt;% map_dfr(show_best, &quot;rmse&quot;, .id = &quot;name&quot;) %&gt;% left_join(word_counts) %&gt;% mutate(name = paste0(name, &quot; (&quot;, words, &quot; words)&quot;)) %&gt;% ggplot(aes(fct_reorder(name, words), mean, color = name)) + geom_point(size = 3, alpha = 0.8, show.legend = FALSE) + labs( x = NULL, y = &quot;mean RMSE for five best models&quot;, title = &quot;Model performance for three stop word lexicons&quot;, subtitle = &quot;For this dataset, the Snowball lexicon performed best&quot; ) ## Joining, by = &quot;name&quot; The RMSE is marginally better using the snowball set of stopwords, but is still quite terrible! 5.5 Adjusting n-grams When tokenizing, we can in general consider text strings of any length. So far we have been considering one-word strings, which we could call unigrams. We could also consider two-word strings and three-word strings, called bigrams and trigrams respectively. We might expect using n-grams, where n&gt;1, to increase our accuracy because it will let us capture more of the syntactic information in our text. For example, if we only consider 1-grams then the short phrase Not bad! becomes not and bad, and our model has no way to differentiate between cases where they occur alone (which might be negative) and together (which might be positive). But if we also consider not bad, then the model might learn that that phrase is associated with positive reviews. As before, we follow SMLTAR s6.7 and set up a wrapper function that will let us easily change our model recipe to use different n-grams: ngram_rec &lt;- function(ngram_options) { recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text, token = &quot;ngrams&quot;, options = ngram_options) %&gt;% step_tokenfilter(text, max_tokens = 1e3) %&gt;% step_tfidf(text) } step_tokenize() takes two arguments, n for the highest-n n-grams to consider, and n_min for the lowest-n ngrams to consider. We will pass these values in the variable ngram_options. We then out these all together into a wrapper function that will let us run many different models easily: tune_ngram &lt;- function(ngram_options) { tune_grid( tunable_wf %&gt;% add_recipe(ngram_rec(ngram_options)), yelp_folds, grid = lambda_grid ) } We will try three cases, using n-grams where n=1, n=1,2, and n=1,2,3. Ive added tic()/toc() calls for loose benchmarking. The processing time goes up with each additional n-gram: 1-grams: 186s 2-grams: 267s 3-grams: 495s set.seed(123) unigram_rs &lt;- tune_ngram(list(n = 1)) set.seed(234) bigram_rs &lt;- tune_ngram(list(n = 2, n_min = 1)) set.seed(345) trigram_rs &lt;- tune_ngram(list(n = 3, n_min = 1)) And we can plot the results using a dot-plot, as per SMLTAR: list( `1` = unigram_rs, `1 and 2` = bigram_rs, `1, 2, and 3` = trigram_rs ) %&gt;% map_dfr(collect_metrics, .id = &quot;name&quot;) %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% ggplot(aes(name, mean, fill = name)) + geom_dotplot( binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, binpositions = &quot;all&quot;, show.legend = FALSE ) + labs( x = &quot;Degree of n-grams&quot;, y = &quot;mean RMSE&quot;, title = &quot;Model performance for different degrees of n-gram tokenization&quot;, subtitle = &quot;For the same number of tokens, unigrams alone performed best&quot; ) Amusingly, the fastest &amp; simplest approach of using only 1-grams worked best. 5.6 Full final regression After working through each piece of the regression preprocessing and recipe, well now followed SMLTAR s6.10s lead and put it all together. We will: Train on the cross-validation resamples; Tune both the lasso regularization parameter and the number of tokens used in the model; Only include unigrams; Remove the snowball stop words; And evaluate on the testing set. Here is our final recipe. note that we are using tune() as our max_tokens value. This will let us fit the model to a grid of values and see which one performs best. final_rec &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text, stopword_source = &quot;snowball&quot;) %&gt;% step_tokenfilter(text, max_tokens = tune()) %&gt;% step_tfidf(text) final_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Tokenization for text ## Stop word removal for text ## Text filtering for text ## Term frequency-inverse document frequency with text Then we specify our model again: tune_model &lt;- linear_reg( penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) tune_model ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Then we set up our workflow: tune_wf &lt;- workflow() %&gt;% add_recipe(final_rec) %&gt;% add_model(tune_model) tune_wf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ----------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Next well tune the model. To do so, we need to choose the set of parameter values for the penalty and number of tokens well test. We do this by setting up a grid of the value combinations using grid_regular(). With 20 steps for the penalty and with 6 steps for the tokens, well have 120 combinations to test in total. This took 2180s on my machine. final_grid &lt;- grid_regular( penalty(range = c(-4,0)), max_tokens(range = c(1e3, 6e3)), levels = c(penalty = 20, max_tokens = 6) ) final_grid %&gt;% head(10) ## # A tibble: 10 x 2 ## penalty max_tokens ## &lt;dbl&gt; &lt;int&gt; ## 1 0.0001 1000 ## 2 0.000162 1000 ## 3 0.000264 1000 ## 4 0.000428 1000 ## 5 0.000695 1000 ## 6 0.00113 1000 ## 7 0.00183 1000 ## 8 0.00298 1000 ## 9 0.00483 1000 ## 10 0.00785 1000 Next we train our models using the tuning grid: final_rs &lt;- tune_grid( tune_wf, yelp_folds, grid = final_grid, metrics = metric_set(rmse, mae, mape) ) Now we can plot each models performance for the different numbers of tokens and regularization penalties. We see the familiar dip-shaped graph we expect in lasso regularization but the dips are much more pronounced for larger token numbers, suggesting that regularization is much more important as we use more tokens. Also note that the best performance happens with an intermediate number of tokens: for some reason, model performace gets worse on this dataset if you use more than 3000 tokens. final_rs %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = as.factor(max_tokens))) + geom_line(size = 1.5, alpha = 0.5) + geom_point(size = 2, alpha = 0.9) + facet_wrap(~.metric, scales = &quot;free_y&quot;) + scale_x_log10() + labs( color = &quot;Number of tokens&quot;, title = &quot;Lasso model performance across regularization penalties and number of tokens&quot;, subtitle = &quot;The best model includes a high number of tokens but also significant regularization&quot; ) We can extract the lowest MAE value from our models: lowest_mae &lt;- final_rs %&gt;% select_best(&quot;mae&quot;) lowest_mae ## # A tibble: 1 x 3 ## penalty max_tokens .config ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.0127 3000 Preprocessor3_Model11 And then we can use this value to set a final workflow: final_wf &lt;- finalize_workflow( tune_wf, lowest_mae ) final_wf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ----------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.0127427498570313 ## mixture = 1 ## ## Computational engine: glmnet Which we can then use to do one last final fit and view its metrics: final_fitted &lt;- last_fit(final_wf, yelp_split) collect_metrics(final_fitted) ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.845 Preprocessor1_Model1 ## 2 rsq standard 0.406 Preprocessor1_Model1 This plot uses the vip package to extract the most important positive and negative terms, so we can see what our lasso regression is picking up on. Overall, the terms look kind of random. I would have expected words like delicious, great, and awesome to have been strongly correlated with positive reviews, and so Im not what to make of the fact that talked, sounded, and dipped are the top three most-associated-with-positive-review words. The negative words look a bit betterunfortunate is #1 and worst is #3but there are still some head-scratchers, like 2.50 and striploin. (Although if you spend $2.50 on a striploin you have no one to blame but yourself.) library(vip) imp &lt;- pull_workflow_fit(final_fitted$.workflow[[1]]) %&gt;% vi(lambda = lowest_mae$penalty) imp %&gt;% mutate( Sign = case_when( Sign == &quot;POS&quot; ~ &quot;Better&quot;, Sign == &quot;NEG&quot; ~ &quot;Worse&quot;, ), Importance = abs(Importance), Variable = str_remove_all(Variable, &quot;tfidf_text_&quot;) ) %&gt;% group_by(Sign) %&gt;% top_n(20, Importance) %&gt;% ungroup() %&gt;% ggplot(aes( x = Importance, y = fct_reorder(Variable, Importance), fill = Sign )) + geom_col(show.legend = FALSE) + scale_x_continuous(expand = c(0, 0)) + facet_wrap(~Sign, scales = &quot;free&quot;) + labs( y = NULL, title = &quot;Variable importance for predicting Yelp review star ratings&quot; ) Finally, we can again plot our final lasso models predicted ratings vs. the actual ratings to see how they compare. There is a definite improvement from the first model, but the results ultimately still arent workable. The range of predictions is still much too wide, and true lower reviews are still predicted as much too high. final_fitted %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(lty = 2, color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted year&quot;, title = &quot;Predicted and true ratings for Yelp Reviews&quot; ) 5.7 Conclusion In this section I followed Silge and Hvitfeldt (2020) s recipe and tried to predict a Yelp reviews star rating from its text using a lasso regression model. I varied a number of parameters, including the lasso regularization penalty, the number of tokens used in the model, the number and type of n-grams, and the lengths of the reviews. Although the models accuracy did improve as I refined them, none of the models were especially effective and none come close to being workable in practice. There are at least two possibilities: The problem might be with the dataset. The dataset may be too small, or too imbalanced (there are far fewer negative reviews than positive reviews), or have some other deficiency that makes it unsuitable for lasso regression. Linear regression may not be the right tool for the job. Given the relatively small number of discrete rating categories, this might be better modeled as a classification problem. We will look at both of these possibilities in subsequent entries. References "],["yelp-classification-and-sentiment-test.html", "Chapter 6 Yelp Classification and Sentiment Test 6.1 Yelp Dataset 6.2 Kaggle Yelp dataset 6.3 NEXT STEPS 6.4 SessionInfo", " Chapter 6 Yelp Classification and Sentiment Test This notebook outlines my efforts to build a classification model that can predict Yelp star ratings based on Yelp review text. My previous attempts used linear regression to predict star rating as a real-valued function of input text. In this notebook, I will instead approach prediction as a classification problem and try to predict star ratings as discrete factors. Intead of trying to predict exact star ratings, I will follow standard practice and divide ratings into positive (POS) and negative (NEG) reviews. As Liu (2015) notes, Sentiment classification is usually formulated as a two-class classification problem: positive and negative A review with 4 or 5 stars is considered a positive review, and a review with 1 to 2 stars is considered a negative review. Most research papers do not use the neutral class (3-star ratings) to make the classification problem easier (49). But if the results are good, we can always experiment with three- or five-class problems. A note on sourcing: My analysis here will closely follow the examples in Silge and Hvitfeldt (2020) (which I will often refer to as SMLTAR, for Supervised Machine Learning and Text Analysis in R) and Silge and Robinson (2020). In some cases I have used examples or hints from websites like Stack Overflow, and Ive noted that where applicable. A note on aesthetics: in the interest of time I havent piped my outputs through kable(). Most outputs are straight console printouts. 6.1 Yelp Dataset Lets begin with the Yelp dataset I collected. As a reminder, this dataset was collected in October 2020 and has 9,402 reviews for restaurants in Ottawa. Reviews were overwhelmingly positive, as can be seen in the following histogram. reviews_gr &lt;- read_csv(&quot;../tests/data/goodreads_all.csv&quot;) reviews_mec &lt;- read_csv(&quot;../tests/data/mec-reviews.csv&quot;) reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% rename(rating_num = rating) reviews_yelp %&gt;% ggplot(aes(x=rating_num)) + geom_histogram(bins=5) + labs(title = &quot;Small Yelp Dataset: Histogram of Star Ratings (n=9,402)&quot;, x = &quot;Star Rating&quot;, y = &quot;Count&quot;) The dataset is quite imbalanced: nearly 79% of reviews give 4 or 5 stars, our only about 9% give 1 or 2 stars. As we will see, this will create problems for our modeling. reviews_yelp %&gt;% group_by(rating_num) %&gt;% summarise(n = n()) %&gt;% mutate(pct = n/sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## rating_num n pct ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 343 0.0365 ## 2 2 510 0.0542 ## 3 3 1077 0.115 ## 4 4 2958 0.315 ## 5 5 4514 0.480 6.1.1 AFINN AFINN is a dictionary-based one-dimensional sentiment model that gives texts an integer score for how positive or negative they are. It treats texts as a bag of words, which means it does not consider any syntax or semantics beyond the values given in its dictionary. Each word in a text is given a pre-determined positive or negative score, and those scores are summed to give an overall rating for a text. For example, here are the AFINN scores for the top 5 positive words. Strongly negative words are generally NSFW and so I wont print them here. afinn %&gt;% arrange(desc(value)) %&gt;% head(5) ## # A tibble: 5 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 breathtaking 5 ## 2 hurrah 5 ## 3 outstanding 5 ## 4 superb 5 ## 5 thrilled 5 Following the Tidytext method from Silge &amp; Robinson, we get an AFINN score for each Yelp review: afinn_yelp &lt;- reviews_yelp %&gt;% select(comment, rating_num) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T), rating_num = mean(rating_num)) We can make a boxplot to show the distribution of AFINN scores for reviews grouped by star rating. This actually looks moderately promising, since we can see that higher star ratings seem to be associated with somewhat higher AFINN scores. afinn_yelp %&gt;% mutate(rating_num = as.factor(rating_num)) %&gt;% ggplot(aes(x = rating_num, y=afinn_sent)) + geom_boxplot() + geom_smooth(method=&quot;lm&quot;) + labs( title = &quot;AFINN Scores by Star Rating&quot;, subtitle = &quot;Small Yelp dataset (n=9402)&quot;, x = &quot;Star Rating&quot;, y = &quot;AFINN Sentiment Score&quot; ) 6.1.2 Classification: Naive Bayes Classifier To approach this as classification problem, we will divide reviews into two groups: positive (&gt;3 stars) and negative (&lt;3 stars). factor_yelp &lt;- reviews_yelp %&gt;% bind_cols(afinn_yelp %&gt;% select(afinn_sent)) %&gt;% filter(rating_num != 3) %&gt;% mutate(rating_factor = case_when( rating_num &lt;3 ~ &quot;NEG&quot;, rating_num &gt;3 ~ &quot;POS&quot;), rating_factor = as.factor(rating_factor)) #factor_yelp Here well follow SMLTAR Ch 7 very closely and set up a naive Bayes classifier that takes AFINN sentiment as its only input and predicts positive or negative sentiment as its only output. The code here follows SMLTAR very closely except where otherwise specified. Note that SMLTAR actually uses the text itself, and not a real-valued variable like AFINN sentiment; we can try this next. First we set up testing and training split: set.seed(1234) yelp_split &lt;- initial_split(factor_yelp, strata = rating_factor) yelp_test &lt;- testing(yelp_split) yelp_train &lt;- training(yelp_split) Then we set up a recipe, set up a workflow, specify a naive Bayes model, and fit this model to our training data: yelp_rec &lt;- recipe(rating_factor ~ afinn_sent, data = yelp_train) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) nb_spec &lt;- naive_Bayes() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;naivebayes&quot;) nb_fit &lt;- yelp_wf %&gt;% add_model(nb_spec) %&gt;% fit(data = yelp_train) nb_fit ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: naive_Bayes() ## ## -- Preprocessor ---------------------------------------------------------------- ## 0 Recipe Steps ## ## -- Model ----------------------------------------------------------------------- ## ## ======================================================================================== Naive Bayes ========================================================================================= ## ## Call: ## naive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE) ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Laplace smoothing: 0 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## A priori probabilities: ## ## NEG POS ## 0.1024984 0.8975016 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Tables: ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::NEG (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (640 obs.); Bandwidth &#39;bw&#39; = 1.66 ## ## x y ## Min. :-32.98 Min. :0.0000012 ## 1st Qu.:-11.74 1st Qu.:0.0003679 ## Median : 9.50 Median :0.0035701 ## Mean : 9.50 Mean :0.0117586 ## 3rd Qu.: 30.74 3rd Qu.:0.0137708 ## Max. : 51.98 Max. :0.0584066 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::POS (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (5604 obs.); Bandwidth &#39;bw&#39; = 1.195 ## ## x y ## Min. :-11.59 Min. :7.100e-07 ## 1st Qu.: 17.96 1st Qu.:1.262e-04 ## ## ... ## and 7 more lines. We will use resampling to evaluate the model, again with 10 cross-fold validation sets. yelp_folds &lt;- vfold_cv(yelp_train) nb_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) %&gt;% add_model(nb_spec) nb_rs &lt;- fit_resamples( nb_wf, yelp_folds, control = control_resamples(save_pred = TRUE) ) nb_rs_metrics &lt;- collect_metrics(nb_rs) nb_rs_predictions &lt;- collect_predictions(nb_rs) Lets see the fit metrics: nb_rs_metrics ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.914 10 0.00283 ## 2 roc_auc binary 0.784 10 0.0121 We can also plot an ROC curve, which is supposed to show a models accuracy and how well a model trades off false positives and false negatives. Better models are associated with curves that bend farther away from the line y=x (citation needed). According to the standard story about ROC curves, this looks okay. nb_rs_predictions %&gt;% group_by(id) %&gt;% roc_curve(truth = rating_factor, .pred_NEG) %&gt;% autoplot() + labs( color = NULL, title = &quot;Receiver operator curve for small Yelp dataset&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) We can also look at a heat map and a confusion matrix to see how often the model was correct and incorrect. nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) ## Truth ## Prediction NEG POS ## NEG 18 2 ## POS 42 563 But looking at the confusion matrix shows a problem: there are so many fewer true NEG cases that our models performance doesnt mean much. The Bayes classifier achieved ~91.4% accuracy, but since ~89.7% of the data is classified as POS we could get nearly as much accuracy by just guessing POS in each case. The data is heavily unbalanced. factor_yelp %&gt;% group_by(rating_factor) %&gt;% summarise(n = n()) %&gt;% mutate(pct = n/sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## rating_factor n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NEG 853 0.102 ## 2 POS 7472 0.898 We need to balance our dataset so that there is a roughly equal number of positive and negative reviews. The easiest way is by downsampling, where you remove items from the larger set until you have two sets of about the same size. But to get a balanced dataset we would need to throw away nearly 80% of our data, and since our dataset is somewhat small we might not have enough to work with. TODO cite SMLTAR or Text Mining with R. There are more sophisticated balancing approaches that are out of scope here, but the easiest approach for our puposes is to find a much larger public dataset to work with. 6.2 Kaggle Yelp dataset Yelp makes a huge dataset available for teaching and research at this link through Kaggle. A larger dataset will probably help us build a better model, especially if we need to balance our datasets to have roughly equal numbers of positive and negative reviews. The dataset is enormous: it has around 6 gigabytes of review text and around 5 million reviews. This is too big to load using conventional methods on my machine. After a few failures, I found a discussion on StackOverflow that helped me read just the first n lines from the jsonLine file and parse them. For the present, well read the first 100k reviews: # figure out how to do it reading between the lines of this stackoverflow: # https://stackoverflow.com/questions/53277351/read-first-1000-lines-from-very-big-json-lines-file-r yelp_big &lt;- readLines(&quot;../tests/data/yelp_academic_dataset_review.json&quot;, n = 100000) %&gt;% textConnection() %&gt;% jsonlite::stream_in(verbose=FALSE) yelp_big &lt;- yelp_big %&gt;% select(stars, text) And plot a histogram of the star distributions. The star distributions look very similar to the data I collected manually, but with a slight spike at 1 that we didnt find in my Yelp data. We did find this 1-spike in the MEC data, so there may be a common review phenomenon here. yelp_big %&gt;% ggplot(aes(x=stars)) + geom_histogram(bins=5) + labs(title = &quot;Large Yelp Dataset: Histogram of Star Ratings (n=100,000)&quot;) Lets classify the reviews into NEG and POS again, once more classifying reviews with fewer than 3 stars as negative, more than 3 stars as positive, and discarding reviews with 3 stars. yelp_big_factor &lt;- yelp_big %&gt;% mutate(rating_factor = case_when( stars &lt; 3 ~ &quot;NEG&quot;, stars &gt; 3 ~ &quot;POS&quot;) %&gt;% as.factor() ) %&gt;% select(-stars) %&gt;% drop_na() yelp_big_factor %&gt;% summary() ## text rating_factor ## Length:88821 NEG:21928 ## Class :character POS:66893 ## Mode :character This dataset is quite imbalanced: there are ~67k positive reviews and ~22 negative reviews. Since classification engines can have trouble with unbalanced sets, we will downsample our dataset by randomly removing some positive reviews so that we have around the same number of negatvie and positive reviews. This new balanced dataset will have ~22k positive and negative reviews, still far more than we had in the dataset I collected myself. set.seed(1234) yelp_balanced &lt;- yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% bind_rows(yelp_big_factor%&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n=yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% nrow() )) yelp_balanced %&gt;% summary() ## text rating_factor ## Length:43856 NEG:21928 ## Class :character POS:21928 ## Mode :character Lets try AFINN again on the balanced set. First well get the AFINN sentiments for all our reviews. tic() afinn_yelp_big &lt;- yelp_balanced %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T)) toc() ## 9.96 sec elapsed yelp_big_bal_afinn &lt;- afinn_yelp_big %&gt;% left_join(yelp_balanced %&gt;% rowid_to_column()) %&gt;% select(-rowid) And we can make a boxplot of the AFINN distributions for POS and NEG reviews. There is enough difference between the POS and NEG reviews that this looks like it might plausibly work. yelp_big_bal_afinn %&gt;% ggplot(aes(x=rating_factor,y=afinn_sent)) + geom_boxplot() + labs( title = &quot;AFINN Scores by Star Rating&quot;, subtitle = paste0(&quot;Big Yelp dataset (n=&quot;,nrow(yelp_big_bal_afinn),&quot;)&quot;), x = &quot;Star Rating&quot;, y = &quot;AFINN Sentiment Score&quot; ) And for another view, heres a density plot: yelp_big_bal_afinn %&gt;% ggplot(aes(x=afinn_sent, fill=rating_factor)) + geom_density(alpha=0.5) + labs(title = &quot;Density Distributions of AFINN Sentiment for POS and NEG Reviews&quot;, subtitle = &quot;Large Balanced Yelp Dataset, n=43,855&quot;, x = &quot;AFINN Sentiment&quot;, y =&quot;Density&quot;) 6.2.1 Naive Bayes Classifier We will again go through the tidymodels process of setting up a naive Bayes classifier. First we do a test/train split of our large balanced dataset. set.seed(1234) yelp_split &lt;- initial_split(yelp_big_bal_afinn, strata = rating_factor) yelp_test &lt;- testing(yelp_split) yelp_train &lt;- training(yelp_split) Then we set up a recipe, a naive Bayes model, and a workflow, and then fit our model to our training data. yelp_rec &lt;- recipe(rating_factor ~ afinn_sent, data = yelp_train) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) nb_spec &lt;- naive_Bayes() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;naivebayes&quot;) nb_fit &lt;- yelp_wf %&gt;% add_model(nb_spec) %&gt;% fit(data = yelp_train) nb_fit ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: naive_Bayes() ## ## -- Preprocessor ---------------------------------------------------------------- ## 0 Recipe Steps ## ## -- Model ----------------------------------------------------------------------- ## ## ======================================================================================== Naive Bayes ========================================================================================= ## ## Call: ## naive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE) ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Laplace smoothing: 0 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## A priori probabilities: ## ## NEG POS ## 0.5 0.5 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Tables: ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::NEG (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (16446 obs.); Bandwidth &#39;bw&#39; = 0.7709 ## ## x y ## Min. :-64.31 Min. :0.000e+00 ## 1st Qu.:-29.41 1st Qu.:2.549e-05 ## Median : 5.50 Median :2.295e-04 ## Mean : 5.50 Mean :7.155e-03 ## 3rd Qu.: 40.41 3rd Qu.:3.387e-03 ## Max. : 75.31 Max. :7.088e-02 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::POS (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (16446 obs.); Bandwidth &#39;bw&#39; = 0.9637 ## ## x y ## Min. :-33.891 Min. :0.0000000 ## 1st Qu.: 2.054 1st Qu.:0.0000199 ## ## ... ## and 7 more lines. Then we use resampling to evaluate the model, again with 10 cross-fold validation sets. yelp_folds &lt;- vfold_cv(yelp_train) nb_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) %&gt;% add_model(nb_spec) nb_rs &lt;- fit_resamples( nb_wf, yelp_folds, control = control_resamples(save_pred = TRUE) ) nb_rs_metrics &lt;- collect_metrics(nb_rs) nb_rs_predictions &lt;- collect_predictions(nb_rs) Lets see the fit metrics. Our accuracy is ~78.7%, which is quite a bit better than chance so there is good evidence that the model is getting something right. # create a character a vector with the accuracy % that we can use in the text later nb_acc &lt;- nb_rs_metrics %&gt;% pull(mean) %&gt;% head(1) %&gt;% round(3) %&gt;% `*`(100) %&gt;% paste0(&quot;%&quot;,.) # print out the metrics nb_rs_metrics ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.787 10 0.00188 ## 2 roc_auc binary 0.854 10 0.00167 We can also look at the ROC curve, which again shows some good performance: nb_rs_predictions %&gt;% group_by(id) %&gt;% roc_curve(truth = rating_factor, .pred_NEG) %&gt;% autoplot() + labs( color = NULL, title = &quot;Receiver operator curve for big balanced Yelp dataset, AFINN sentiment&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) And a confusion matrix: nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) ## Truth ## Prediction NEG POS ## NEG 1195 230 ## POS 448 1417 Our naive Bayes classifier did quite a bit better than chance on our balanced dataset. We would have expected about 50% accuracy by chance, and it was accurate %78.7 of the time on our training data. 6.2.2 Logistic Regression Its also worth trying a logistic regression, for at least two reasons: Its good practice; and Its a simple and common model. For the code here, I referred to this website to remind me of the basics of doing logistic regression in R. I elected not to do it in a tidymodels framework. Well use the same big balanced dataset. First well split our data into testing and training: index &lt;- sample(c(T,F), size = nrow(yelp_big_bal_afinn), replace = T, prob=c(0.75,0.25)) train &lt;- yelp_big_bal_afinn[index,] test &lt;- yelp_big_bal_afinn[!index,] Then well use glm() to run a simple logistic regression, predicting the rating factor based on the AFINN sentiment score. Here is the model output: logit &lt;- glm(data= train, formula= rating_factor ~ afinn_sent, family=&quot;binomial&quot;) summary(logit) ## ## Call: ## glm(formula = rating_factor ~ afinn_sent, family = &quot;binomial&quot;, ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6936 -0.7363 0.0076 0.8166 3.7598 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.166717 0.018845 -61.91 &lt;2e-16 *** ## afinn_sent 0.190342 0.002265 84.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45641 on 32922 degrees of freedom ## Residual deviance: 32490 on 32921 degrees of freedom ## AIC: 32494 ## ## Number of Fisher Scoring iterations: 5 Our results are strongly significant, so we have some reason to take this model seriously. Referring to this website for more pointers, we can use our logistic regression results to predict rating scores for our test dataset. The simplest way to do this is to say that we predict whichever outcome the model says is more likely. In other words, if a review has a predicted probability &gt;0.5 of being positive, then we predict its positive. How accurate would we be? pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) %&gt;% summarise(accuracy = sum(correct) / nrow(.)) logit_acc &lt;- test_results %&gt;% `*`(100) %&gt;% round(3) %&gt;% paste0(&quot;%&quot;,.) For this data, a simple logistic regression was only a little bit less accurate than the naive Bayes classifier: %76.848, as opposed to %78.7. 6.3 NEXT STEPS Consider another sentiment-detection algorithm / dictionary. Naive Bayes classifier based on review text, intead of AFINN sentiment score. Consider review length as a tuning paramter. 6.4 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] vip_0.2.2 glmnet_4.0-2 Matrix_1.2-18 ggridges_0.5.2 discrim_0.1.1 tictoc_1.0 textrecipes_0.3.0 lubridate_1.7.9 yardstick_0.0.7 workflows_0.2.0 ## [11] tune_0.1.1 rsample_0.0.8 recipes_0.1.13 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 ## [21] tidytext_0.2.5 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 SnowballC_0.7.0 ## [10] prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 pROC_1.16.2 packrat_0.5.0 ## [19] dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 htmltools_0.5.0 tools_4.0.2 gtable_0.3.0 ## [28] glue_1.4.1 naivebayes_0.9.7 rappdirs_0.3.1 Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 iterators_1.0.12 timeDate_3043.102 ## [37] gower_0.2.2 xfun_0.16 stopwords_2.0 globals_0.13.0 rvest_0.3.6 lifecycle_0.2.0 future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 ## [46] hms_0.5.3 parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 ## [55] textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 lava_1.6.8 rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 ## [64] labeling_0.3 tidyselect_1.1.0 plyr_1.8.6 magrittr_1.5 bookdown_0.20 R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.6 ## [73] haven_2.3.1 withr_2.2.0 survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 modelr_0.1.8 crayon_1.3.4 utf8_1.1.4 rmarkdown_2.3 ## [82] usethis_1.6.1 grid_4.0.2 readxl_1.3.1 blob_1.2.1 reprex_0.3.0 digest_0.6.25 webshot_0.5.2 munsell_0.5.0 GPfit_1.0-8 ## [91] viridisLite_0.3.0 kableExtra_1.1.0 References "],["the-mvp-classification-accuracy-as-a-function-of-review-length-and-volume.html", "Chapter 7 The MVP: Classification Accuracy as a Function of Review Length and Volume 7.1 Introduction 7.2 Preparing the Data 7.3 Experiment 1: Logistic Regression on Globally Balanced Data 7.4 Experiment 2: Logistic Regression on Micro-Balanced Data 7.5 Conclusions 7.6 Next Steps 7.7 SessionInfo 7.8 References", " Chapter 7 The MVP: Classification Accuracy as a Function of Review Length and Volume 7.1 Introduction In this notebook Im trying to build a minimum viable project (MVP) that answers the following research questions: RQ1: When using a dataset to build a model to predict review ratings based on review sentiment, how does accuracy vary with the number of reviews (volume)? RQ2: When using a dataset to build a model to predict review ratings based on review sentiment, how does accuracy vary with the word length of those reviews? Building on the last section, I will use a logistic regression to create a classification model to predict Yelp reviews star ratings based on their sentiment as measured by AFINN. I will divide ratings into positive (POS) and negative (NEG) reviews, again following Liu (2015)s recommendation, and use the approaches outlined in Silge and Hvitfeldt (2020) and Silge and Robinson (2020). In some cases I have used examples or hints from websites like Stack Overflow, and Ive noted that where applicable. 7.2 Preparing the Data I will again work with the large Yelp dataset available at this link, this time loading the first 500k reviews: # figure out how to do it reading between the lines of this stackoverflow: # https://stackoverflow.com/questions/53277351/read-first-1000-lines-from-very-big-json-lines-file-r yelp_big &lt;- readLines(&quot;../tests/data/yelp_academic_dataset_review.json&quot;, n = 500000) %&gt;% textConnection() %&gt;% jsonlite::stream_in(verbose=FALSE) yelp_big &lt;- yelp_big %&gt;% select(stars, text) Plotting a histogram in Figure 7.1, we see the now-familiar distribution of a slight bump at 1 star followed by an exponential increase towards 5 stars. yelp_big %&gt;% ggplot(aes(x=stars)) + geom_histogram(bins=5) + labs( title = paste0(&quot;Large Yelp Dataset (n=&quot;,nrow(yelp_big),&quot;)&quot;), x = &quot;Stars&quot;, y = &quot;Count&quot;) + theme_minimal() Figure 7.1: Histogram of star ratings for the large Yelp dataset. Lets classify the reviews into NEG and POS again, once more classifying reviews with fewer than 3 stars as negative, more than 3 stars as positive, and discarding reviews with 3 stars. yelp_big_factor &lt;- yelp_big %&gt;% mutate(rating_factor = case_when( stars &lt; 3 ~ &quot;NEG&quot;, stars &gt; 3 ~ &quot;POS&quot;) %&gt;% as.factor() ) %&gt;% select(-stars) %&gt;% drop_na() yelp_big_factor %&gt;% summary() ## text rating_factor ## Length:444222 NEG:111045 ## Class :character POS:333177 ## Mode :character Since we found that classification didnt work well with an unbalanced dataset, we will downsample the dataset so that we have the same number of positive and negative reviews. set.seed(1234) yelp_balanced &lt;- yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% bind_rows(yelp_big_factor%&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n=yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% nrow() )) yelp_balanced %&gt;% summary() ## text rating_factor ## Length:222090 NEG:111045 ## Class :character POS:111045 ## Mode :character Lets try AFINN again on the balanced set. First well get the AFINN sentiments for all our reviews. tic() afinn_yelp_big &lt;- yelp_balanced %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T)) toc() ## 71.83 sec elapsed yelp_big_bal_afinn &lt;- afinn_yelp_big %&gt;% left_join(yelp_balanced %&gt;% rowid_to_column()) %&gt;% select(-rowid) The density plot in Figure 7.2 shows that NEG and POS reviews still have overlapping but different distributions in this dataset, which suggests that our model might reasonably be able to tell them apart. yelp_big_bal_afinn %&gt;% ggplot(aes(x=afinn_sent, fill=rating_factor)) + geom_density(alpha=0.5) + labs(#title = &quot;Density Distributions of AFINN Sentiment for POS and NEG Reviews&quot;, title = paste0(&quot;Large Balanced Yelp Dataset (n=&quot;,nrow(yelp_big_bal_afinn),&quot;)&quot;), x = &quot;AFINN Sentiment&quot;, y =&quot;Density&quot;, fill=&quot;Sentiment&quot;) + theme_minimal() Figure 7.2: Density Distributions of AFINN Sentiment for POS and NEG Reviews. We will now compute the word length for each review so we can see how review length affects our predictions. As we can see in Figure 7.3, most of our reviews are quite shortroughly 200,0000 are under 250 wordsbut a few extend beyond 1000 words. wordcounts_yp &lt;- yelp_big_bal_afinn %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_yp %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =paste0(&quot;Large Yelp Dataset (n=&quot;,nrow(yelp_big_bal_afinn),&quot;)&quot;), #: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) Figure 7.3: Large Yelp Dataset: Cumulative distribution of word lengths. Next well join the word-length column to our balanced Yelp dataset, completing the pre-processing. yelp_data &lt;- bind_cols( yelp_big_bal_afinn, wordcounts_yp %&gt;% arrange(rowid) %&gt;% select(words = n) ) 7.3 Experiment 1: Logistic Regression on Globally Balanced Data In this section we will look at how review length and volume affect classification accuracy using a logistic regression based on review sentiment. I will divide the data into \\(n\\) non-overlapping subsets based on their lengths, and then I will divide those subsets into \\(n\\) overlapping subsets of increased size, and then will run a logistic regression on each of these latter subsets. The output will be an \\(n\\times n\\) matrix plotted as a heat map where each cell represents model accuracy for a given number of reviews with lengths within a given range. More precisely, here are the steps I will follow: Choose a number of quantiles \\(n\\), and divide reviews into \\(n\\) quantiles by word length. Find how many reviews are in each quantile. Take the smallest total number of reviews \\(mintotal\\): for comparability, this is the largest number of reviews we will consider. Within each quantile, consider \\(n\\) overlapping subsets of increasing size ranging from \\(mintotal/n\\) to \\(mintotal\\). For each quantile, for each group of reviews, run a logistic regression to predict review ratings and log its accuracy. After some initial experimentation, Ive chosen to use \\(n=5\\) quantiles since it gives us a good number of subsets of reasonable size. First, we set up a function to run a logistic regression on an arbitrary dataset and return the prediction accuracy. This is a functionized version of the code I used earlier. do_logit &lt;- function (dataset) { # for a train/test split: get a random vector as long as our dataset that is 75% TRUE and 25% FALSEe. index &lt;- sample(c(T,F), size = nrow(dataset), replace = T, prob=c(0.75,0.25)) # extract train and test datasets by indexing our dataset using our random index train &lt;- dataset[index,] test &lt;- dataset[!index,] # use `glm()` to run a logistic regression predicting the rating factor based on the AFINN score. logit &lt;- glm(data= train, formula= rating_factor ~ afinn_sent, family=&quot;binomial&quot;) pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) # now predict the outcome based on whichever has the greater probability, find out if each prediction is correct, and compute the accuracy test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) %&gt;% summarise(accuracy = sum(correct) / nrow(.)) %&gt;% unlist() return (test_results) } Its not quite tidy, but we can run this analysis easily with two nested for loops. Here I break the reviews into 5 quantiles by word length, and then break each quantile down into 5 overlapping subsets of increasing length. # for reproducibility, set the random number generator seed set.seed(1234) # how many quantiles? num_qtiles &lt;- 5 # get the limits of the word-quantiles for display purposes qtiles &lt;- quantile(yelp_data$words, probs = seq(0, 1, (1/num_qtiles))) # find the word-quantile for each review using the fabricatr::split_quantile() function yelp_data &lt;- yelp_data %&gt;% mutate(qtile = fabricatr::split_quantile(words, type=num_qtiles)) # get the number of reviews in the smallest quantile. # we&#39;re going to use this to compare groups of the same/similar size. minn &lt;- yelp_data %&gt;% group_by(qtile) %&gt;% summarise(n = n()) %&gt;% summarise(minn = min(n)) %&gt;% unlist() # set up an empty results tibble. results &lt;- tibble() # boolean flag: will we print updates to the console? # I used this for testing but it should be disabled in the final knit verbose &lt;- FALSE tic() # Consider each quantile of review word lengths one at a time for (word_qtile in 1:num_qtiles){ # within each quantile of reviews broken down by length, consider several different numbers of reviews for (num_qtile in 1:num_qtiles){ # number of reviews we will consider in this iteration. num_reviews &lt;- num_qtile * minn/num_qtiles # message for me to keep track if (verbose == TRUE) { message (paste0(&quot;Considering &quot;, num_reviews, &quot; reviews with word length in the range (&quot;,qtiles[[word_qtile]],&quot;,&quot;,qtiles[[word_qtile+1]],&quot;)&quot;)) } # filter the rows we want: the right number of words, and the right number of reviews, then run a logistic regression on them data_for_logit &lt;- yelp_data %&gt;% filter(qtile==word_qtile) %&gt;% slice_sample(n = num_reviews) # get true percentage of positives, so we can look at sample balance pct_true_pos &lt;- data_for_logit %&gt;% summarise(n = sum(rating_factor == &quot;POS&quot;) / nrow(.)) %&gt;% unlist() # run the logistic regression on our data result &lt;- data_for_logit %&gt;% do_logit() # add our result to our results tibble. this wouldn&#39;t be best practice for thousands of rows, but it&#39;s fine here. results &lt;- bind_rows( results, tibble(word_qtile = word_qtile, num_qtile = num_qtile, accuracy = result, pct_true_pos = pct_true_pos) ) } } toc() ## 2.75 sec elapsed The code runs quickly (&lt;5s on my machine) and gives some interesting-looking results shown below in Figure 7.4. First, all of the accuracy metrics are quite high: our success rates ranged from around 80% to 86%. But interestingly, it looks like we get better results from shorter reviews! results %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=accuracy)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (1:num_qtiles * minn/num_qtiles)) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;Accuracy&quot;) Figure 7.4: Heat map of logistic regression prediction accuracy for the large balanced Yelp dataset. However, before drawing conclusions we should look more closely at the data. As we can see below in Figure 7.5, there is a big difference in each quantiles true positive rate. And based just on visual inspection, it looks like higher true positive rates in Figure 7.5 are correlated with higher prediction accuracy rates in Figure 7.4. results %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=pct_true_pos)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (1:num_qtiles * minn/num_qtiles)) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;True Positive Rate&quot;) Figure 7.5: Heat map of the percentage of true positive reviews in each quantile. We can confirm this intuition by plotting each subsets true positive rate versus its quantile, as shown in Figure 7.6. We can see very strong correlation between the two variables. This correlation casts some doubt on the apparent results in Figure 7.4, since we know from a previous experiment that an imbalanced dataset can lead to wonky predictions. Are we really seeing that shorter reviews lead to more accurate predictions, or are we actually seeing that datasets with higher true positive rates are easier to classify? Theres no easy way to disentangle this. results %&gt;% ggplot(aes(x = word_qtile, y =pct_true_pos)) + geom_point() + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;True Positive Rate&quot;) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + theme_minimal() Figure 7.6: True positive rates vs. review lengths for each subset, showing strong correlation. However, since our predictions were reasonably accurate (80%-86%) across a wide range of true-positive rates (0.35-0.65), we can draw one positive preliminary conclusion from this experiment: Preliminary Conclusion: Logistic regression based on AFINN sentiment provides an accurate (&gt;80%) method of predicting review sentiment on datasets across a wide range of word lengths, review counts, and true-positive rates. This is good news, but the confounding effect of the true-positive rates means we dont have a direct answer to our original question of how accuracy varies with review length and volume. This will require some additional processing so that we can operate on a collection of balanced sub-sets. 7.4 Experiment 2: Logistic Regression on Micro-Balanced Data In this experiment, I will address the correlation between each data subsets true-positive rate and review length by further balancing each subset. This probably has a technical name, but here I will call it micro-balancing. The rest of the algorithm will be the same. Recall that in Experiment 1 above, we found that our data sub-sets were imbalanced between positive and negative reviews. This suggests that reviews tend to differ in length according to their sentiment, and as we can see in Figure 7.7, negative reviews do tend to be longer than positive reviews. yelp_data %&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% pull(words) %&gt;% ecdf() %&gt;% plot(col=&quot;green&quot;, main = &quot;ECDF for POS (green) and NEG (red) reviews&quot;, xlab = &quot;Review Length&quot;, ylab = &quot;Proportion&quot;) yelp_data %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% pull(words) %&gt;% ecdf() %&gt;% lines(col=&quot;red&quot;) Figure 7.7: Empirical cumulative distribution function for lengths of positive and negative reviews. The simplest approach is to further downsample the data so that each subset is of the same size and balanced between positive and negative reviews. The following code block runs largely the same analysis as in Experiment 1, except this time I balance each data subset by downsampling before running through a logistic regression. To ensure that all samples are the same size, I first find the smallest number of positive or negative reviews in any subset. Then, in each step of the analysis I randomly downsample the positive and negative reviews to have exactly this many entries. # for reproducibility, set the random number generator seed set.seed(1234) # how many quantiles? num_qtiles &lt;- 5 # get the limits of the word-quantiles for display purposes qtiles &lt;- quantile(yelp_data$words, probs = seq(0, 1, (1/num_qtiles))) # find the word-quantile for each review using the fabricatr::split_quantile() function yelp_data &lt;- yelp_data %&gt;% mutate(qtile = fabricatr::split_quantile(words, type=num_qtiles)) # get the number of reviews in the smallest subset of BOTH rating and length quintile. # we&#39;re going to use this to compare groups of the same/similar size. minn &lt;- yelp_data %&gt;% group_by(qtile, rating_factor) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% summarise(minn = min(n)) %&gt;% unlist() ## `summarise()` regrouping output by &#39;qtile&#39; (override with `.groups` argument) # set up an empty results tibble. results &lt;- tibble() # boolean flag: will we print updates to the console? # I used this for testing but it should be disabled in the final knit verbose &lt;- FALSE tic() # Consider each quantile of review word lengths one at a time for (word_qtile in 1:num_qtiles){ # within each quantile of reviews broken down by length, consider several different numbers of reviews for (num_qtile in 1:num_qtiles){ # number of reviews we will consider in this iteration. num_reviews &lt;- num_qtile * minn/num_qtiles # message for me to keep track if (verbose == TRUE) { message (paste0(&quot;Considering &quot;, num_reviews*2, &quot; reviews with word length in the range (&quot;,qtiles[[word_qtile]],&quot;,&quot;,qtiles[[word_qtile+1]],&quot;)&quot;)) } # I&#39;m doing this in two steps to keep it simple, since we need to get the same number of positive and negative reviews. # First, filter the positive rows we want: the right number of words, and the right number of reviews data_pos &lt;- yelp_data %&gt;% filter(qtile == word_qtile) %&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n = num_reviews) # Then filter the negative rows we want: data_neg &lt;- yelp_data %&gt;% filter(qtile == word_qtile) %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% slice_sample(n = num_reviews) # then combine the positive and negative rows. data_for_logit &lt;- bind_rows(data_pos, data_neg) # get true percentage of positives, so we can look at sample balance pct_true_pos &lt;- data_for_logit %&gt;% summarise(n = sum(rating_factor == &quot;POS&quot;) / nrow(.)) %&gt;% unlist() # run the logistic regression on our data result &lt;- data_for_logit %&gt;% do_logit() # add our result to our results tibble. this wouldn&#39;t be best practice for thousands of rows, but it&#39;s fine here. results &lt;- bind_rows( results, tibble(word_qtile = word_qtile, num_qtile = num_qtile, accuracy = result, pct_true_pos = pct_true_pos) ) } } toc() ## 2.63 sec elapsed Although the additional downsampling takes a bit more time, the code still runs quickly (&lt;10s on my machine). However, before looking at the results lets confirm that each data subset was balanced between positive and negative reviews. Figure 7.8 below shows that the data subsets were balanced, so we can look at our prediction accuracy without worrying about unbalanced data affecting our results. results %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=pct_true_pos)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;True Positive Rate&quot;) Figure 7.8: Heat map of the percentage of true positive reviews in each quantile of the micro-balanced dataset. The results shown below in Figure 7.9 are promising. The accuracy metrics are still quite high, and range from around 79% to around 83%. This is a bit worse overall than in Experiment 1, but we can be more confident now that these are real results and not an artefact of any underlying imbalance in the data. results %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=accuracy)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;Accuracy&quot;) Figure 7.9: Heat map of logistic regression prediction accuracy for the micro-balanced Yelp dataset. Were now in a position to draw some conclusions from our analysis. First, shorter reviews are effective for predicting ratings, and the longest reviews are the least effective. We can see this trend clearly in Figure 7.10 below, where the first three quintiles perform reasonably well, but then accuracy degrades quickly in Q4 and Q5. We can hypothesize about why this might be. For example, shorter reviews might have more information density and longer reviews might tend to ramble on and be noisier. Its much easier to get the gist of This place sucks, I hate it than it is of an 800-word essay that begins Upon entering the establishment, I was first greeted by an aroma of results %&gt;% ggplot() + geom_boxplot(aes(x=as.factor(word_qtile), y = accuracy)) + theme_minimal() + scale_x_discrete(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Accuracy&quot;) Figure 7.10: Experiment 2: Boxplots of review accuracy by word-length quintile. Second, our results were not dependent on the number of reviews, and we achieved good accuracy with even a modest number of reviews. Figure 7.11 shows the distribution of model accuracy according to the number of reviews, and the distributions overlap substantially. There is no clear trend here, suggesting that this approach to classification doesnt benefit from having more than on the order of 10,000 input reviews. results %&gt;% ggplot() + geom_boxplot(aes(x=as.factor(num_qtile), y = accuracy)) + theme_minimal() + scale_x_discrete(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(x = &quot;Number of Reviews&quot;, y = &quot;Accuracy&quot;) Figure 7.11: Experiment 2: Boxplots of review accuracy by number of reviews. If one were so inclined, one could also demonstrate this with a stylish Joy-Division-style ridge-density plot. results %&gt;% ggplot() + ggridges::geom_density_ridges(aes(x = accuracy, y=as.factor(num_qtile))) + theme_minimal() + scale_y_discrete(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(y = &quot;Number of Reviews&quot;, x = &quot;Accuracy&quot;) Figure 7.12: Experiment 2: Shameless pandering to the ref with a Joy Division ridge plot of review accuracy by number of reviews. If one were a stickler for parametric statistics, one might want to see this lack of correlation demonstrated with a linear regression. Here I will run a linear regression to predict a models accuracy from the number of reviews it considers. lm.fit &lt;- lm(data = results, accuracy ~ num_qtile) summary(lm.fit) ## ## Call: ## lm(formula = accuracy ~ num_qtile, data = results) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027484 -0.006335 0.002200 0.007103 0.016486 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8148239 0.0053016 153.695 &lt;2e-16 *** ## num_qtile 0.0004102 0.0015985 0.257 0.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0113 on 23 degrees of freedom ## Multiple R-squared: 0.002855, Adjusted R-squared: -0.0405 ## F-statistic: 0.06586 on 1 and 23 DF, p-value: 0.7997 As expected, the volume of reviews is not a statistically significant predictor of accuracy: the p-value for the num_qtile variable is 0.8, the p-value for the model overall is roughly 0.8, and the Adjusted \\(R^2\\) is negative(!). My results show no statistical evidence that, over these ranges, the number of input reviews is associated with a models accuracy. 7.5 Conclusions In this section I ran two experiments to predict a Yelp reviews rating based on its AFINN sentiment using logistic regression. In each experiment, I built and evaluated 25 models using subsets of my data with different word lengths and numbers of reviews. I demonstrated that you can get good accuracy (~80%) with a relatively small number of reviews (~10,000) using a simple sentiment-detection algorithm (AFINN) and a simple classification model (logistic regression). In Experiment 1 I balanced my overall dataset between positive and negative reviews by random down-sampling. However, I found that my subsets were unbalanced, and found furthermore that the degree of imbalance was strongly correlated with accuracy. Still, I noted that the overall accuracy was still quite good across the entire range of imbalance, and so one interpretation is that this method is quite robust on unbalanced datasets. In Experiment 2 I balanced each subset to have approximately the same number of positive and negative reviews, again using random down-sampling. Using these micro-balanced datasets, I derived the following answers to my two research questions: A1: Review accuracy was better with shorter reviews, and the longest reviews were the least effective. A2: Review accuracy was not correlated with the number of reviews used as inputs, provided the number of reviews is on the order of 10,000. Results in Experiment 2 were very good overall: accuracy ranged from around 79% to around 83% across all models. 7.6 Next Steps Consider evaluating model performance across the entire dataset, not just the testing component of the subset used to generate the model. For discussion.K Consider a more complex sentiment-detection algorithm. Consider a more complex classification engine, e.g. Naive Bayes Classifier using text tokens instead of a real-valued sentiment score. 7.7 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] vip_0.2.2 glmnet_4.0-2 Matrix_1.2-18 ggridges_0.5.2 discrim_0.1.1 tictoc_1.0 textrecipes_0.3.0 lubridate_1.7.9 yardstick_0.0.7 workflows_0.2.0 ## [11] tune_0.1.1 rsample_0.0.8 recipes_0.1.13 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 ## [21] tidytext_0.2.5 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 SnowballC_0.7.0 ## [10] prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 pROC_1.16.2 packrat_0.5.0 ## [19] dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 htmltools_0.5.0 tools_4.0.2 gtable_0.3.0 ## [28] glue_1.4.1 naivebayes_0.9.7 rappdirs_0.3.1 Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 iterators_1.0.12 timeDate_3043.102 ## [37] gower_0.2.2 xfun_0.16 stopwords_2.0 globals_0.13.0 rvest_0.3.6 lifecycle_0.2.0 future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 ## [46] hms_0.5.3 parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 ## [55] textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 lava_1.6.8 rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 ## [64] labeling_0.3 tidyselect_1.1.0 fabricatr_0.10.0 plyr_1.8.6 magrittr_1.5 bookdown_0.20 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [73] pillar_1.4.6 haven_2.3.1 withr_2.2.0 survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 modelr_0.1.8 crayon_1.3.4 utf8_1.1.4 ## [82] rmarkdown_2.3 usethis_1.6.1 grid_4.0.2 readxl_1.3.1 blob_1.2.1 reprex_0.3.0 digest_0.6.25 webshot_0.5.2 munsell_0.5.0 ## [91] GPfit_1.0-8 viridisLite_0.3.0 kableExtra_1.1.0 7.8 References References "],["beyond-the-mvp-looking-at-longer-reviews.html", "Chapter 8 Beyond the MVP: Looking at Longer Reviews 8.1 Introduction 8.2 Results from the Previous Section 8.3 Finding Misclassified Reviews 8.4 Qualitative Analysis of Misclassified Reviews 8.5 Negations 8.6 Readability 8.7 Summing Up So Far 8.8 Logistic Regression on AFINN + negators 8.9 Logistic Regression on Mean AFINN + negators 8.10 Comparing Models: Multiple Trials 8.11 Re-Running the Big Analysis 8.12 Conclusion 8.13 SessionInfo", " Chapter 8 Beyond the MVP: Looking at Longer Reviews 8.1 Introduction In this notebook Ill dig deeper into the last sections strange finding that longer reviews generated worse predictions. Recall that we used a logistic regression to create a classification model to predict Yelp reviews star ratings based on their sentiment as measured by AFINN. After evaluating 25 models using data subsets with review lengths and volumes, the two main results were: A1: Review accuracy was better with shorter reviews, and the longest reviews were the least effective. A2: Review accuracy was not correlated with the number of reviews used as inputs, provided the number of reviews is on the order of 10,000. Here Ill test two hypotheses: H1: The presence of negators like but and not are associated with both incorrect predictions and with increasing review length. H2: Decreasing readability scores are associated with both incorrect predictions and with increasing review length. The intuition is twofold: first, that AFINNs simple bag-of-words approach cant capture complex sentence structures; and second, that the number of complex sentence structures will tend to increase as the length of a review increases. As a result, we would expect more complex reviews to have worse predictions using a simple model based on AFINN scores. 8.2 Results from the Previous Section I will again work with the large Yelp dataset available at this link. For brevity Im omitting the code here (see the previous section or .Rmd source file), but the code: Loads the first 500k reviews; Converts integer star ratings to NEG and POS factors; Balances the NEG and POS reviews using random downsampling; Calculates each reviews AFINN sentiment score; and Calculates each reviews word length. In the last section we found that prediction accuracy was better with shorter texts and poor with longer texts. To give a fairer analysis of the model and to smooth out any potential for us to get poor results by chance, here I extend the analysis a bit to run the model 30 times for each data subsetrandomly resampling a test/train split each timeand computing the average accuracy across all trials. The results are similar to what we found in the last section and are shown below in Figure 8.1. Accuracy ranges from around 79% to around 83%, and it looks like results are worse for longer reviews. results_oldmodel %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=accuracy)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;Accuracy&quot;) Figure 8.1: Overview of results: Heat map of logistic regression prediction accuracy for the micro-balanced Yelp dataset. Each cell shows average accuracy for 30 tests on random samples. This trend is clear in Figure 8.2 below, where the first three quintiles perform reasonably well, but then accuracy degrades quickly in Q4 and Q5. results_oldmodel %&gt;% ggplot() + geom_boxplot(aes(x=as.factor(word_qtile), y = accuracy)) + theme_minimal() + scale_x_discrete(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Accuracy&quot;) Figure 8.2: Experiment 2: Boxplots of review accuracy by word-length quintile, showing the worst performance with the longest reviews. Each point is an average of model results based on 30 random samples within the subset. 8.3 Finding Misclassified Reviews Well modify our logistic regression function so that it returns its prediction for each review, not only the accuracy for the whole set of reviews. logit_predict &lt;- function (dataset) { # for a train/test split: get a random vector as long as our dataset that is 75% TRUE and 25% FALSEe. index &lt;- sample(c(T,F), size = nrow(dataset), replace = T, prob=c(0.75,0.25)) # extract train and test datasets by indexing our dataset using our random index train &lt;- dataset[index,] test &lt;- dataset[!index,] # use `glm()` to run a logistic regression predicting the rating factor based on the AFINN score. logit &lt;- glm(data= train, formula= rating_factor ~ afinn_sent, family=&quot;binomial&quot;) pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) # now predict the outcome based on whichever has the greater probability, find out if each prediction is correct test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) return (test_results) } For this analysis, we will look at the largest set of the longest reviews that we considered in the previous section. This code takes the full set of the longest reviews, downsamples it so that there are the same number of positive and negative reviews, and then fits a logistic regression to a training subset and returns predictions for a test subset. set.seed(1234) word_qtil &lt;- 5 minnnum_reviews &lt;- 5 * minn/num_qtiles # I&#39;m doing this in two steps to keep it simple, since we need to get the same number of positive and negative reviews. # First, filter the positive rows we want: the right number of words, and the right number of reviews data_pos &lt;- yelp_data %&gt;% filter(qtile == word_qtile) %&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n = num_reviews) # Then filter the negative rows we want: data_neg &lt;- yelp_data %&gt;% filter(qtile == word_qtile) %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% slice_sample(n = num_reviews) # then combine the positive and negative rows. data_for_logit &lt;- bind_rows(data_pos, data_neg) # run the logistic regression on our data predictions &lt;- data_for_logit %&gt;% logit_predict() Well use this new dataset for the rest of this section. 8.4 Qualitative Analysis of Misclassified Reviews Lets begin by looking at a few misclassified reviews in detail. Table 8.1 shows a review that was actually positive, but was predicted to be negative. Although it says a lot of positive-valence words like epic and nice, it also uses negative-valence words like desperate, chintzy, and cheesy that balance out for an overall AFINN score of just 8. A human reader can tell that the reviewer introduces these negative words just to dismiss them (as in Feels modern and not too cheesy), but AFINN just sees these words and scores them as negative. predictions[7,]$text %&gt;% knitr::kable(col.names = &quot;Review Text (True Pos, Misclassified)&quot;, caption=&quot;True positive misclassified as negative. Note the number of negative-valence words like &#39;cheesy.&#39;&quot;) %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Table 8.1: True positive misclassified as negative. Note the number of negative-valence words like cheesy. Review Text (True Pos, Misclassified) First and foremost, I want to say that the people watching here is EPIC. Beyond that, its pretty nice for a casino. Feels modern and not too cheesy although its kind of hard to have a casino and NOT have some chintzy decor. Also, this casino doesnt quite have that desperate feeling that a lot of other, older, established casinos seem to have. Im not a gambler so I wont speak to the gaming aspect of things but I think this casino does a good job of drawing people in who arent looking to lose money at the slots and would prefer to drop their hard-earned cash at a bar. The bar areas here are loungy and well laid out and it doesnt feel like everyone who works at the casino is giving you the stinkeye bc you want to sit down somewhere that isnt connected to a machine or at a table. I could totally see myself coming in here to grab a drink or two before venturing on to some other bar. Table 8.2 shows a true negative review that was misclassified as positive. The reviewer uses many positive-valence words like quality, and uses few negative-valence words. Again the positive-valence words are negated or muted by modifiers (medium-quality lamb), but AFINN cant detect this subtlety. set.seed(1233) predictions %&gt;% filter(pred==&quot;POS&quot; &amp; correct==FALSE) %&gt;% slice_sample(n=1) %&gt;% select(text) %&gt;% knitr::kable(col.names = &quot;Review Text (True Neg, Misclassified)&quot;, caption=&quot;True negative misclassified as positive. Note the number of positive-valence words like &#39;quality&#39; and &#39;good,&#39; and the relative absence of negative-valenec words like &#39;bad&#39; or &#39;unpleasant.&#39;&quot; ) %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Table 8.2: True negative misclassified as positive. Note the number of positive-valence words like quality and good, and the relative absence of negative-valenec words like bad or unpleasant. Review Text (True Neg, Misclassified) I feel trolled by fellow Yelpers and IG foodies for thinking that James Cheese would be a great dinner choice. This is one of those cases where it looks good, but doesnt taste good and Im not afraid to call it as it is, despite the impeccable service that we got. Taste should come before presentation and service, sorry. I would describe a meal at James Cheese as a waste of a cheat day. Go do AYCE sushi instead. We got the kimchi fries, coconut pineapple drink and the signature ribs platter in mild and medium. The kimchi fries were a bit dry and not nearly as good as the ones from Banh Mi Boyz, although kudos for the meat. It was just dry while at the same time soggy with kimchi juice. There wasnt a discernable difference between the mild and medium ribs and both are overly sweet. The combo of ribs, mozzarella cheese, corn, scrambled eggs, hotdogs, and kimchi seems so mismatched. I would go so far to call out that anyone who gave this restaurant more than 3 stars not a real foodie. Please, I want to eat real good food, bonus if it looks good. As a final check, lets look at the true-negative review with the highest AFINN score to see where AFINN went most wrong. Table 8.3 shows this review, which received an AFINN score of 113 despite being negative. The author here also uses positive-valence words with negating words (e.g. I wasnt particularly impressed), which confuses AFINN and leads to a high score. In addition, the author does just have a lot of nice things to say about the meal, which even I find a bit confusing. predictions %&gt;% filter(correct==FALSE) %&gt;% arrange(desc(afinn_sent)) %&gt;% head(1) %&gt;% select(text) %&gt;% mutate(text = gsub(&quot;\\n&quot;, &quot; &quot;, text)) %&gt;% knitr::kable(col.names = &quot;Review Text&quot;, caption = &quot;This review was a true negative, but because it is very long it has a very high AFINN score of 113.&quot;) %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Table 8.3: This review was a true negative, but because it is very long it has a very high AFINN score of 113. Review Text Tastefully decorated with wine bottles, glasses, and interesting lighting fixtures, the interior feels open and bright thanks to floor-to-ceiling windows. A fantastic outdoor back patio includes an outdoor bar and lounge areas. The service was pleasant and patient - We were given a bread basket with crispy flat bread seasoned with tarragon and a few slices of white bread. It was accompanied with a rather plain marinara sauce, which foreshadowed the upcoming meal. Baby Red Romaine &amp; Escarole Salad Cara Cara orange, fennel, shaved Parmigiano-Reggiano and old wine vinaigrette A light start of fresh lettuce, a slightly tart vinaigrette, ripe Cara Cara orange, and cheese. While the orange and the dressing was enjoyable, something was left to be desiredperhaps its because of my unrefined wine and cheese palette, but I wasnt particularly impressed. Memorable Summerlicious starter salads include the butterleaf truffle salad at Truffles at the Four Seasons (which has since closed) and Tutti Mattis carpaccio salads. Bigeye Tuna Tartare &amp; Spicy Sopressata Cracked olives, garlic grissini, arugula, lemon and Sicilian organic extra virgin olive oil Fresh tuna tartare on a bed of salami was a welcome dish, especially since Ive never tried sopressata before. The saltyness of the cured ham paired with the freshnesses of the tuna created an interesting balance between bites. The olives were typical, and the bread sticks brought a crunch but no flavour to the plate. Perhaps if they were toasted a bit more or rolled in sesame, it wouldve been a nice compliment to the dish. Wild &amp; Tame Mushroom Soup Six kinds of mushrooms (no butter or cream) Thick, warm, and hearty - this had the rich texture of a comforting soup despite the lack of cream or butter. I was impressed with the consistency and I love the taste of mushrooms so I thoroughly enjoyed this soup. As with the salad, there was something lacking - the wow factor that I always hope for at licious restaurants. The best mushroom soup that Ive tasted was Wild Mushroom Soup from Tutti Matti - unfortunately not on their current menu. Grilled Top Sirloin Fettina Charred onion, heirloom tomato and rocket salad with crumbled Gorgonzola The vegetables were nice and warm, and the sauce surrounding the steak sweet however the steak wasnt particularly interesting or great. Tender and nicely cooked, it was a good, simple cut of sirloin but didnt blow me away or leave a lasting impression on my palette. Grilled Jail Island Salmon Pickled summer beets, heirloom carrots, watercress salad and horseradish crème fraîche Despite my half-success with the fish at Biffs Bistro, I decided that it was worth another go. While tasty, Im quickly getting bored with this Summerlicious lunch. The beets were nice, warm, and soft and the horseradish crème fraîche had great flavour. However the salmon wasjust salmon. Roasted Portobello Mushroom &amp; Brie Quesadilla Caramelized onions, pico de gallo, chipotle crema, plantain tortilla chips and cilantro cress My personal favourite of the entrées, the chipotle cream had a tangy flavour that tasted great with the pico de gallo and quesadilla. The pico de gallo (salsa) was ripe and simple. Always a fan of portobello, these quesadillas had great flavour. While an interesting visual, the tortilla chips were lacking the flavour I usually enjoy from plantains. As a plantain chip lover, they were a disappointment. Id rather crack open a bag of the Samai Plantain chips that I got addicted to Barbados. Chocolate Toffee Crunch Cheesecake Dulce de leche cream Thick cheesecake that was more toffee-like than chocolate. The caramel was pleasant with the dense cake. I wasnt a fan of the bit of toffee that accompanied the cake but overall Id say that this was the best dessert out of the three. Bavarian Vanilla Cream Ontario strawberry compote and dark cherry balsamic Nice and warm with a smooth texture, the vanilla cream dessert sat on one of the better compotes that Ive tried. I found myself scooping this dessert up more for the compote than the cream. Coconut Rum Baba Tropical fruit salad and banana cream This spongy cake was covered with coconut shavings over pineapple, mango, and lychee. A sweet, generous drizzle of rum syrup soaked into the cake quite nicely. While I wasnt the biggest fan of this dessert initially, over time as I enjoyed my company it grew on me. It wasnt too rich or filling. Wonderfully designed and decorated, Jump Café &amp; Bar creates a vibrant, young, and upscale environment quite well. I would return for a drink at the bar, and the service was fantastic. However, I thought it was missing that the oomph factor that would make me wholeheartedly recommend this restaurant for future Summerlicious romps. This is a good spot to take someone who wants a very simple no-frills meal. In addition, this shows a potential source of error when using raw AFINN scores: there is no weighting for review length. A review that makes 50 lukewarm statements will be rated 50 times more positive than a shorter review that only makes one. As Figure 8.3 shows, there does seem to be a positive correlation between a reviews word length and its AFINN score. It may therefore be worth looking at normalizing AFINN scores, either to word length or number of sentences, to see if we can improve our accuracy. yelp_data %&gt;% ggplot(aes(x=words, y=afinn_sent)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula = y~x) + theme_minimal() + labs(x=&quot;Words&quot;, y=&quot;AFINN Sentiment&quot;) Figure 8.3: Longer reviews seem to have slightly higher AFINN sentiment scores on average, and it also looks like variance increases with word length. lm_fit &lt;- lm(data = yelp_data, formula = afinn_sent ~ words) lm_fit %&gt;% broom::tidy() %&gt;% knitr::kable( caption = &quot;Results from a linear regrssion predicting AFINN sentiment from review length in words. Although the $R^2$ is low, the model shows good statistical significance.&quot; ) Table 8.4: Results from a linear regrssion predicting AFINN sentiment from review length in words. Although the \\(R^2\\) is low, the model shows good statistical significance. term estimate std.error statistic p.value (Intercept) 4.5909815 0.0322987 142.14137 0 words 0.0190131 0.0002022 94.02087 0 We can draw two conclusions from this (mostly) qualitative look at misclassified reviews: Misclassified reviews often use many opposite-valence words but negate them using words like but or not. The most-misclassified review by AFINN score was extremely long, suggesting that normalizing AFINN scores by review length might improve our accuracy. 8.5 Negations Lets investigate the effects of negations/negators in our review texts. Its possible, for example, that misclassified reviews could tend to have more negators like not or but. Since AFINN is a pure bag-of-words approach it cant distinguish between I am happy this is very good and I am not happy this is not very good. Accounting for negators might therefore give us a way to capture this information and improve our predictions. 8.5.1 Negations and Prediction Accuracy Lets look at buts and nots in the correct vs incorrect predictions. predictions &lt;- predictions %&gt;% mutate(buts = stringr::str_count(text, &quot;but &quot;), nots = stringr::str_count(text, &quot;not &quot;), buts_nots = buts + nots) The average numbers or buts and nots are different: predictions %&gt;% group_by(correct) %&gt;% summarise(avg_negs = mean(buts_nots)) %&gt;% knitr::kable() correct avg_negs FALSE 3.703857 TRUE 3.370872 Lets look more closely at the number of negators in correctly and incorrectly predicted reviews. Table 8.5 below shows the results of three two-sided t-tests comparing the average number of buts, nots, and combined buts &amp; nots for correct and incorrect predictions. While buts and buts &amp; nots differ significantly, the difference in the number of nots is not statistically significant at the standard level of \\(p&lt;0.05\\). The other two results show strong significance. pred_bn &lt;- predictions %&gt;% t_test(buts_nots ~ correct) %&gt;% mutate(measure = &quot;buts &amp; nots&quot;) pred_b &lt;- predictions %&gt;% t_test(buts ~ correct) %&gt;% mutate(measure = &quot;buts&quot;) pred_n &lt;- predictions %&gt;% t_test(nots ~ correct) %&gt;% mutate(measure = &quot;nots&quot;) bind_rows(pred_bn, pred_b, pred_n) %&gt;% select(measure, statistic, t_df, p_value, alternative) %&gt;% knitr::kable(caption = &quot;Results for two-sided t-tests for equivalence of means in the numbers of &#39;nots&#39;, &#39;buts&#39;, and &#39;nots and buts&#39; in correct and incorrect predictions for the subset of longer reviews.&quot;) Table 8.5: Results for two-sided t-tests for equivalence of means in the numbers of nots, buts, and nots and buts in correct and incorrect predictions for the subset of longer reviews. measure statistic t_df p_value alternative buts &amp; nots 3.934286 2026.129 0.0000863 two.sided buts 4.564349 2092.784 0.0000053 two.sided nots 1.875926 2024.922 0.0608092 two.sided This suggests that negators might play a role in lowering our models accuracy, and that accounting for negators somehow might improve our predictions. 8.5.2 Negations and Word Length Lets look at the number of negations vs. word length for the longer reviews. Figure 8.4 below shows that the number of buts and nots tends to increase with word length. predictions %&gt;% ggplot(aes(x=words, y=buts_nots)) + geom_point(position = &quot;jitter&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal()+ labs(x = &quot;# Words&quot;, y = &quot;# buts and nots&quot;) Figure 8.4: The combined number of buts and nots tends to increase as reviews get longer. A linear regression shows that this association is strongly significant, with both model and coefficient p-values below \\(2^{-16}\\). The \\(R^2\\) is 0.295, which looks reasonable based on the plot. lm(data = predictions, formula = buts_nots ~ words) %&gt;% summary() ## ## Call: ## lm(formula = buts_nots ~ words, data = predictions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.403 -1.546 -0.288 1.263 15.672 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0563061 0.0648218 0.869 0.385 ## words 0.0119693 0.0002105 56.867 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.283 on 7719 degrees of freedom ## Multiple R-squared: 0.2953, Adjusted R-squared: 0.2952 ## F-statistic: 3234 on 1 and 7719 DF, p-value: &lt; 2.2e-16 We can conclude that buts and nots are associated with both incorrect predictions and with increasing word lengths. 8.6 Readability In this section well look at how a reviews readability relates to model accuracy. Well use the sylcount packages readability() function to calculate Flesch-Kincaid (FK) readability scores and see how they interact with accuracy. FK scores are a common way to measure a texts complexity and will be familiar to anyone who uses Microsoft Word. Without going into details, texts with higher FK scores are assumed to be easier to read. predictions_re &lt;- predictions %&gt;% pull(text) %&gt;% sylcount::readability() %&gt;% select(-words) %&gt;% bind_cols(predictions, .) Figure 8.5 shows FK readability ease vs. word length: predictions_re %&gt;% ggplot(aes(x=words, y=re)) + geom_point() + theme_minimal() + labs(x = &quot;# Words&quot;, y = &quot;Flesch-Kincaid Reasing Ease Score&quot;) Figure 8.5: FK reading ease for the original set of long reviews. While most scores are between 0 and 100, there are many outliers as low as -250. We immediately see that the FE index has no theoretical minimum value, and this is a problem. Lets look at the shortest super low one to see whats going on. Looks like these reviewers use \" . \" instead of . so the algorithm thinks its all one sentence. predictions_re %&gt;% filter( re == min(re)) %&gt;% filter(words == min(words)) %&gt;% pull(text) %&gt;% stringr::str_trunc(150) ## [1] &quot;Leaving it 2 stars the food was bland the Greek fries were a little dry nothing special about the gyro but not uneatable . Also a bit over priced 1...&quot; So well fix this by replacing \" .\" with . and try again. Results are shown below in Figure 8.6. predictions_re &lt;- predictions %&gt;% mutate(text = stringr::str_replace_all(text, &quot; \\\\.&quot;, &quot;\\\\.&quot;)) %&gt;% pull(text) %&gt;% sylcount::readability() %&gt;% select(-words) %&gt;% bind_cols(predictions, .) predictions_re %&gt;% ggplot(aes(x=words, y=re)) + geom_point() + theme_minimal() + labs(x = &quot;# Words&quot;, y = &quot;Flesch-Kincaid Reasing Ease Score&quot;) Figure 8.6: FK reading ease for the set of long reviews with non-standard period breaks removed. Note there are still many low outliers. Since there are still many low outliers, the new shortest least-readable review is presented below in Table 8.6. We can see two things: first, its honestly full of run-on sentences; and second, it looks like the reviewer is using carriage returns as sentence markers. We can fix the second item by replacing carriage returns with periods. predictions_re %&gt;% filter(re == min(re)) %&gt;% filter(words == min(words)) %&gt;% pull(text) %&gt;% stringr::str_trunc(500) %&gt;% knitr::kable(col.names = &quot;Shortest Least-Readable Review&quot;, caption = &quot;Statistics for two-sided t-tests comparing the number of correct and incorrect predictions based on the number of nots, buts, and nots &amp; buts they contain.&quot;) Table 8.6: Statistics for two-sided t-tests comparing the number of correct and incorrect predictions based on the number of nots, buts, and nots &amp; buts they contain. Shortest Least-Readable Review My review not to criticise people personally but the business when we booked they said that they will pick us 6:15 am from our hotel so we waited almost half an hours extra outside the hotel till the picked us up Then we spend more time till they distribute us to another busses and kept asking if we would like to upgrade the seat or the bus for extra money we waste one and a half hours until we hit the road the busses was not so good very old no USB to charge phone because we where thinking | As can be seen in Figure 8.7, there are still very low readability scores after fixing these errors in the dataset. This suggests that many reviews have inherent features, like run-on sentences, that result in genuinely low FK scores. predictions_re &lt;- predictions %&gt;% mutate(text = stringr::str_replace_all(text, &quot; \\\\.&quot;, &quot;\\\\.&quot;), text = stringr::str_replace_all(text, &quot;\\\\s*\\\\n\\\\s*&quot;, &quot;. &quot;)) %&gt;% pull(text) %&gt;% sylcount::readability() %&gt;% select(-words) %&gt;% bind_cols(predictions, .) predictions_re %&gt;% ggplot(aes(x=words, y=re)) + geom_point() + theme_minimal() + labs(x = &quot;# Words&quot;, y = &quot;Flesch-Kincaid Reasing Ease Score&quot;) Figure 8.7: FK reading ease for the set of long reviews with non-standard period breaks and carriage returns removed. Outliers persist. Note that we have one value that has -Inf readability! This review, included as Table ?? below, has a lot of text but no sentence-ending punctuation so it breaks the readability algorithms. We will filter this entry out in the rest of this section. predictions_re %&gt;% filter(re == -Inf) %&gt;% pull(text) %&gt;% stringr::str_trunc(300) %&gt;% knitr::kable(col.names = &quot;Infinitely Unreadable Review&quot;) Infinitely Unreadable Review Well not a good experience so got approved for credit threw Conns and was happy for that they delivered the furniture day just like they,said so I get back home and see they didnt set the tv up ok so I call my brother over we set the tv up on the stand and the tv doesnt work what a surprise  So lets again now look at readability and prediction accuracy. The boxplots for the distributions look quite similar, as can be seen below in Figure 8.8. predictions_re %&gt;% filter(re &gt; -Inf) %&gt;% ggplot() + geom_boxplot(aes(x=as.factor(correct), y=re)) + theme_minimal() + labs(x = &quot;Prediction Accuracy&quot;, y = &quot;Flesch-Kincaid Reasing Ease Score&quot;) Figure 8.8: Distributions of FK reading ease scores for reviews with correct and incorrect predictions. The boxplots are very similar. Looking at the sample means, theyre not so different (after removing the one value of -Inf!) predictions_re %&gt;% group_by(correct) %&gt;% filter(re &gt; -Inf) %&gt;% summarise(avg_re = mean(re)) %&gt;% knitr::kable() correct avg_re FALSE 81.77320 TRUE 81.85309 And a two-sided t-test does not show statistical evidence that the distribution means are different. predictions_re %&gt;% filter(re &gt; -Inf) %&gt;% t_test(re ~ correct) %&gt;% knitr::kable() statistic t_df p_value alternative lower_ci upper_ci -0.2975279 2157.305 0.7660922 two.sided -0.6064841 0.4466979 To sum up, I found two problems with using readability measures on this data. First, many texts use either non-standard sentence breaks (e.g. \" . \" or carriage returns) which confuses the algorithms. But even after fixing these data issues, many texts have little or no punctuation at all. Readability algorithms rely on sentence counts, so they wont work on informal texts without conventional punctuation. So readability is not associated with prediction accuracy in this sample of longer reviews. 8.7 Summing Up So Far Lets do a quick recap on what weve learned so far in this section: Misclassified reviews often use many opposite-valence words but negate them using words like but or not. The most-misclassified review by AFINN score was extremely long, suggesting that normalizing AFINN scores by review length might improve our accuracy. Negators like buts and nots are associated with both incorrect predictions and with increasing word lengths. Readability is not associated with prediction accuracy in this sample of longer reviews. 8.8 Logistic Regression on AFINN + negators Based on our findings, lets create a second model where we add negators to our logistic regression. This next code chunk defines Model 2, and the main difference is that now our logistic regression formula is rating_factor ~ afinn_sent + buts_nots. logit_predict_v2 &lt;- function (dataset) { # for a train/test split: get a random vector as long as our dataset that is 75% TRUE and 25% FALSE. index &lt;- sample(c(T,F), size = nrow(dataset), replace = T, prob=c(0.75,0.25)) # extract train and test datasets by indexing our dataset using our random index train &lt;- dataset[index,] test &lt;- dataset[!index,] # use `glm()` to run a logistic regression predicting the rating factor based on the AFINN score. logit &lt;- glm(data= train, formula= rating_factor ~ afinn_sent + buts_nots, family=&quot;binomial&quot;) pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) # now predict the outcome based on whichever has the greater probability, find out if each prediction is correct test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) return (test_results) } Lets run the predictive algorithm again using Model 2: data_for_logit &lt;- data_for_logit %&gt;% mutate(buts = stringr::str_count(text, &quot;but &quot;), nots = stringr::str_count(text, &quot;not &quot;), buts_nots = buts + nots) predictions_v2 &lt;- data_for_logit %&gt;% logit_predict_v2() pred_v1 &lt;- predictions %&gt;% summarise(v1_accuracy = sum(correct)/ n()) pred_v2 &lt;- predictions_v2 %&gt;% summarise(v2_accuracy = sum(correct)/ n()) bind_cols(pred_v1, pred_v2) %&gt;% knitr::kable() v1_accuracy v2_accuracy 0.8119415 0.8341827 Its about 1.5% more accurate. Not a lot, but a bit. 8.9 Logistic Regression on Mean AFINN + negators Above, we found qualitative reasons to think that longer reviews might have more variable AFINN scores and some quantitative evidence that AFINN scores for longer reviews tended to be slightly more positive. Here well try using a reviews mean AFINN score, instead of the sum of all its word-level AFINN scores, as a predictor along with the number of negators. First well calculate the mean AFINN score for each review. afinn_mean &lt;- data_for_logit %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_mean = mean(value, na.rm = T)) %&gt;% mutate(afinn_mean = if_else(is.na(afinn_mean) | is.nan(afinn_mean), 0, afinn_mean)) data_for_logit &lt;- data_for_logit %&gt;% bind_cols(afinn_mean) %&gt;% select(-rowid) logit_predict_v3 &lt;- function (dataset) { # for a train/test split: get a random vector as long as our dataset that is 75% TRUE and 25% FALSE. index &lt;- sample(c(T,F), size = nrow(dataset), replace = T, prob=c(0.75,0.25)) # extract train and test datasets by indexing our dataset using our random index train &lt;- dataset[index,] test &lt;- dataset[!index,] # use `glm()` to run a logistic regression predicting the rating factor based on the AFINN score. logit &lt;- glm(data= train, formula= rating_factor ~ afinn_mean + buts_nots, family=&quot;binomial&quot;) pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) # now predict the outcome based on whichever has the greater probability, find out if each prediction is correct test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) return (test_results) } And lets run the predictive algorithm again using normalized AFINN score and buts and nots: predictions_v3 &lt;- data_for_logit %&gt;% logit_predict_v3() pred_v1 &lt;- predictions %&gt;% summarise(v1_accuracy = sum(correct)/ n()) pred_v2 &lt;- predictions_v2 %&gt;% summarise(v2_accuracy = sum(correct)/ n()) pred_v3 &lt;- predictions_v3 %&gt;% summarise(v3_accuracy = sum(correct)/ n()) bind_cols(pred_v1, pred_v2, pred_v3) %&gt;% knitr::kable() v1_accuracy v2_accuracy v3_accuracy 0.8119415 0.8341827 0.8280197 It looks like these results are a little bit worse. But these are just results from one trial, so the next step would be to do a number of trials to see how results vary. 8.10 Comparing Models: Multiple Trials To get a better idea of how each model performs, we can effectively do our own cross-fold validation by re-running our trial to see how the results change as we take different test/train samples. Note that well be sampling with replacement, as opposed to setting up mutually exclusive folds. Could we call this bootstrapping, of a sort? set.seed(1234) # logit on just afinn reps &lt;- 100 results_1 &lt;- results_2 &lt;- results_3 &lt;- tibble() for (i in 1:reps) results_1 &lt;- data_for_logit %&gt;% logit_predict() %&gt;% summarise(v1_accuracy = sum(correct)/ n()) %&gt;% bind_rows(results_1) for (i in 1:reps) results_2 &lt;- data_for_logit %&gt;% logit_predict_v2() %&gt;% summarise(v2_accuracy = sum(correct)/ n()) %&gt;% bind_rows(results_2) for (i in 1:reps) results_3 &lt;- data_for_logit %&gt;% logit_predict_v3() %&gt;% summarise(v3_accuracy = sum(correct)/ n()) %&gt;% bind_rows(results_3) results &lt;- bind_cols(results_1, results_2, results_3) %&gt;% pivot_longer(cols = everything(), names_to = &quot;model&quot;, values_to = &quot;accuracy&quot;) results %&gt;% ggplot(aes(x=as.factor(model), y=accuracy)) + geom_boxplot() + labs(x=&quot;Model&quot;, y=&quot;Accuracy&quot;) + scale_x_discrete(labels = c(&quot;M1: Sum AFINN&quot;, &quot;M2: Sum AFINN + Negators&quot;, &quot;M3: Mean AFINN+ Negators&quot;)) + theme_minimal() Figure 8.9: Distribution of results for 100 trials of each model. Model 3, Mean AFINN + Negators, outperforms the other models on average. Figure 8.9 shows the results from 100 trials each of the three models weve developed, logistic regressions based on AFINN sum, AFINN sum plus negators, and mean AFINN plus negators. Model 3, mean AFINN plus negators, clearly outperforms the other models on average for this dataset of longer reviews. We can run the full 5x5 analysis again to get a heat map and box plots and see if it helped. 8.11 Re-Running the Big Analysis Here well re-run the big analysis using Model 3, mean AFINN plus negators. First well calculate the mean AFINN score for each review in our full dataset. afinn_mean &lt;- yelp_data %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_mean = mean(value, na.rm = T)) %&gt;% mutate(afinn_mean = if_else(is.na(afinn_mean) | is.nan(afinn_mean), 0, afinn_mean)) ## Joining, by = &quot;word&quot; ## `summarise()` ungrouping output (override with `.groups` argument) yelp_data &lt;- yelp_data %&gt;% bind_cols(afinn_mean) %&gt;% select(-rowid) yelp_data &lt;- yelp_data %&gt;% mutate(buts = stringr::str_count(text, &quot;but &quot;), nots = stringr::str_count(text, &quot;not &quot;), buts_nots = buts + nots) %&gt;% mutate(afinn_mean = if_else (is.na(afinn_mean), 0, afinn_mean)) Our results, shown below in Figure 8.10, are promising. Accuracy seems to be higher across the board, and while there are some darker patches in the centre, there doesnt seem to be a drop-off as word length increases. results_model3 %&gt;% ggplot() + geom_tile(aes(x=word_qtile, y=num_qtile, fill=accuracy)) + scale_x_continuous(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + scale_y_continuous(breaks = 1:num_qtiles, labels = (2*round(1:num_qtiles * minn/num_qtiles))) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Number of Reviews&quot;, fill = &quot;Accuracy&quot;) Figure 8.10: Heat map of average accuracy for Model 3. Each cell shows average accuracy for 30 trials on random data subsets. The results are even more promising when we compare them to those from Model 1. Figure 8.11 shows Model 1 and Model 3s results side by side, and the difference is dramatic: Model 3 gives better mean predictions across all subsets, and the differences are dramatically positive for longer reviews. results_comp &lt;- results_oldmodel %&gt;% left_join(results_model3 %&gt;% rename(accuracy_m3 = accuracy)) %&gt;% mutate(diff = accuracy_m3 - accuracy) %&gt;% select(-pct_true_pos) results_comp %&gt;% pivot_longer(cols = c(&quot;accuracy&quot;, &quot;accuracy_m3&quot;), values_to = &quot;accuracy&quot;, names_to = &quot;model&quot;) %&gt;% ggplot() + geom_boxplot(aes(x=as.factor(word_qtile), y = accuracy, fill = model)) + theme_minimal() + scale_x_discrete(breaks = 1:num_qtiles, labels = paste0(&quot;Q&quot;,1:num_qtiles,&quot;: &quot;,qtiles, &quot;-&quot;,lead(qtiles)) %&gt;% head(-1)) + labs(x = &quot;Review Word Length by Quantile&quot;, y = &quot;Accuracy&quot;, fill = &quot;Model&quot;) + scale_fill_discrete(labels = c(&quot;M1: Sum AFINN&quot;, &quot;M3: Mean AFINN + Negators&quot;)) + theme(legend.position = &quot;bottom&quot;) Figure 8.11: Comparing average accuracy rates for Model 1 and Model 3. Model 3 gives better on-average predictions for reviews of all word lengths, and some improvements are dramatic. Moving Model 3 to use mean AFINN and the number of negators seems to be a Pareto-improvement over Model 1. Model 3 works about as well for all review lengths, and long reviews are about as easy to predict as short ones. As with Model 1, Figure 8.10 shows that Model 3 seems to work about as well for smaller groups as it does for larger groups. 8.12 Conclusion In this section we looked into last sections strange finding that longer Yelp reviews led to worse predictions using Model 1, a logistic regression on AFINN sentiment. After a qualitative review of some misclassified long reviews, we looked into two hypotheses for what might be confusing AFINN: that longer reviews might use more negations, and that longer reviews might have lower readability scores. We didnt find any strong connection with readability scores, but we did find that longer reviews use more negations like but and not. This led us to suspect that including the number of negations in our regression might improve the model. We also found that longer reviews tend to have slightly higher scores and that score variance seems to increase with length. This suggested that a normalized AFINN score based on a reviews length might be more reliable. We built and tested two new models. Model 3 performed the best, and improved on Model 1 across all data subsets and did especially well on longer reviews (see Figure 8.11). Model 3used a logistic regression based on a reviews mean AFINN score and its number of buts and nots. We also improved our model evaluation by using a process similar to cross-fold evaluation, running it 30 times on each data subset using random test/train splits and taking the average accuracy across all trials. Weve shown that you can build a model that is roughly 82.5% accurate at predicting Yelp ratings using fast and simple models (AFINN sentiment, logistic regression) with insights from qualitative analysis of the data (counting negators, switching to mean AFINN). The models take only a few lines of code, run in seconds, and are completely supervisable and interpretable. Weve also shown two things about using Yelp review text as input data. First, ceterus paribus, review length is not a significant issue when predicting Yelp ratings. Second, our models accuracy didnt change meaningfully based on the number of reviews we used as inputs. Our models were robust across the entire range we considered, from around 6,000 input reviews to around 31,000. 8.13 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 ## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] beepr_1.3 sylcount_0.2-2 ggridges_0.5.2 vip_0.2.2 ## [5] glmnet_4.0-2 Matrix_1.2-18 lubridate_1.7.9 tictoc_1.0 ## [9] discrim_0.1.1 yardstick_0.0.7 workflows_0.2.0 tune_0.1.1 ## [13] rsample_0.0.8 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 ## [17] dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 ## [21] textrecipes_0.3.0 recipes_0.1.13 tidytext_0.2.5 forcats_0.5.0 ## [25] stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 ## [29] tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 ## [5] rstudioapi_0.11 farver_2.0.3 listenv_0.8.0 furrr_0.1.0 ## [9] audio_0.1-7 SnowballC_0.7.0 prodlim_2019.11.13 fansi_0.4.1 ## [13] xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 ## [17] jsonlite_1.7.0 pROC_1.16.2 dbplyr_1.4.4 compiler_4.0.2 ## [21] httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 ## [25] htmltools_0.5.0 tools_4.0.2 gtable_0.3.0 glue_1.4.1 ## [29] Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 ## [33] nlme_3.1-148 iterators_1.0.12 timeDate_3043.102 gower_0.2.2 ## [37] xfun_0.16 globals_0.13.0 rvest_0.3.6 lifecycle_0.2.0 ## [41] future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 hms_0.5.3 ## [45] parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 ## [49] stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 ## [53] lhs_1.0.2 lava_1.6.8 shape_1.4.5 rlang_0.4.7 ## [57] pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 labeling_0.3 ## [61] tidyselect_1.1.0 fabricatr_0.10.0 plyr_1.8.6 magrittr_1.5 ## [65] bookdown_0.20 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [69] mgcv_1.8-31 pillar_1.4.6 haven_2.3.1 withr_2.2.0 ## [73] survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 modelr_0.1.8 ## [77] crayon_1.3.4 rmarkdown_2.3 grid_4.0.2 readxl_1.3.1 ## [81] blob_1.2.1 reprex_0.3.0 digest_0.6.25 munsell_0.5.0 ## [85] GPfit_1.0-8 "],["beyond-the-mvp-support-vector-machine-classification.html", "Chapter 9 Beyond the MVP: Support Vector Machine Classification 9.1 Introduction 9.2 SVM Classifiers 9.3 Preparing the Data 9.4 SVM Classification 9.5 Predicting with last_fit() 9.6 Fitting and Predicting Using fit() 9.7 Conclusions 9.8 SessionInfo 9.9 References", " Chapter 9 Beyond the MVP: Support Vector Machine Classification 9.1 Introduction In this section, I will extend the minimum viable project (MVP) in the last section and build a support vector machine (SVM) classifier to predict Yelp reviews star ratings. This time, instead of just using AFINN sentiment as the model input, Ill predict ratings based on each reviews text, its word length, and its AFINN sentiment. I will again be predicting positive (POS) and negative (NEG) ratings following Liu (2015)s recommendation, and use the approaches outlined in Silge and Hvitfeldt (2020) and Silge and Robinson (2020). In some cases I have used examples or hints from websites like Stack Overflow, and Ive noted that where applicable. Based on some initial experiments, after loading 200,000 Yelp reviews I will use an aggressive train/test split to use 5% of the data for model training and then test its performance on the other 95%. There are two reasons for this. The first reason is pragmatic: I have much more data than processing power, and 5% of the data amounts to 11,106 reviews which already takes nearly 20 minutes to run through an SVM on my machine. The second reason is optimistic: based on some earlier experiments, I have reason to think that roughly 10,000 reviews is enough to train a decent model, so I would like to seize on this huge dataset to do a really robust test. 9.2 SVM Classifiers A support vector machine (SVM) classifier is a mathematical model that assigns observations to one of two classes. The mathematics are complicated, so here I will present a brief non-technical summary based on Hastie, Tibshirani, and Friedman (2009)s exposition (pp. 417-438). Imagine a dataset consisting of \\(N\\) pairs \\((x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\), where the \\(x_i\\in\\mathbb{R}^p\\) and the \\(y_i\\in\\{-1,1\\}\\). In other words, our observations are situated somewhere in a \\(p\\)-dimensional Euclidean space with coordinates \\(x_i\\), and also belong to one of two classes given by \\(y_i\\). Intuitively, we could set \\(p=2\\) and imagine throwing a handful of pennies onto a tabletop: each penny has some position on the tabletop that we could label \\(x_i\\), and each penny is either heads or tails, which we could label \\(y_i\\). For our tabletop example, if were lucky we might be able to draw a straight line separating all the heads and tails. In more general cases we may be able to define a hyperplane that separates all instances of the two classes. We can call these situations separable, and the general approach here is to find the hyperplane that divides the two classes with the widest margin \\(M\\) possible on both sides. In other cases, however, there might be some heads mixed in with the tails, so it may be impossible to draw a straight line or hyperplane that cleanly separates the two classes. If so, we can generalize our approach to permit some misclassifications. The problem then is to find the hyperplane that minimizes the number and degree of misclassifications: in other words, to minimize the number of points on the wrong side of the dividing line and to minimize their distance from it. This is the intuition behind a support vector classifier. A support vector machine classifier generalizes the support vector classifier to the case where the boundary is non-linear. Roughly, an SVM expands the input feature space (i.e. the \\(x_i\\)) using potentially non-linear transformations and then solves the classification problem in this larger space. Linear boundaries in this larger space will generally correspond to non-linear boundaries in the original space, so intuitively this means we are now considering the possibility that we could draw curved lines in our original space to separate our two classes. The details, however, are highly technical, and the reader is referred to Hastie, Tibshirani, and Friedman (2009) (417-438) for more information. 9.3 Preparing the Data I will again work with the large Yelp dataset available at this link, this time loading the first 500k reviews. This code block does the following: Load our data; Factor it into POS (4-5 stars) and NEG (1-2 stars); Balance POS and NEG by random downsampling; Get each reviews AFINN sentiment score; and Get each reviews word count. set.seed(1234) # figure out how to do it reading between the lines of this stackoverflow: # https://stackoverflow.com/questions/53277351/read-first-1000-lines-from-very-big-json-lines-file-r yelp_big_factor &lt;- readLines(&quot;../tests/data/yelp_academic_dataset_review.json&quot;, n = 500000) %&gt;% textConnection() %&gt;% jsonlite::stream_in(verbose=FALSE) %&gt;% select(stars, text) %&gt;% mutate(rating_factor = case_when( stars &lt; 3 ~ &quot;NEG&quot;, stars &gt; 3 ~ &quot;POS&quot;) %&gt;% as.factor() ) %&gt;% select(-stars) %&gt;% drop_na() # random downsampling to balance POS and NEG in the dataset yelp_balanced &lt;- yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% bind_rows(yelp_big_factor%&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n=yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% nrow() )) # get AFINN scores for each review tic() afinn_yelp_big &lt;- yelp_balanced %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T)) toc() # add the AFINN scores to the original tibble yelp_big_bal_afinn &lt;- afinn_yelp_big %&gt;% left_join(yelp_balanced %&gt;% rowid_to_column()) %&gt;% select(-rowid) # get wordcounts wordcounts_yp &lt;- yelp_big_bal_afinn %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) # add wordcounts to create final dataset yelp_data &lt;- bind_cols( yelp_big_bal_afinn, wordcounts_yp %&gt;% arrange(rowid) %&gt;% select(words = n)) # remove transient datasets, keep only the final one rm (yelp_big, yelp_big_factor, yelp_balanced, afinn_yelp_big, yelp_big_bal_afinn, wordcounts_yp) Then well create a train/test split on the entire dataset, using 5% for training and 95% for testing. set.seed(1234) yelp_split &lt;- initial_split(yelp_data, strata = rating_factor, prop = 0.05) yelp_train &lt;- training(yelp_split) yelp_test &lt;- testing(yelp_split) Then we set up ten cross-validation folds that we will use to evaluate the models we build using our training data. yelp_folds &lt;- vfold_cv(yelp_train) 9.4 SVM Classification First we set up our SVM model, here using the liquidSVM package following Silge and Hvitfeldt (2020). svm_model &lt;- svm_rbf() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;liquidSVM&quot;) svm_model ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Computational engine: liquidSVM Next well set up an SVM recipe based on entirely on text tokens. A good next step would be to use what Ive learned in earlier sections, for example by including negators, word count, and sentiment, but to keep it simple well leave them out here. Well process our text using these steps: Tokenizing the text into words; Removing stopwords from the default snowball dictionary; Filtering out tokens that occur fewer than 50 times; Choosing a maximum number of tokens, which we will tune as a hyperparameter; and, Applying a TFIDF to the text. We could also consider n-grams (i.e. considering n-word strings of text), which might be useful for catching negators. Ive included the code in this block but commented it out. For now well stick with individual words. yelp_rec &lt;- recipe(rating_factor ~ text, #+ words + afinn_sent, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text) %&gt;% # step_ngram(text, min_num_tokens = 1L, num_tokens = 1) %&gt;% step_tokenfilter(text, max_tokens = tune(), min_times = 50) %&gt;% step_tfidf(text) yelp_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Tokenization for text ## Stop word removal for text ## Text filtering for text ## Term frequency-inverse document frequency with text Next we set up our workflow: svm_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) %&gt;% add_model(svm_model) svm_wf ## == Workflow ========================================================================================================================================================================================================================= ## Preprocessor: Recipe ## Model: svm_rbf() ## ## -- Preprocessor --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Computational engine: liquidSVM We have set up our recipe to let us try several different values for the maximum number of tokens, so here well set up grid of values to test. Based on some scratch work, Im going to use three values between 500 and 1250. param_grid &lt;- grid_regular( max_tokens(range = c(500,1250)), levels=3 ) Here we set up tuning grid and tune our model on the cv-folds weve set up. On an initial test run using 20k Yelp reviews (~9k after balancing) this took ~480 seconds (8 min). With 11,106 training inputs and only 1-grams it takes ~1200s (20 min) on my machine. With 11,106 training inputs and 1-, 2-, and 3-grams it takes ~ 2300s (38 min). set.seed(1234) tic() tune_rs_svm &lt;- tune_grid( svm_wf, yelp_folds, grid = param_grid, metrics = metric_set(accuracy, sensitivity, specificity), control = control_resamples(save_pred = TRUE) ) toc() Now we can evaluate our modeling by looking at the accuracy across our tuning grid: show_best(tune_rs_svm, metric = &quot;accuracy&quot;) %&gt;% knitr::kable() max_tokens .metric .estimator mean n std_err .config 1250 accuracy binary 0.8928513 10 0.0036235 Preprocessor3_Model1 875 accuracy binary 0.8913215 10 0.0025463 Preprocessor2_Model1 500 accuracy binary 0.8789849 10 0.0029043 Preprocessor1_Model1 The accuracy was particularly surprisingly good across for each number of tokens. Since we got the best results using 1250 tokens, well use 1250 for the rest of our experiment. Then we finalize our workflow using the results of our model tuning. best_accuracy &lt;- select_best(tune_rs_svm, &quot;accuracy&quot;) svm_wf_final &lt;- finalize_workflow( svm_wf, best_accuracy ) At this point, Silge and Hvitfeldt (2020) says we use last_fit() to fit our model to our training data and evaluate it on our testing data. On initial runs, this worked and gave a final accuracy rate of roughly 83%. But two problems arose: It stopped working reliably! I have no idea what changed, but all of my code started to crash at the last_fit() stage. Even behind-the-scenes scratch work now crashes, despite working fine a few days ago. But then it worked again when I tried to knit the final version of this document! When it did work, I couldnt use the final fit object to make predictions. After reading the documentation, it does seem that objects created by last_fit() includes the fitted workflow in a list column called .workflow. However, it took me a while to figure this out, and by the time I did last_fit() had stopped working. When it did work unexpectedly I saved the results to file, and the rest of this report uses the saved results. Here is the the code that worked sporadically: # get final results final_res &lt;- svm_wf_final %&gt;% last_fit(yelp_split, metrics = metric_set(accuracy)) #save(list = &quot;final_res&quot;, file = &quot;data/final_res.Rdata&quot;) load(&quot;data/final_res.Rdata&quot;) We can then see the results which look quite good, with roughly 83% accuracy on the test data: # Then we can see results with `collect_metrics()` and `collect_predictions()`. final_res_metrics &lt;- collect_metrics(final_res) final_res_predictions &lt;- collect_predictions(final_res) final_res_metrics %&gt;% knitr::kable() .metric .estimator .estimate .config accuracy binary 0.8267608 Preprocessor1_Model1 And Figure ?? shows a heatmap of the confusion matrix. The off-diagonals look reasonably symmetric, so the model isnt biased significantly. # Visualize the model&#39;s performance with a heatmap of the confusion matrix. # When it worked, the results were nearly symmetric. final_res_predictions %&gt;% conf_mat(truth = rating_factor, estimate = .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) 9.5 Predicting with last_fit() Now well try to use the results from last_fit() to predict a new result. Here well pull the fitted workflow and try to use it to predict the values for the test set. But this code fails! wf &lt;- final_res$.workflow[[1]] test &lt;- wf %&gt;% predict(new_data = yelp_test[1:10,]) yelp_test %&gt;% bind_cols(test) %&gt;% mutate(correct = (.pred_class == rating_factor)) %&gt;% summarise(sum(correct) / n()) We get the following output: SVM not known from cookie 35 cookies.size: 0! Error in test.liquidSVM(model = object, newdata = newdata, labels = 0, : Should not happen!! liquid_svm_test This concludes my experiment with last_fit(). 9.6 Fitting and Predicting Using fit() Instead of using last_fit(), we should be able to just use fit() to fit our final workflow to our training data. This creates a fitted workflow object that includes our preprocessing recipe and our fitted model. Its a 20.4 MB file when saved to disk. final_fit &lt;- fit(svm_wf_final, data = yelp_train) final_fit ## == Workflow [trained] =============================================================================================================================================================================================================== ## Preprocessor: Recipe ## Model: svm_rbf() ## ## -- Preprocessor --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## SVM model on 877 features (cookie=33) ## Formula: new(&quot;formula&quot;, .S3Class = &quot;formula&quot;, structure(list(), class = &quot;formula&quot;, .Environment = &lt;environment&gt;)) ## trained and selected on a 10x706 grid ## has a $last_result because there has been predicting or testing Then we can use our final_fit object to predict rating factors for our test data. preds &lt;- final_fit %&gt;% predict(new_data = yelp_test) To evaluate our fit, we can bind our prediction columns to our test data and check to see how often the true and predicted ratings agree. yelp_test &lt;- bind_cols(yelp_test, preds) %&gt;% mutate(correct = (.pred_class == rating_factor)) yelp_test %&gt;% summarise(sum(correct) / n()) ## # A tibble: 1 x 1 ## `sum(correct)/n()` ## &lt;dbl&gt; ## 1 0.671 But now our accuracy drops to 67% and Im not sure why! According to the help docs last_fit() is supposed to [f]it the final best model to the training set and evaluate the test set, and thats exactly what I did above. But the results here are quite different. Since the SVM process is slow and since Ive had some kind of toolchain breakdown, I wasnt able to get to the root of the problem in time for this weeks report. 9.7 Conclusions In this section I created a support vector machine (SVM) classifier model to predict whether Yelp reviews were positive or negative based on their text. In an initial experiment, according to tune::last_fit() the model achieved ~89% testing accuracy on~11,000 observations, and 83% accuracy on ~211,000 testing observations. However, on subsequent runs tune::last_fit() stopped working. When it started to work again, I wasnt able to use the fitted model to make predictions. I then fit the final model manually on the training data and tested it against the test data, but the accuracy dropped to 67%. 9.8 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 ## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] vip_0.2.2 glmnet_4.0-2 Matrix_1.2-18 lubridate_1.7.9 ggridges_0.5.2 ## [6] tictoc_1.0 discrim_0.1.1 yardstick_0.0.7 workflows_0.2.0 tune_0.1.1 ## [11] rsample_0.0.8 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 ## [16] scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 textrecipes_0.3.0 recipes_0.1.13 ## [21] tidytext_0.2.5 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 ## [26] readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 ## [5] rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 ## [9] SnowballC_0.7.0 prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 ## [13] codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 ## [17] pROC_1.16.2 dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 ## [21] backports_1.1.7 assertthat_0.2.1 cli_2.0.2 htmltools_0.5.0 ## [25] tools_4.0.2 gtable_0.3.0 glue_1.4.1 rappdirs_0.3.1 ## [29] Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 ## [33] iterators_1.0.12 timeDate_3043.102 gower_0.2.2 xfun_0.16 ## [37] globals_0.13.0 stopwords_2.0 rvest_0.3.6 lifecycle_0.2.0 ## [41] future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 hms_0.5.3 ## [45] parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 ## [49] stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 ## [53] textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 ## [57] lava_1.6.8 rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 ## [61] lattice_0.20-41 tidyselect_1.1.0 bookdown_0.20 plyr_1.8.6 ## [65] magrittr_1.5 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [69] pillar_1.4.6 haven_2.3.1 withr_2.2.0 survival_3.1-12 ## [73] nnet_7.3-14 janeaustenr_0.1.5 modelr_0.1.8 crayon_1.3.4 ## [77] utf8_1.1.4 rmarkdown_2.3 usethis_1.6.1 grid_4.0.2 ## [81] readxl_1.3.1 liquidSVM_1.2.4 blob_1.2.1 reprex_0.3.0 ## [85] digest_0.6.25 GPfit_1.0-8 munsell_0.5.0 9.9 References References "],["building-a-shiny-app.html", "Chapter 10 Building a Shiny App 10.1 The App! 10.2 Exporting Our Models 10.3 Prepping and Testing Our Models 10.4 Saving the Data 10.5 The Logic of the App 10.6 Putting it All Together", " Chapter 10 Building a Shiny App 10.1 The App! Lets start with the fun stuff: the completed app. If the app doesnt load in the window below, see it in action at this link. knitr::include_app(url=&quot;https://chris31415926535.shinyapps.io/11-shiny-app/&quot;, height = &quot;600px&quot;) 10.2 Exporting Our Models To start, well train the model on the full balanced Yelp dataset of ~211k reviews. Recall that we broke input data into five quantiles and fit a logistic regression on each quantile. Then well save the information we need to run this model in an app, namely the logistic regression coefficients and the quantile boundaries in word numbers. But the model objects that we get from glm are enormous! Their size in memory is approximately 282 megabytes. # create and extract a list of 5 models, one for each quintile models &lt;- yelp_data %&gt;% group_by(qtile) %&gt;% nest() %&gt;% mutate(logit = purrr::map(data, glm, formula = rating_factor ~ afinn_mean + buts_nots, family = &quot;binomial&quot;)) %&gt;% select(qtile, logit) %&gt;% arrange(qtile) object.size(models) %&gt;% format(units = &quot;Kb&quot;) ## [1] &quot;276215.9 Kb&quot; And according to this link, its actually the even larger serialized size that matters. Either way thats way too big, since all we actually need is some numeric coefficients. The glm objects have a bunch of extra baggage in them including the entire dataset used to create the model and the residuals. I tried setting a lot of model pieces to NULL to get the size down, but I couldnt get the 5 model objects below about 20 megs. But all we need is the coefficients, so lets try extracting those. Since weve arranged() the models by quantile, we can extract the coefficients into an ordered list. model_coefs &lt;- models %&gt;% pull(logit) %&gt;% purrr::map(coefficients) model_coefs %&gt;% object.size() %&gt;% format(units = &quot;Kb&quot;) ## [1] &quot;2.4 Kb&quot; This fits into about 2.4 Kb, for a compression ratio of roughly 8.688855310^{-6}. 10.3 Prepping and Testing Our Models Of course then to use these coefficients we need to put them into the right equation. A quick trip to the Wikipedia page for logistic regression will remind us that, in this case, the probability that a text input has classification POS is: \\(P = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 )}}\\) So we can define a function to get this probability given some prepared input data and a named vector of coefficients: # function to get probability of classification get_prob &lt;- function (input_data, coefs){ # first get log odds log_odds &lt;- coefs[&quot;(Intercept)&quot;] + coefs[&quot;afinn_mean&quot;] * input_data$afinn_mean + coefs[&quot;buts_nots&quot;] * input_data$buts_nots %&gt;% unname() # then get prob prob &lt;- 1 / (1 + exp(-log_odds)) %&gt;% unname() return (prob) } Before we can test get_prob(), we need to define a helper function to prepare some input text by calculating its mean AFINN score and the number of buts and nots. # function to prepare a text vector and return a prepared tibble with afinn_mean and buts_nots prepare &lt;- function(text) { input_data &lt;- tibble(text = text) input_data &lt;- input_data %&gt;% tidytext::unnest_tokens(output = word, input = text) %&gt;% left_join(afinn, by=&quot;word&quot;) %&gt;% summarise(afinn_mean = mean(value, na.rm = T)) %&gt;% mutate(afinn_mean = if_else(is.na(afinn_mean) | is.nan(afinn_mean), 0, afinn_mean)) %&gt;% bind_cols(input_data) %&gt;% mutate(buts = stringr::str_count(text, &quot;but &quot;), nots = stringr::str_count(text, &quot;not &quot;), buts_nots = buts + nots) return(input_data) } Now we can use our model to calculate the probability that a sample input text, say I am happy, is a positive review: get_prob(prepare(&quot;I am happy&quot;), model_coefs[[1]]) ## [1] 0.9520802 95% seems good enough for me. As a check, we can calculate the same probability using our glm object and predict(): models$logit[[1]] %&gt;% predict(prepare(&quot;I am happy&quot;), type = &quot;response&quot;) %&gt;% unname() ## [1] 0.9520802 We get the exact same result down to 7 decimal points, so we can be confident that weve set up the equations right. We also need to extract the quantile boundaries, so that we know which model to apply to a given input text: # how many quantiles? num_qtiles &lt;- 5 # get the limits of the word-quantiles for display purposes qtiles &lt;- quantile(yelp_data$words, probs = seq(0, 1, (1/num_qtiles))) qtiles ## 0% 20% 40% 60% 80% 100% ## 1 39 65 102 169 1033 10.4 Saving the Data Now we can save all of these values to file so we can load them and use them later. The final file size is about 13 kilobytes. # save the models and quantile boundaries #save(list = c(&quot;model_coefs&quot;,&quot;qtiles&quot;), file = &quot;model_specs.Rdata&quot;) 10.5 The Logic of the App The app should have two inputs: A text box for entering a review. A button that says something like Predict. When the user pushes the button, the app should do the following: Take the text in the input box. prepare() it using our function. Figure out which model applies (i.e. how many words is it, which quantile does it fall into). Use the right model to predict the probability of a POS review. Display the probability, and either POS or NEG depending on whether \\(p&gt;0.5\\). Lets define a function to get a texts quintile: # function to get quintile get_qtile &lt;- function(text, qtiles = qtiles){ # count words: count the number of spaces and add 1 words &lt;- stringr::str_count(text, &quot; &quot;) + 1 qtile &lt;- case_when( words %in% qtiles[1]:qtiles[2] ~ 1, words %in% qtiles[2]:qtiles[3] ~ 2, words %in% qtiles[3]:qtiles[4] ~ 3, words %in% qtiles[4]:qtiles[5] ~ 4, words &gt; qtiles[5] ~ 5 ) return(qtile) } And a function that pulls it all together to predict the probability that a given review is positive: prob_text &lt;- function(text, model_coefs, qtiles){ # get quintile for text based on word length qtile &lt;- get_qtile(text, qtiles = qtiles) # prepare the text by getting afinn sentiment and counting buts/nots prepped_text &lt;- prepare(text) # get the probability this text is positive prob &lt;- get_prob(prepped_text, model_coefs[[qtile]]) # return the probability return(prob) } Lets test: user_text &lt;- &quot;I am happy.&quot; prob_text(user_text, model_coefs, qtiles) ## [1] 0.9520802 And one with some negative words and a negator: user_text &lt;- &quot;I am not happy, this place sucks.&quot; prob_text(user_text, model_coefs, qtiles) ## [1] 0.1634059 And lets make another very simple function that returns POS or NEG based on the probability. In practice it will give POS if the probability is &gt;50%, but in principle we could set the threshold anywhere depending on our cost function. Well test this function with the same negative review we just used. pred_text &lt;- function(prob, threshold = 0.5){ if (prob &gt;= threshold) result &lt;- &quot;POS&quot; if (prob &lt;= threshold) result &lt;- &quot;NEG&quot; return (result) } prob_text(user_text, model_coefs, qtiles) %&gt;% pred_text() ## [1] &quot;NEG&quot; 10.6 Putting it All Together Now that we have our logic and basic design, the next step is to build and deploy the Shiny app. This is easier to show than it is to tell, so please check out: The deployed app here on Shinyapps.io The app code here on GitHub "],["references-2.html", "References", " References "]]
