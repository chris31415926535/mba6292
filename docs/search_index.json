[["web-scraping-our-data.html", "Chapter 3 Web Scraping Our Data 3.1 Foreword: Ethics 3.2 The General Idea 3.3 Goodreads: CSS Selectors 3.4 Yelp: Embedded JSON 3.5 MEC: Reverse-Engineering Client-Side API Calls 3.6 Summary", " Chapter 3 Web Scraping Our Data In this section I will discuss ethical concerns related to web scraping and describe three different approaches I used to scrape datasets from Yelp, Goodreads, and MEC. 3.1 Foreword: Ethics I this section I will briefly consider some ethical aspects of web scraping. Although its a rich topic, my treatment here will be superficial and my conclusion will be that this project is fine. What is web scraping? As a working definition, lets say that web scrapingwhich can also be called crawling, trawling, indexing, harvesting, or any number of other termsmeans automatically visiting websites to collect and store information. So at one extreme, browsing Facebook at work doesnt count since its not automatic. On the other extreme, automatically sending billions of requests to a server in an attempt to overload (i.e. a DDoS attack) doesnt count either, since nothing is being done with the information the server sends back. Why might web scraping be wrong? Here Ill consider three potential objections based on access, burdening the scrapee, and purpose, and show how Ive designed this project to mitigate those concerns. Web scraping might be wrong if were taking things were not supposed to have access to. For example, if data were held on a password-protected server, one might think it wrong to collect it all automatically and re-create that dataset elsewhere. To mitigate this concern, we will only scrape publicly accessible data. Web scraping might be wrong if it posed an undue burden on the sites were scraping. For example, if we were to scrape millions of pages or records from a single site in a short time, it might overload their servers or disrupt other peoples access. To mitigate this concern, we can scrape a smallish number of pages and spread our requests out so that we dont overload any servers. Web scraping might be wrong if we were to use the data we collect unethically. As an example, one might think it would be unethical to scrape data and use it for political or financial purposes. To mitigate this concern, we will only use the data we collect for non-commercial educational purposes. Note also that web scraping is an extremely common business model. To take an obvious example, Googles entire search business is based on information it has extracted from websitesin other words, web scraping (Google 2020). Beyond Google, news agencies report that between 30% and 50% of all web traffic may be from automated web-scraping bots (Bruell 2018; LaFrance 2017). And programming languages, including R, ship with packages that make web scraping relatively easy (Wickham 2020). So we can say at least that in some cases, web scraping on a massive scale is a commonly accepted business practice. In summary, in this project Ive made the following choices to mitigate ethical concerns about web scraping: Were only scraping publicly accessible information; Were scraping a reasonably small number of pages/reviews; Were being considerate of their servers by spacing out our requests; and, Were collecting and using the data for educational non-commercial purposes. 3.2 The General Idea Web scraping these sites follows a two-step process: Get a list of urls for pages you want to scrape (generating an index). Usually well get these urls by first scraping another page. Use a loop to scrape the information from each page (loading the content). Since different sites have different structures, well need custom code for the index and content pages. Also, by random chance these three sites all use different web-design principles, so well also need to use different techniques. 3.3 Goodreads: CSS Selectors Goodreads describes itself as the worlds largest site for readers and book recommendations (Goodreads (2020)). Registered users can leave reviews and ratings for books, any anyone can use the site to browse user-submitted reviews and a variety of information about books. Goodreads pages are standard html, so we can use css selectors to isolate the exact parts of the page were interested in. I used Rs rvest package, and the package documentation has details about the methods and about css selectors in general (Wickham 2020). To find the css selectors I used SelectorGadget, a point-and-click Chrome extension. 3.3.1 Scraping the Index Goodreads assigns books to genres like sci-fi and romance, and curates lists of each genres 100 most-read books in the past week. By scraping these pages, we can get links to content pages for hundreds of books across different genres. Here is a code block to get the links to the 100 most-read books in the classics genre. The code could be functionized or run several times for other genres. library(tidyverse) library(rvest) # choose a genre genre &lt;- &quot;classics&quot; url &lt;- paste0(&quot;https://www.goodreads.com/genres/most_read/&quot;,genre) # read the page page &lt;- read_html(url) # extract the links to each book&#39;s page using css selectors book_links &lt;- page %&gt;% html_nodes(&quot;.coverWrapper&quot;) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% paste0(&quot;https://www.goodreads.com&quot;, .) book_links %&gt;% as_tibble() %&gt;% write_csv(paste0(&quot;book_links_&quot;,genre,&quot;.csv&quot;)) 3.3.2 Scraping the Content The next step is to load each content link and extract the information about the book, its author, and all the reviews. Books can have several pages of reviews, so we need to figure out how mayn pages there are and how to crawl through them. Since not all reviews have both text and star ratings, we also need to be careful to make sure we handle missing data appropriately. #https://www.goodreads.com/genres/most_read/non-fiction links &lt;- book_links # set up empty results tibble results &lt;- tibble() # remove any links we&#39;ve already seen, if we crashed and are resuming links &lt;- links[!links %in% results$url] for (i in 1:length(links)) { # pause briefly pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,length(links),&quot;: &quot;, url)) # choose a random useragent each time we load the page -- anti-anti-scraping measure httr::user_agent(random_useragent()) %&gt;% httr::set_config() # read the url page &lt;- read_html(url) # read the review page&#39;s html reviews_html &lt;- page %&gt;% html_nodes(&quot;.review&quot;) # extract the informaiton we&#39;re interested in book_title &lt;- page %&gt;% html_nodes(&quot;#bookTitle&quot;) %&gt;% html_text() %&gt;% stringr::str_trim() author_name &lt;- page %&gt;% html_nodes(&quot;.authorName span&quot;) %&gt;% html_text() %&gt;% head(1) review_names &lt;- purrr::map_chr(reviews_html, function(x) { html_nodes(x, &quot;.user&quot;) %&gt;% html_text() }) review_dates &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.reviewDate&quot;) %&gt;% html_text()}) review_text &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.readable span&quot;) %&gt;% html_text() %&gt;% paste0(., &quot; &quot;) %&gt;% na_if(y=&quot; &quot;) %&gt;% str_trim() %&gt;% tail(1)}) review_rating &lt;- purrr::map_chr(reviews_html, function(x) {html_nodes(x, &quot;.staticStars&quot;) %&gt;% html_text() %&gt;% paste0(., &quot; &quot;) %&gt;% na_if(y=&quot; &quot;) %&gt;% str_trim()}) # how many pages of reviews? # there may be an easier way but this should work num_pages &lt;- page %&gt;% html_text() %&gt;% str_extract_all(&quot;(?&lt;=previous).*?(?=next)&quot;) %&gt;% unlist() %&gt;% tail(1) %&gt;% stringr::str_trim() %&gt;% stringr::str_split(&quot; &quot;) %&gt;% unlist() %&gt;% map_dbl(as.double) %&gt;% max() # put it all together page_reviews &lt;- tibble( book_title = book_title, author_name = author_name, comment = review_text, names = review_names, rating = review_rating, dates = lubridate::mdy(review_dates), url = url, num_pages = num_pages ) results &lt;- bind_rows(results, page_reviews) } filename &lt;- paste0(&quot;goodreads_&quot;,genre,&quot;_reviews.csv&quot;) results %&gt;% write_csv(path = filename) 3.4 Yelp: Embedded JSON Yelp, according to its website, connects people with great local businesses (Yelp 2020). Businesses can upload information like their location, hours, and services, and registered users can can leave reviews with text, star ratings, and pictures. Yelps web design includes structured json data within its html. In other words, each Yelp review page has machine-readable data hidden inside it if you know where to look. Well exploit this by using a regular expression to extract the json from the html, then parse the json and work with it directly. 3.4.1 Scraping the Index First well get the urls for each restaurant in Ottawa. We start at the base url for Ottawa restaurants and iterate through all of the pages: we could get the page numbers automatically, but here I just saw that there are 24 and hard-coded that number in. We extract the urls from the json in the page without parsing it using a regex. We could have parsed the json and done it using structured data, but since were only looking for one value type this was faster and worked fine. # the base url for restaurants in Ottawa baseurl &lt;- &quot;https://www.yelp.ca/search?cflt=restaurants&amp;find_loc=Ottawa%2C%20Ontario%2C%20CA&quot; # an empty tibble for our links links &lt;- tibble() # loop through all 24 pages of Ottawa restaurants. (The number 24 was hard-coded to keep things moving.) for (pagenum in 1:24){ Sys.sleep(1) # get the url for the page we&#39;re loading url &lt;- paste0(baseurl, if(pagenum&gt;1){ paste0(&quot;&amp;start=&quot;,(pagenum-1)*10) }) # load the html for the page and print an update message text &lt;- read_html(url) %&gt;% html_text() message(&quot;**PAGE &quot;,pagenum,&quot;: &quot;, url) # extract the urls using a straight regex based on the json value key. we&#39;re not parsing any json here. urls &lt;- text %&gt;% str_extract_all(&#39;(?&lt;=businessUrl&quot;:&quot;)(.*?)(?=&quot;)&#39;) %&gt;% unlist() %&gt;% enframe() %&gt;% select(-name) %&gt;% filter (!str_detect(value, &quot;ad_business_id&quot;)) %&gt;% distinct() %&gt;% transmute(url = paste0(&quot;http://www.yelp.ca&quot;, value)) # add to our results links &lt;- bind_rows(links, urls) } links %&gt;% write_csv(&quot;yelp_ottawa_links.csv&quot;) 3.4.2 Scraping the Content Scraping the content has two steps. First, now that we have a list of content urls, we can load each in turn and extract the reviews and the links for any additional review pages for this business. In the second step well load these new links and get those reviews. This function loads a single review page, extracts the machine-readable json using a regex, parses the json, and extracts the information were interested in. It then returns that information in a tibble. get_review &lt;- function(page, url) { # get the html text &lt;- page %&gt;% html_text() # extract the json with the review data json_text &lt;- text %&gt;% str_extract(&#39;(?&lt;=&quot;reviewFeedQueryProps&quot;:)(.*)(&quot;query&quot;:&quot;&quot;\\\\}\\\\})&#39;) # set our review_page results variable to NA, in case we don&#39;t get a results review_page &lt;- NA # make sure we have valid json text before we try to parse it if (!is.na(json_text)){ # parse the json json_parse &lt;- json_text %&gt;% jsonlite::fromJSON() # pull out the variables we&#39;re interested in review_text &lt;- json_parse$reviews$comment$text review_rating &lt;- json_parse$reviews$rating review_name &lt;- json_parse$reviews$user$markupDisplayName review_date &lt;- json_parse$reviews$localizedDate review_business &lt;- json_parse$reviews$business$name review_url &lt;- rep(url, length(review_text)) # put them all into a tibble review_page &lt;- tibble(business = review_business, name = review_name, date = review_date, comment = review_text, rating = review_rating, url = review_url) } # return either NA or a results tibble return (review_page) } # simple function to pause for a random period of time pause &lt;- function(min_wait = 1, max_wait = 3){ runif(n=1, min=min_wait, max = max_wait) %&gt;% Sys.sleep() } We then proceed with step one, loading the initial list of links, extracting the reviews there, and collecting any more links to more reviews: # load our set of restaurant page links base_links &lt;- read_csv(&quot;yelp_ottawa_links.csv&quot;) # set up an empty tibble for our reviews reviews &lt;- tibble() # set up an empty tibble for the links we&#39;re going to visit later more_links &lt;- tribble(~links) # now we&#39;re going to visit each page, extract the reviews from it, and find out how many *more* pages there are for this restaurant. # we&#39;ll keep track of those other pages and visit them later in a random order. for (i in 1:nrow(base_links)) { # pause briefly pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,nrow(base_links),&quot;: &quot;, url)) # read the url page &lt;- read_html(url) # extract the reviews from the page review_page &lt;- get_review(page, url) # add these reviews to our list of reviews reviews &lt;- bind_rows(reviews, review_page) # now find out how many other pages there are for this restaurant # we&#39;ll regex to find the second half of &quot;dd of dd&quot;, where d is a digit (and it could be either one or two digits--see the regex below) num_pages &lt;- page %&gt;% html_node((&quot;.text-align--center__373c0__2n2yQ .text-align--left__373c0__2XGa-&quot;)) %&gt;% html_text() %&gt;% str_extract(&quot;(?&lt;=of )(\\\\d\\\\d?)&quot;) %&gt;% as.integer() # make sure we don&#39;t get an NA if (is.na(num_pages)) num_pages &lt;- 1 # if there&#39;s more than one page, construct the links and add them to our list of links to read next if (num_pages &gt; 1) { more_links &lt;- more_links %&gt;% add_row(links = paste0(url, &quot;?start=&quot;,(1:(num_pages-1))*20) ) } } # end for i in 1:nrow(base_links) # save our results reviews %&gt;% write_csv(&quot;data/ottawa-reviews-1.csv&quot;) more_links %&gt;% write_csv(&quot;data/ottawa_more_links.csv&quot;) In step two, well repeat the process for the new links we collected: Now lets do the same thing for the extra links we got: note its stopping me every 136 or so and giving a 503 error, so im either rebooting my modem to get a new ip address or tethering to my phone for a bit links &lt;- more_links for (i in 1:length(links)) { # pause briefly for random interval pause() # get the url we&#39;re interested in url &lt;- links[[i]] # write an update, since I&#39;m impatient and want to know what&#39;s happening message(paste0(i,&quot;/&quot;,length(links),&quot;: &quot;, url)) message(&quot; Loading page.&quot;) # read the url page &lt;- read_html(url) message(&quot; Parsing review.&quot;) # extract the reviews from the page review_page &lt;- get_review(page, url) if (!is.na(review_page)){ message (&quot; Adding to inventory.&quot;) # add these reviews to our list of reviews reviews &lt;- bind_rows(reviews, review_page) } else { message (&quot; No valid json found.&quot;) } } # end for i in 1:nrow(base_links) reviews %&gt;% write_csv(&quot;ottawa-reviews-2.csv&quot;) 3.5 MEC: Reverse-Engineering Client-Side API Calls MECs website uses a completely different design principle that makes it seem more difficult to extract information. If you inspect the html for one of MECs product pages, youll find that the review information simply isnt there! Its quite mysterious. The secret is that MECs site uses client-side API calls to download the data which is then displayed locally. To solve this puzzle, I needed to use Chromes developer console (opened with Control-Shift-J) to see the network activity (under the Network tab) happening each time I loaded a new product page. I discovered that my browser was making API calls to a specific server, and by comparing the calls for a few products I found that the main difference was the product ID. This let me reverse-engineer the syntax just enough to be able to call it myself and get reviews for any product based on its ID. I also found that there was one API call for the first page of reviews and a different one for loading more reviews, so I built functions for both of them. As a result, the index in this case is a list of product IDs rather than urls, and the content is the result of API calls rather than web pages. However, the principles remain the same. 3.5.1 Scraping the Index This code block collects product IDs for mittens and gloves. Each product category has a different catalogue page, so I modified the code to load a few different kinds of products. We load the first page, use a regex to figure out how many pages there are, then use css selectors to extract the IDs for products with reviews. # enter the base url by hand base_url &lt;- &quot;https://www.mec.ca/en/products/clothing/clothing-accessories/gloves-and-mittens/c/987&quot; # enter the product type by hand product_type &lt;- &quot;gloves-and-mittens&quot; # read the page page &lt;- read_html(base_url) # get the number of items using a CSS selector and a regex # we expect to find between one and three digits num_items &lt;- page %&gt;% html_nodes(&quot;.qa-filter-group__count&quot;) %&gt;% html_text() %&gt;% str_extract(&quot;(\\\\d\\\\d?\\\\d?)&quot;) %&gt;% as.integer() # there are at most 36 items per page num_pages &lt;- (num_items / 36) %&gt;% ceiling() # first let&#39;s do the items on this page # find each link to a product, filter out any that don&#39;t have reviews yet, extract the product ids product_ids &lt;- page %&gt;% html_nodes(&quot;.rating__count__link&quot;) %&gt;% html_attrs() %&gt;% enframe() %&gt;% unnest_wider(value) %&gt;% filter(!str_detect(title, &quot;No reviews yet&quot;)) %&gt;% mutate(product_id = str_extract(href, &quot;\\\\d\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d&quot;)) %&gt;% select(-name, -class) # now we load the extra pages, if there are any if (num_pages &gt; 1) { # we iterate from 1 to num_pages-1, because MEC calls the first extra page page 1 for (i in 1:(num_pages-1)){ # send an update to the console message(paste0(i,&quot;/&quot;,(num_pages-1))) # wait a little bit patience(min_wait = 3, max_wait = 10) # get the new url for the next page url &lt;- paste0(base_url,&quot;?page=&quot;,i) # load the next page page &lt;- read_html(base_url) # find each link to a product, filter out any that don&#39;t have reviews yet, extract the product ids new_product_ids &lt;- page %&gt;% html_nodes(&quot;.rating__count__link&quot;) %&gt;% html_attrs() %&gt;% enframe() %&gt;% unnest_wider(value) %&gt;% filter(!str_detect(title, &quot;No reviews yet&quot;)) %&gt;% mutate(product_id = str_extract(href, &quot;\\\\d\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d&quot;)) %&gt;% select(-name, -class) # add it to our list product_ids &lt;- bind_rows(product_ids, new_product_ids) } # end for (i in 1:(num_pages-1)) } # end if (num_pages &gt;1) product_ids %&gt;% write_csv(paste0(&quot;data/product_ids_&quot;,product_type,&quot;.csv&quot;)) 3.5.2 Functions for API Calls Next, I defined functions to make the API calls and to process their results. The API calls are quite uglyI could have spent more time figuring out exactly how they worked and slimmed them down, but this worked. get_first_api_url &lt;- function(product_code){ api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=products&amp;filter.q0=id%3Aeq%3A&quot;,product_code,&quot;&amp;stats.q0=questions%2Creviews&amp;filteredstats.q0=questions%2Creviews&amp;filter_questions.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_answers.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;resource.q1=questions&amp;filter.q1=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q1=totalanswercount%3Adesc&amp;stats.q1=questions&amp;filteredstats.q1=questions&amp;include.q1=authors%2Cproducts%2Canswers&amp;filter_questions.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_answers.q1=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q1=10&amp;offset.q1=0&amp;limit_answers.q1=10&amp;resource.q2=reviews&amp;filter.q2=isratingsonly%3Aeq%3Afalse&amp;filter.q2=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q2=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q2=reviews&amp;filteredstats.q2=reviews&amp;include.q2=authors%2Cproducts%2Ccomments&amp;filter_reviews.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q2=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q2=8&amp;offset.q2=0&amp;limit_comments.q2=3&amp;resource.q3=reviews&amp;filter.q3=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q3=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q3=1&amp;resource.q4=reviews&amp;filter.q4=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q4=isratingsonly%3Aeq%3Afalse&amp;filter.q4=issyndicated%3Aeq%3Afalse&amp;filter.q4=rating%3Agt%3A3&amp;filter.q4=totalpositivefeedbackcount%3Agte%3A3&amp;filter.q4=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q4=totalpositivefeedbackcount%3Adesc&amp;include.q4=authors%2Creviews%2Cproducts&amp;filter_reviews.q4=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q4=1&amp;resource.q5=reviews&amp;filter.q5=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q5=isratingsonly%3Aeq%3Afalse&amp;filter.q5=issyndicated%3Aeq%3Afalse&amp;filter.q5=rating%3Alte%3A3&amp;filter.q5=totalpositivefeedbackcount%3Agte%3A3&amp;filter.q5=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q5=totalpositivefeedbackcount%3Adesc&amp;include.q5=authors%2Creviews%2Cproducts&amp;filter_reviews.q5=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q5=1&amp;callback=BV._internal.dataHandler0&quot;) return(api_url) } get_second_api_url &lt;- function(product_code){ api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=reviews&amp;filter.q0=isratingsonly%3Aeq%3Afalse&amp;filter.q0=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q0=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q0=reviews&amp;filteredstats.q0=reviews&amp;include.q0=authors%2Cproducts%2Ccomments&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q0=30&amp;offset.q0=8&amp;limit_comments.q0=3&amp;callback=bv_351_44883&quot;) #api_url &lt;- paste0(&quot;https://api.bazaarvoice.com/data/batch.json?passkey=dm7fc6czngulvbz4o3ju0ld9f&amp;apiversion=5.5&amp;displaycode=9421-en_ca&amp;resource.q0=reviews&amp;filter.q0=isratingsonly%3Aeq%3Afalse&amp;filter.q0=productid%3Aeq%3A&quot;,product_code,&quot;&amp;filter.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;sort.q0=helpfulness%3Adesc%2Ctotalpositivefeedbackcount%3Adesc&amp;stats.q0=reviews&amp;filteredstats.q0=reviews&amp;include.q0=authors%2Cproducts%2Ccomments&amp;filter_reviews.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_reviewcomments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;filter_comments.q0=contentlocale%3Aeq%3Aen*%2Cfr_CA%2Cen_CA&amp;limit.q0=500&amp;offset.q0=0&amp;limit_comments.q0=3&amp;callback=bv_351_44883&quot;) return(api_url) } # function to go through the list and extract meaningful results get_review &lt;- function(x){ product_id &lt;- ifelse(!is.null(x$ProductId), x$ProductId, &quot;&quot;) user_name &lt;- ifelse(!is.null(x$UserNickname), x$UserNickname, &quot;&quot;) rating &lt;- ifelse(!is.null(x$Rating), x$Rating, 0) review_date &lt;- ifelse(!is.null(x$SubmissionTime), x$SubmissionTime, &quot;&quot;) review_text &lt;- ifelse(!is.null(x$ReviewText), x$ReviewText, &quot;&quot;) review_title &lt;- ifelse(!is.null(x$Title), x$Title, &quot;&quot;) # message (&quot;sofa sogood&quot;) results &lt;- tibble( product_id = product_id, user_name= user_name, rating_num = rating, review_date =review_date, review_text =review_text , review_title = review_title ) return(results) } 3.5.3 Scraping the Content Now that we have the product ids, we can loop through them and call the API to get the reviews. # set up our results tibble all_reviews &lt;- tibble() # #20 seems to have no reviews, json_results didn&#39;t have SubmissionTime, so added that to conditions for (i in 1:nrow(product_ids)){ # print an update message and wait nicely product_id &lt;- product_ids$product_id[[i]] message(paste0(&quot;Product #&quot;,i,&quot;/&quot;,nrow(product_ids),&quot;: &quot;,product_id)) Sys.sleep(2) api_url &lt;- get_first_api_url(product_id) # call the API text &lt;- GET(api_url) %&gt;% content(&quot;text&quot;) # parse the returned text into json json_parsed &lt;- text %&gt;% str_extract(&quot;\\\\{(.*)\\\\}&quot;) %&gt;% #str_extract(&quot;(?&lt;=BV._internal.dataHandler0\\\\()(.*)&quot;)#(?=\\\\))&quot;) %&gt;% jsonlite::parse_json() # get the product information product &lt;- json_parsed$BatchedResults$q0$Results[[1]] product_name &lt;- product$Name product_brand &lt;- product$Brand$Name reviews &lt;- json_parsed$BatchedResults$q2$Results # use purrr::map to apply get_review() to each individual review reviews1 &lt;- tibble( x = purrr::map(reviews, get_review) ) %&gt;% unnest(cols = &quot;x&quot;) message (&quot; First API call done and processed.&quot;) ####################################3 # SECOND API CALL. Try to load additional reviews: api_url &lt;- get_second_api_url(product_id) # test &lt;- read_html(api_url) # text &lt;- test %&gt;% html_text() text &lt;- GET(api_url) %&gt;% content(&quot;text&quot;) json_parsed &lt;- text %&gt;% str_extract(&quot;(?&lt;=\\\\()(.*)(?=\\\\))&quot;) %&gt;% jsonlite::fromJSON() json_results &lt;- json_parsed$BatchedResults$q0$Results # set our second set of reviews to NULL in case we don&#39;t find any reviews2 &lt;- NULL # if we do find some, set them to that! if (!is.null(json_results) &amp; length(json_results)&gt;0) { if (any(str_detect(names(json_results), &quot;SubmissionTime&quot;))){ reviews2 &lt;- json_results %&gt;% as_tibble() %&gt;% select(review_date = SubmissionTime, user_name = UserNickname, review_title = Title, review_text = ReviewText, rating_num = Rating ) %&gt;% mutate(product_id = product_code) } } message (&quot; Second API call done and processed.&quot;) # put the new reviews together: new_reviews &lt;- bind_rows(reviews1, reviews2) %&gt;% mutate(product_name = product_name, product_brand= product_brand) all_reviews &lt;- bind_rows(all_reviews, new_reviews) } # end (for i) all_reviews %&gt;% distinct() %&gt;% write_csv(paste0(&quot;reviews-&quot;,product_type,&quot;.csv&quot;)) 3.6 Summary References "]]
