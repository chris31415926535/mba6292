[
["index.html", "Telfer 2020 Directed Reading: Natural Language Processing Chapter 1 Preface", " Telfer 2020 Directed Reading: Natural Language Processing Christopher Belanger 2020-11-16 Chapter 1 Preface This is a work product for the Telfer School of Management course MBA6292, Directed Readings in Natural Language Processing with Dr. Peter Rabinovitch. This document is intended as a final-work-in-progress: I will continue to revise and add through it during the semester, with the intention that at the end of the semester it will stand as a final product. This document is written in RMarkdown using RStudio and the bookdown package. For details and to learn how to create your own, see Xie (2020). References "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Introduction text will go here. For now, here is some sample text that shows how to label and refer to figures. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["web-scraping-our-data.html", "Chapter 3 Web Scraping Our Data 3.1 Yelp: Embedded JSON 3.2 Goodreads: CSS Selectors 3.3 MEC: Reverse-Engineering Client-Side API Calls", " Chapter 3 Web Scraping Our Data This section will describe the three different approaches I used to scrape the datasets from Yelp, Goodreads, and MEC. 3.1 Yelp: Embedded JSON 3.2 Goodreads: CSS Selectors 3.3 MEC: Reverse-Engineering Client-Side API Calls "],
["data-summary-eda-initial-model-attempts.html", "Chapter 4 Data Summary, EDA, &amp; Initial Model Attempts 4.1 Introduction 4.2 Goodreads 4.3 Yelp 4.4 Mountain Equipment Co-op (MEC) 4.5 Comparing Goodreads, MEC, and Yelp 4.6 Reviews Over Time 4.7 Proposed Next Steps 4.8 SessionInfo", " Chapter 4 Data Summary, EDA, &amp; Initial Model Attempts 4.1 Introduction I have three original datasets for analysis, both of which were collected from public websites between October 21 and 27, 2020. Yelp Reviews: 9,402 reviews for restaurants in Ottawa, which I believe includes all reviews available as of October 21. Each review includes: Business Name: The name the business is listed as operating under on Yelp. (Character) Reviewer Name: The screen name of the user who wrote the review. (Character) Review Date: The date the review was posted. (Character in mm/dd/yyyy format) Review Text: The full text of the review. (Character) Star Rating: The number of stars associated with the review (Integer from 1 to 5) Review URL: The URL from which the review was downloaded for traceability. (Character) Goodreads Reviews: 17,091 book reviews, culled from the first-page reviews of the “100 most-read books” in a number of genres. Each review includes: Book Title: The title of the book. (Character) Book Genre: The Goodreads-assigned genre of the book, e.g. “scifi” or “romance.” (Character) Book Author: The author of the book. (Character) Reviewer Name: The screen name of the user who wrote the review. (Character) Review Date: The date the review was posted. (Character in yyyy-mm-dd format) Review Text: The full text of the review. (Character) Star Text: Goodreads’ text equivalent for star ratings. (Character) Star Rating: The number of stars associated with the review (Integer from 1 to 5) Review URL: The URL from which the review was downloaded for traceability. (Character) Mountain Equipment Co-op (MEC) Reviews: 2,392 reviews for products for sale from MEC. Each review includes: Product Type: MEC’s categorization for the product (e.g. mittens, bicycle components.) (Character) Product Brand: The brand under which the product is marketed on MEC’s website. (Character) Product Name: The name of the product. (Character) Product ID: MEC’s internal product ID, used to call the API. (Character) Reviewer Name: The username of the review writer. (Character) Review Date: The date the review was left. (Character) Review Title: The title of the review. (Character) Review Text: The complete text of the review. (Character) Star Rating: The number of stars associated with the review. (Integer from 1 to 5) In this section, I’ll take a look at these two datasets to get a feel for the star ratings and review text. I will consider each dataset in turn. reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% rename(rating_num = rating) reviews_gr &lt;- read_csv(&quot;../tests/data/goodreads_all.csv&quot;) reviews_mec &lt;- read_csv(&quot;../tests/data/mec-reviews.csv&quot;) %&gt;% rename(comment = review_text, date = review_date) 4.2 Goodreads 4.2.1 Star Ratings The following histogram shows the overall distribution of star ratings. Reviews are overwhelmingly positive: there are move 5-star reviews than there are 1-, 2-, and 3-star reviews combined. This may make modeling more difficult, since there will be fewer low-star ratings to train our models. reviews_gr %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Goodreads Ratings: Rating Count, Overall&quot;, x=&quot;Star Rating&quot;, y=NULL) The next histogram shows that the pattern is broadly consistent across genres. There are some minor differences: for example, graphic-novel and mystery reviews have nearly the same number of 4- and 5-star ratings, whereas nonfiction and romance novels show markedly more 5-star reviews than 4-star reviews. But for present purposes the overall pattern looks largely the same–for example, there are no U-shaped distributions, or exponential-type distributions with the opposite skew. reviews_gr %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Goodreads Ratings: Rating Count by Genre&quot;, x = &quot;Star Rating&quot;, y=NULL) + facet_wrap(facets = vars(genre)) However, if we look at the level of individual books, the distributions look a bit more interesting. All the histograms are unimodal, but some of them peak at 3 or 4. (Poor Brian K. Vaughan.) top_6_books &lt;- reviews_gr %&gt;% group_by(book_title) %&gt;% summarise(n = n()) %&gt;% slice_max(n=6, order_by=n, with_ties=FALSE) %&gt;% pull(book_title) ## `summarise()` ungrouping output (override with `.groups` argument) reviews_gr %&gt;% filter(book_title %in% top_6_books) %&gt;% ggplot(aes(x = rating_num)) + geom_histogram( binwidth=1, boundary=0.5, bins=5) + facet_wrap(facets = vars(book_title)) + theme_grey() + labs(title = &quot;Star Ratings for 6 of the Most-Reviewed Books&quot;, subtitle = &quot;Sampled randomly from across all genres.&quot;, x = &quot;Star Rating&quot;, y = &quot;# of Ratings&quot;) 4.2.2 Word Count Turning to word count, the following graph shows the cumulative density of word counts in our review dataset. In other words, as word count increases on the x-axis, the y-axis shows us how many reviews have at most that many words. I have counted words here using unnest_tokens() from the tidytext package (as per Tidy Text Mining). There may be an easier way, but this worked! We find that most reviews are very short: about 15,000 are below 500 words, and they go as short as one word. Some reviews are quite long, and one stretches out past 3,500 words. wordcounts_gr &lt;- reviews_gr %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_gr %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;Goodreads Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) This distribution may also make our modeling task more difficult. With so many short reviews it’s unlikely that they will have many words in common, and so a lasso regression at the word level may not work very well. However, short reviews may still be useful for sentiment analysis. The following table shows the five shortest reviews, since I wanted to check and make sure it wasn’t a data error. One reviewer left a single word: “SUCKS.” Concise and informative. wordcounts_gr %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_gr, .) %&gt;% select(book_title,author_name, rating_num, comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Book Title&quot;, &quot;Book Author&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Book Title Book Author Stars Review The Alchemist Paulo Coelho 1 SUCKS. The Mysterious Affair at Styles Agatha Christie 5 Classic Siddhartha Hermann Hesse 2 Eh. Treasure Island Robert Louis Stevenson 5 ARRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR… Logan Likes Mary Anne! Gale Galligan 4 cool # # %&gt;% # kableExtra::column_spec(column = 1:4, # width = c(&quot;15cm&quot;,&quot;10cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.2.3 Reviewers The following histogram shows that while most Goodreads users posted only a handful of reviews in our dataset, some posted over 50. reviewers_gr &lt;- reviews_gr %&gt;% group_by(names) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_gr %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;Goodreads: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looking at the following table, we can see that the top 10 reviewers all posted over 50 reviews, and one posted 95. reviewers_gr %&gt;% top_n(10, wt = n) ## # A tibble: 11 x 2 ## names n ## &lt;chr&gt; &lt;int&gt; ## 1 Ahmad Sharabiani 95 ## 2 Lisa 89 ## 3 Matthew 63 ## 4 jessica 61 ## 5 Sean Barrs 59 ## 6 Emily May 56 ## 7 Michelle 53 ## 8 Jennifer 52 ## 9 Elyse Walters 51 ## 10 Melissa &lt;U+2665&gt; Dog/Wolf Lover &lt;U+2665&gt; Martin 50 ## 11 Nilufer Ozmekik 50 Out of curiosity (and as a check on our data quality), let’s investigate the 95 reviews from our top poster, Ahmad Sharabiani: reviews_gr %&gt;% filter(names == &quot;Ahmad Sharabiani&quot;) %&gt;% select(book_title, author_name, rating_num, comment) %&gt;% mutate (comment = str_trunc(comment, 80)) %&gt;% arrange(desc(author_name)) %&gt;% slice_head(n=10) %&gt;% knitr::kable(col.names = c(&quot;Book Title&quot;, &quot;Book Author&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Book Title Book Author Stars Review Romeo and Juliet William Shakespeare 5 Romeo and Juliet = The Tragedy of Romeo and Juliet, William ShakespeareRomeo … Othello William Shakespeare 4 Othello = The Tragedy of Othello, William ShakespeareOthello (The Tragedy of … Othello William Shakespeare 5 The Tragedy of Othello, The Moor of Venice, William ShakespeareOthello is a t… Lord of the Flies William Golding 4 Lord of the flies, William GoldingLord of the Flies is a 1954 novel by N… A Room of One’s Own Virginia Woolf 4 A Room of One’s Own, Virginia WoolfA Room of One’s Own is an extended essay b… Beowulf Unknown 5 Beowulf, Anonymous Anglo-Saxon poetBeowulf is an Old English epic poem consis… In Cold Blood Truman Capote 4 In Cold Blood, Truman CapoteThis article is about the book by Truman Capote. … The Bluest Eye Toni Morrison 4 The Bluest Eye, Toni MorrisonThe Bluest Eye is a novel written by Toni M… The Bell Jar Sylvia Plath 4 Victoria Lucas = The Bell Jar, Sylvia PlathThe Bell Jar is the only nove… The Shining Stephen King 4 The Shining (The Shining #1), Stephen KingThe Shining is a horror novel by Am… # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Something looks a bit suspicious here. First, many books have more than one review (for example, Othello has 2 and The Catcher in the Rye has 3). Second, the reviews all seem to begin with the title of the book and a factual summary without much personality. If we do a Google search for the opening text of Ahmad’s review for Farenheit 451, “Fahrenheit 451 is a dystopian novel by American”, we find that exact text in the first line of the book’s Wikipedia page. Google also suggests we look at Farenheit 451’s Goodreads page, which includes Ahmad’s review. If we look at Ahmad’s review more closely, we see that it includes an English-language summary and then a lot of text in a non-Latin alphabet. reviews_gr %&gt;% filter(names == &quot;Ahmad Sharabiani&quot; &amp; book_title == &quot;Fahrenheit 451&quot;) %&gt;% pull(comment) %&gt;% str_trunc(700) ## [1] &quot;Fahrenheit 451, Ray BradburyFahrenheit 451 is a dystopian novel by American writer Ray Bradbury, published in 1953. Fahrenheit 451 is set in an unspecified city at an unspecified time in the future after the year 1960.Guy Montag is a \\&quot;fireman\\&quot; employed to burn houses containing outlawed books. He is married but has no children. One fall night while returning from work, he meets his new neighbor, a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and he calls for medical attention. ...&lt;U+062A&gt;&lt;U+0627&gt;&lt;U+0631&gt;&lt;U+06CC&gt;&lt;U+062E&gt; &lt;U+0646&gt;&lt;U+062E&gt;&lt;U+0633&gt;&lt;U+062A&gt;&lt;U+06CC&gt;&lt;U+0646&gt; &lt;U+062E&gt;&lt;U+0648&gt;&lt;U+0627&gt;&lt;U+0646&gt;&lt;U+0634&gt;: &lt;U+0631&gt;&lt;U+0648&gt;...&quot; Google Translate tells me the language is Persian, and the translated text includes a brief note–“Date of first reading: The third day of February 1984”–and then another summary of the book written in Persian. The text does not seem to have any actual review or opinion in it. I’m not sure what’s going on here, but we have learned that: * Some users post a large number of reviews; * Some users post useless/non-review reviews, e.g. copy/pasting text from Wikipedia; and, * At least one super-poster posts such reviews. This bears looking into more, since reviews that are copy/pasted from Wikipedia are unlikely to have any predictive value at all and may need to be identified and filtered out in pre-processing. These users may even be bots, especially given the short timeframe for the Goodreads dataset (see below). 4.3 Yelp 4.3.1 Star Ratings Repeating the process for Yelp, this histogram shows the distribution of star ratings. Reviews are again very positive and show a similar distribution. reviews_yelp %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;Yelp Ratings by Star&quot;, x=&quot;Star Rating&quot;, y=NULL) The Yelp data didn’t include restaurant type, so we can’t do a genre-specific investigation as we did for Goodreads. However, we can repeat the analysis where we look at star distributions for the top 6 businesses. Overall the distributions look the same, but here, finally, we get the first hint of bimodality in our distributions. Two restaurants, Sansotei Ramen and Shawarma Palace, have slight second peaks at 1 star. However, the overall story is the same and this could arguably be random fluctuations. top_6_restos &lt;- reviews_yelp %&gt;% group_by(business) %&gt;% summarise(n = n()) %&gt;% slice_max(n=6, order_by=n, with_ties=FALSE) %&gt;% pull(business) ## `summarise()` ungrouping output (override with `.groups` argument) reviews_yelp %&gt;% filter(business %in% top_6_restos) %&gt;% ggplot(aes(x = rating_num)) + geom_histogram( binwidth=1, boundary=0.5, bins=5) + facet_wrap(facets = vars(business)) + theme_grey() + labs(title = &quot;Star Ratings for 6 of the Most-Reviewed Restaurants&quot;, x = &quot;Star Rating&quot;, y = &quot;# of Ratings&quot;) 4.3.2 Word Count As with the Goodreads data, most Yelp reviews are very short. wordcounts_yelp &lt;- reviews_yelp %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_yelp %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;Yelp Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) And again, let’s review the five shortest Yelp reviews in the table below. They seem to be genuine good-faith reviews that include helpful words, and so may be workable for our models. wordcounts_yelp %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_yelp, .) %&gt;% select(business,rating_num,comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Business Stars Review Kallisto Greek Restaurant 4 Great takeout, service, ambiance and … Bite Burger House 4 Delicious, juicy, interesting burgers… BeaverTails 4 BeaverTails pastry..no words needed….. Saigon Boy Noodle House 3 Very decent pho shop, well priced. Supreme Kabob House 5 Excellent Afghani Food and Good Space # %&gt;% # kableExtra::column_spec(column = 1:3, # width = c(&quot;5cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling()%&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.3.3 Reviewers The following histogram shows how many reviews were posted be users. Its distribution is similar to the one we found for Goodreads: most users posted only a few times, but some posted over 50. reviewers_yelp &lt;- reviews_yelp %&gt;% group_by(name) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_yelp %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;Yelp: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looking at our top-10 Yelp reviewers, the drop-off is quite a bit sharper than it was for Goodreads. reviewers_yelp %&gt;% top_n(10, wt = n) %&gt;% knitr::kable(col.names = c(&quot;Name&quot;, &quot;# Reviews&quot;), align = c(&quot;l&quot;,&quot;c&quot;)) Name # Reviews Jennifer P. 78 Amelia J. 77 Dawn M. 51 Samantha M. 44 Eric B. 41 Amanda B. 35 Coy W. 34 Drew K. 27 Spike D. 25 Amy B. 23 # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) The following table shows the first 10 reviews by our top reviewer, Jennifer P., in chronological order. reviews_yelp %&gt;% filter(name == &quot;Jennifer P.&quot;) %&gt;% select(date, business, rating_num, comment) %&gt;% mutate(date = lubridate::mdy(date), comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% slice_head(n=10) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Business Stars Review 2012-06-22 Mr B’s-March House Restaurant 4 I never would have tried this restaurant had it not been recommende… 2012-07-04 Alirang Restaurant 3 I was here last week with my husband, my brother and his girlfriend… 2013-01-24 Corazón De Maíz 4 I can&amp;amp;#39;t believe that I walk by this place all the time, but… 2013-06-24 222 Lyon Tapas Bar 5 This place is absolutely delicious, but man is it ever expensive! … 2013-09-02 Benny’s Bistro 5 I was visiting from out of town for my best friend&amp;amp;#39;s weddin… 2013-09-20 Art Is In Bakery 4 My husband and I were here for their Sunday Brunch recently with an… 2013-11-22 Gezellig 3 Sorry, I&amp;amp;#39;m going to have to downgrade this place to 3 stars… 2013-12-30 Thai Coconut 4 I went here with my husband today for the lunch buffet. It was gre… 2014-05-07 Bite Burger House 4 I had an early dinner here with my husband recently. Bite Burger H… 2014-07-07 Pookies Thai 4 I went here for dinner recently with my husband on a whim. We&amp;amp;… # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) These all seem to be good-faith restaurant reviews. And since this user has been active since 2012, to write 78 reviews they would have to write fewer than one per month. From this brief glance, we have no reason to think that Yelp users are posting insincere reviews. However, I note that the reviews have some html junk in them: &amp;amp;#39; instead of an apostrophe, for example. These will need to be cleaned up before we use the data. 4.4 Mountain Equipment Co-op (MEC) 4.4.1 Star Ratings This histogram shows the distribution of star ratings for MEC reviews. It’s broadly similar to the Yelp and Goodreads reviews, except there is a small second peak at 1 star. reviews_mec %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count, Overall&quot;, x=&quot;Star Rating&quot;, y=NULL) If we break out the reviews by category, we can see that they all follow the same kind of exponential distribution except bicycle components. reviews_mec %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count by Product Category&quot;, x=&quot;Star Rating&quot;, y=NULL) + facet_wrap(~product_type) We can break the bicycle compoenents category down further by individual product. The facet wrap is messy, but we can clearly see that there are a few produts with anomalous spikes in 1-star ratings, and that ecah of these products has the word “tube” in the title. reviews_mec %&gt;% filter(product_type==&quot;bike-components&quot;) %&gt;% ggplot() + geom_bar(aes(x=rating_num)) + theme_minimal() + labs(title = &quot;MEC Ratings: Rating Count by Product&quot;, subtitle = &quot;Bicycle Components&quot;, x=&quot;Star Rating&quot;, y=NULL) + facet_wrap(~product_name) We can conclude that MEC’s reviews follow the same pattern as Yelp and Goodreads overall, except for bicycle inner tubes which have unusually high numbers of 1-star reviews. We should keep this in mind when modeling using the MEC data. 4.4.2 Word Counts Most MEC reviews are very short. They look to be shortest of all three datasets, both in terms of the shape of the dsitribution and the maximum review lengths. We will see this below in a later section when we plot all three distributions at once. wordcounts_mec &lt;- reviews_mec %&gt;% select(comment) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% mutate(id = 1, cumdist = cumsum(id)) wordcounts_mec %&gt;% ggplot() + geom_point(aes(y=cumdist, x=n)) + theme_minimal() + labs(title =&quot;MEC Reviews: Cumulative Distribution of Word-Lengths&quot;, x = &quot;Word Length&quot;, y = &quot;# of Reviews&quot;) If we look at the five shortest reviews, they all seem to be short but legitimate so we can be comfortable with our data quality. wordcounts_mec %&gt;% arrange(n) %&gt;% head(5) %&gt;% pull(rowid) %&gt;% slice(reviews_mec, .) %&gt;% select(product_name,rating_num,comment) %&gt;% mutate(across(where(is.character), str_trunc, width=40)) %&gt;% knitr::kable(booktabs = T, col.names = c(&quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Business Stars Review MEC 700 x 23-25C Tube (48mm Presta Va… 2 Lasted 1 season basically disposable Smartwool Liner Gloves - Unisex 5 Love smartwool products, the gloves a… Scarpa Moraine Mid Gore-Tex Light Tra… 5 light, comfortable and good looking! … Scarpa Moraine Mid Gore-Tex Light Tra… 4 good pair of shoes. lightweight but … La Sportiva TC Pro Rock Shoes - Unisex 5 Flat stiff shoe. Perfect for vertical… # %&gt;% # kableExtra::column_spec(column = 1:3, # width = c(&quot;5cm&quot;,&quot;3cm&quot;,&quot;10cm&quot;)) %&gt;% # kableExtra::kable_styling()%&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) 4.4.3 Reviewers As with the other datasets, it first appears that most users leave only a few reviews but there are some “super-users” who leave quite a few. reviewers_mec &lt;- reviews_mec %&gt;% group_by(user_name) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## `summarise()` ungrouping output (override with `.groups` argument) reviewers_mec %&gt;% ggplot(aes(x=n)) + geom_histogram() + theme_minimal() + labs(title = &quot;MEC: Distribution of Reviews per User&quot;, x = &quot;# of Reviews&quot;, y = &quot;# of Users&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Upon closer inspection, however, we see that our largest “user” is NA, suggesting that most users leave a smallish number of reviews but that some leave reviews anonymously. reviewers_mec %&gt;% top_n(10, wt = n) %&gt;% knitr::kable(col.names = c(&quot;Name&quot;, &quot;# Reviews&quot;), align = c(&quot;l&quot;,&quot;c&quot;))%&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) Name # Reviews NA 76 Matt 9 Chris 8 Mike 8 Ryan 7 John 6 Mark 6 VicCyclist40 6 Dave 5 Paul 5 Steph 5 The following table shows all 9 reviews by our top reviewer, Matt, in chronological order. reviews_mec %&gt;% filter(user_name == &quot;Matt&quot;) %&gt;% select(date, product_name, rating_num, comment) %&gt;% mutate(comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Product&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Product Stars Review 2016-02-29 04:59:40 Black Diamond Mercury Mitts - Men’s 3 I bought these to replace the BD mercury mitts I purchased 6 years … 2017-08-23 20:16:05 MEC Mallard -5C Down Sleeping Bag - Unisex 5 I usually use a bag with a hood but found my self feeling confined … 2017-11-14 03:28:48 Oboz Bridger Mid Bdry Hiking Shoes - Men’s 5 Bought these boots two years ago. Hiked up Sulphur Skyline in Jaspe… 2018-01-20 21:55:36 MEC Goto Fleece Gloves - Unisex 5 These are casual use gloves for me and I wear them around town when… 2018-02-01 11:13:17 Black Diamond Guide Gloves - Men’s 2 Very warm, but not very durable. Considering the cost, these gloves… 2018-07-24 20:03:37 MEC Creekside 0C Sleeping Bag - Unisex 1 I’ve used this bag twice and froze both times at temperatures betwe… 2019-05-15 04:00:46 Scarpa Kailash Trek Gore-Tex Hiking Boots - Men’s 5 While i have so far only logged one day of hiking in my new Scarpas… 2019-06-11 00:19:21 La Sportiva Finale Rock Shoes - Men’s 5 I’m relatively new to the sport and decided to go with these as my … 2019-08-25 17:47:30 MEC Reactor 10 Double Sleeping Pad - Unisex 5 I recently bought this mattress for car camping and it is incredibl… # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) This looks like a legit usage pattern with real reviews. However, we should also spot-check some reviews assigned to NA: reviews_mec %&gt;% filter(is.na(user_name)) %&gt;% select(date, product_name, rating_num, comment) %&gt;% slice_head(n=10) %&gt;% mutate(comment = str_trunc(comment, 70)) %&gt;% arrange(date) %&gt;% knitr::kable(booktabs = TRUE, col.names = c(&quot;Date&quot;, &quot;Business&quot;, &quot;Stars&quot;, &quot;Review&quot;), align = c(&quot;l&quot;,&quot;l&quot;,&quot;c&quot;,&quot;l&quot;)) Date Business Stars Review 2012-08-05 02:01:07 SRAM PC-971 9 Speed Chain 3 I used this chain on both my road and mountain bikes. It’s done fi… 2013-09-23 01:57:46 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 Don’t waste your time with these. I’ve been through 4 this season a… 2014-05-12 17:00:57 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 This is the third of these tubes I have had split down the seam. Th… 2014-06-25 03:13:43 MEC 700 x 23-25C Tube (60mm Presta Valve) 1 I’m afraid that I have to add my voice to the chorus of negative re… 2014-10-04 02:21:33 MEC 700 x 23-25C Tube (48mm Presta Valve) 4 I’m not sure where this chorus of negative reviews is coming from. … 2014-10-25 21:28:29 MEC 700X32-35C (27\"x1 1/4) Tube Schrader Valve 1 I’ve bought two of these tubes and had to return both of them, I wi… 2015-08-06 21:25:01 MEC 700X32-35C (27\"x1 1/4) Tube Schrader Valve 3 Have used three of these over a couple of years and the only one th… 2015-11-16 19:45:33 MEC 700 x 23-25C Tube (48mm Presta Valve) 1 I ride ten kilometres to work and ten kilometres home from work eve… 2016-02-27 13:47:26 SRAM PC-971 9 Speed Chain 4 I have been using PC971 chains for many years. Currently I do most … 2016-06-22 21:38:52 SRAM PC-971 9 Speed Chain 4 The chain that I have purchased from MEC is really a good chain. I … # # %&gt;% # kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;) These also look like legitimate reviews, so it’s possible that these were legitimately left anonymously or that there was a data-parsing issue with the API. 4.5 Comparing Goodreads, MEC, and Yelp 4.5.1 Star Ratings When we compare Yelp and Goodreads reviews by the number of star ratings, the distributions look very similar. There are fewer Yelp reviews, but the shape of the distribution looks like a scaled-down version of the Goodreads distribution. There are far fewer MEC reviews, and it looks like the distribution has a slight second peak at 1 star. gr &lt;- reviews_gr %&gt;% group_by(rating_num) %&gt;% summarise(gr = n()) ## `summarise()` ungrouping output (override with `.groups` argument) yp &lt;- reviews_yelp %&gt;% group_by(rating_num) %&gt;% summarise(yp = n()) ## `summarise()` ungrouping output (override with `.groups` argument) mc &lt;- reviews_mec %&gt;% group_by(rating_num) %&gt;% summarise(mc = n()) ## `summarise()` ungrouping output (override with `.groups` argument) compare &lt;- left_join(gr, yp) %&gt;% left_join(mc) ## Joining, by = &quot;rating_num&quot; ## Joining, by = &quot;rating_num&quot; compare_long &lt;- compare %&gt;% pivot_longer(cols = c(&quot;gr&quot;, &quot;yp&quot;,&quot;mc&quot;), names_to = &quot;source&quot;, values_to = &quot;num&quot;) compare_long %&gt;% ggplot() + geom_col(aes(x=rating_num, y=num, group=source, fill=source), position = &quot;dodge&quot;) + theme_minimal() + labs(title = &quot;Goodreads, MEC, and Yelp Reviews: Total Counts by Rating&quot;, x = &quot;Star Rating&quot;, y = &quot;n&quot;, fill = &quot;Source&quot;) + scale_fill_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) To get a better feel for how the distributions vary, we can plot the proportional breakdown of star reviews for each source. The following plot shows that the Goodreads and Yelp distributions track each other somewhat closely but the MEC reviews are quite different. compare_long %&gt;% group_by(source) %&gt;% mutate(prop = num / sum(num)) %&gt;% ggplot() + geom_col(aes(x=rating_num, y=prop, group=source, fill=source), position = &quot;dodge&quot;) + theme_minimal() + labs(title = &quot;Goodreads, MEC, and Yelp Reviews: Proportion of Counts by Rating&quot;, x = &quot;Star Rating&quot;, y = &quot;Proportion&quot;, fill = &quot;Source&quot;) + scale_fill_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) We can use a standard Pearson’s Chi-squared test to see if the Goodreads and Yelp distributions differ meaningfully. t &lt;- chisq.test(compare$gr, compare$yp) ## Warning in chisq.test(compare$gr, compare$yp): Chi-squared approximation may be incorrect tt &lt;- chisq.test(matrix(c(compare$gr, compare$yp), ncol=5)) tt ## ## Pearson&#39;s Chi-squared test ## ## data: matrix(c(compare$gr, compare$yp), ncol = 5) ## X-squared = 8357.3, df = 4, p-value &lt; 2.2e-16 We find that yes, we can reject the null hypothesis that there is no difference between the two distributions with a large amount of confidence. However, the two review distributions are still qualitatively similar, it’s not clear that the difference between them is large or meaningful–we could look into that later. 4.5.2 Word Counts Out of interest, let’s also check the differences in word-count distributions between the three datasets. From the figure below, we can see that Yelp reviews tend to be much shorter than Goodreads reviews. Just by visual inspection, we can estimate that the 80th percentile Goodreads review is about 500 words, whereas the 80th percentile Yelp review is only about half of that. The MEC reviews are shortest of all. wordcounts_all &lt;- wordcounts_gr %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;goodreads&quot;) %&gt;% bind_rows( wordcounts_yelp %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;yelp&quot;)) %&gt;% bind_rows( wordcounts_mec %&gt;% select(n, cumdist) %&gt;% mutate(source = &quot;mec&quot;)) wordcounts_all %&gt;% group_by(source) %&gt;% mutate (prop = cumdist / max(cumdist)) %&gt;% ggplot() + geom_point(aes(y=prop, x=n, colour = source)) + labs(title = &quot;Cumulative Distribution of Word Lengths&quot;, subtitle = &quot;Comparing Goodreads, MEC, and Yelp&quot;, x = &quot;Word Length&quot;, y = &quot;Cumulative Probability&quot;, colour = &quot;Source&quot;) + scale_color_viridis_d(labels = c(&quot;Goodreads&quot;, &quot;MEC&quot;, &quot;Yelp&quot;)) + theme_minimal() To test for difference, we can confirm do a non-parametric Kolmogorov-Smirnov test to see if the Goodreads and Yelp distributions differ. # pull the word lengths for goodreads into a vector grd &lt;- wordcounts_all %&gt;% filter(source == &quot;goodreads&quot;) %&gt;% pull(n) # pull the word lengths for yelp into a vector ypd &lt;- wordcounts_all %&gt;% filter(source == &quot;yelp&quot;) %&gt;% pull(n) # run KS test comparing the two vectors ks.test(grd, ypd) ## Warning in ks.test(grd, ypd): p-value will be approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: grd and ypd ## D = 0.24968, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided # remove the vectors to keep environment clean rm(grd, ypd) We can again reject the null hypothesis that there is no difference between the two distributions. We can hypothesize about why there might be a difference: Goodreads reviewers are writing about books, and so might be expected to be interested in expressing themselves through writing. Yelp reviewers, by and large, are interested in restaurants, and so may not put as much effort into writing full reports. We might expect the difference in distributions to have an effect on our future modeling, since shorter reviews may contain less information. 4.6 Reviews Over Time This section looks at how our review datasets change over time, to see how recent reviews are and if there are any trends in volume. 4.6.1 Goodreads The following chart shows the monthly volume of reviews in the Goodreads dataset. reviews_gr %&gt;% mutate(dates = lubridate::ymd(dates) %&gt;% lubridate::floor_date(&quot;months&quot;)) %&gt;% group_by(dates) %&gt;% summarise(n = n()) %&gt;% ggplot(aes(x=dates,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;Goodreads Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) While some reviews date from as far back as 2005, most reviews are from 2020 and the majority are from the past few months. However, it’s unlikely that this distribution represents an actual exponential growth in the number of reviews posted. Instead, recall that I collected reviews for the 100 most-read books in the past week across a few genres. In other words, I collected reviews from books that were being reviewed a lot at that moment in time, so my data collection is heavily biased towards more recent reviews. There may a trend in usage–for example, home-bound readers may be posting more reviews during COVID-19–but we can’t draw any conclusions from this distribution. 4.6.2 Yelp The following chart shows the monthly volume of reviews in the Yelp dataset. reviews_yelp %&gt;% mutate(date = lubridate::mdy(date) %&gt;% lubridate::floor_date(&quot;months&quot;)) %&gt;% group_by(date) %&gt;% summarise(n = n()) %&gt;% ggplot(aes(x=date,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;Yelp Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) Since I collected all Yelp reviews for restaurants in Ottawa, we can use this dataset to make statements about how review volumes have changed over time. We can see a steep decline in the early months of 2020, coinciding with the start of the COVID-19 pandemic and worldwide lockdowns. However, the volumes also tell an interesting story pre-COVID. From 2010 to 2015 we can see what looks like slow but steady growth, and then after 2015 usage increases dramatically. From 2015-2020 we can see what look like seasonal trends, but it looks like overall volumes stopped growing and may have started declining. In other words, Yelp may have been in trouble before the pandemic hit. For our purposes, we can be satisfied that our restaurant review dataset spans a long period of time both pre- and post-COVID. 4.6.3 MEC The following chart shows the monthly volume of reviews in the MEC dataset for each complete month. The data was collected in the first few days of November, so I have left November out. reviews_mec %&gt;% mutate(date = lubridate::floor_date(date, &quot;months&quot;)) %&gt;% group_by(date) %&gt;% summarise(n = n()) %&gt;% slice_head(n = nrow(.)-1) %&gt;% ggplot(aes(x=date,y=n)) + geom_line() + theme_minimal() + labs(title = &quot;MEC Reviews: Monthly Volume of New Reviews&quot;, x = &quot;Date&quot;, y = &quot;# of Reviews&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) We can expect several biases in the MEC data, so we will need to be cautious about making inferences from this time series. First, I collected MEC data from only a few product categories which may have seasonal trends (e.g. biking in the summer, snowshoeing in the winter). Second, MEC only lists products on its website if they’re currently for sale, so the maximum review age is limited by the longevity of MEC’s product lines. So we should expect to see a decay in review volume as we go further back in time caused by MEC naturally rotating its product line. That said, we can still see a big dip in early 2020 and then a big spike in summer 2020. This could correspond to a big drop in sales with the COVID lockdown and associated uncertainty, and then a bike spike in outdoor sporting goods as people tried to find socially distanced ways of entertaining themselves over the summer. Out of curiosity, here are the 10 oldest reviews in our dataset: reviews_mec %&gt;% arrange(date) %&gt;% slice_head(n=10) %&gt;% select(date, product_name, review_title) ## # A tibble: 10 x 3 ## date product_name review_title ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2007-05-22 00:00:00 MEC V-Brake Cartridge Brake Pads I need more!!! ## 2 2007-08-12 00:00:00 MEC V-Brake Shoe/Pad Assembly Buy them once, love them forever ## 3 2007-09-05 00:00:00 SRAM PC-971 9 Speed Chain Great value ## 4 2008-02-28 00:00:00 MEC V-Brake Cartridge Brake Pads Decent brake pads ## 5 2008-11-02 00:00:00 MEC V-Brake Shoe/Pad Assembly Great product. ## 6 2008-12-16 00:00:00 MEC V-Brake Shoe/Pad Assembly Best value in a V-brake pad! ## 7 2008-12-18 00:00:00 Zamberlan Vioz GT Gore-Tex Backpacking Boots - Women&#39;s Mènent au sommet! ## 8 2009-01-15 00:00:00 SRAM PC-971 9 Speed Chain Decent deal on a higher end chain. ## 9 2009-07-21 18:37:26 SRAM PC-830 8 Speed Chain short life ## 10 2009-08-17 19:44:51 SRAM PC-830 8 Speed Chain Not impressed! Not surprisingly, 9 out of 10 are for standard bicycle components that are more about function than fashion: it seems that MEC and SRAM have been offering the same brake pads and chains for more than 10 years. And we can take a look at the first review for the Zamberlan boots: reviews_mec %&gt;% filter(product_name==&quot;Zamberlan Vioz GT Gore-Tex Backpacking Boots - Women&#39;s&quot;) %&gt;% slice_head(n=1) %&gt;% transmute(date = date, comment = str_trunc(comment, 150)) ## # A tibble: 1 x 2 ## date comment ## &lt;dttm&gt; &lt;chr&gt; ## 1 2015-05-26 20:40:15 I&#39;ve been using these Zamberlan Viozes for the past 4 years. I&#39;ve owned 4 pairs in that time and I&#39;m just about to start my 5th. I buy a pair every... These boots seem to have been around for a while (and certainly seem to have committed fans), so we can be confident that these reviews are legit. 4.7 Proposed Next Steps Sentiment analysis Regression models LASSO regression to predict star rating from review text. Potential to use minimum review length as a parameter. Linear regression to predict star rating from review sentiment. Classification models 4.8 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] glmnet_4.0-2 Matrix_1.2-18 discrim_0.1.1 vip_0.2.2 tictoc_1.0 textrecipes_0.3.0 lubridate_1.7.9 yardstick_0.0.7 workflows_0.2.0 tune_0.1.1 ## [11] rsample_0.0.8 recipes_0.1.13 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 tidytext_0.2.5 ## [21] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 SnowballC_0.7.0 ## [10] prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 pROC_1.16.2 packrat_0.5.0 ## [19] servr_0.18 dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 later_1.1.0.1 htmltools_0.5.0 ## [28] tools_4.0.2 gtable_0.3.0 glue_1.4.1 naivebayes_0.9.7 rappdirs_0.3.1 Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 ## [37] iterators_1.0.12 timeDate_3043.102 gower_0.2.2 xfun_0.16 stopwords_2.0 globals_0.13.0 rvest_0.3.6 mime_0.9 lifecycle_0.2.0 ## [46] future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 promises_1.1.1 hms_0.5.3 parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 ## [55] stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 lava_1.6.8 ## [64] rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 labeling_0.3 tidyselect_1.1.0 plyr_1.8.6 magrittr_1.5 bookdown_0.20 ## [73] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.6 haven_2.3.1 withr_2.2.0 survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 ## [82] modelr_0.1.8 crayon_1.3.4 utf8_1.1.4 rmarkdown_2.3 usethis_1.6.1 grid_4.0.2 readxl_1.3.1 blob_1.2.1 reprex_0.3.0 ## [91] digest_0.6.25 webshot_0.5.2 httpuv_1.5.4 munsell_0.5.0 GPfit_1.0-8 viridisLite_0.3.0 kableExtra_1.1.0 "],
["a-first-lasso-attempt.html", "Chapter 5 A First LASSO Attempt 5.1 Introduction 5.2 A First Regression: Yelp Data 5.3 Trying lasso again 5.4 Removing stop words 5.5 Adjusting n-grams 5.6 Full final regression 5.7 Conclusion", " Chapter 5 A First LASSO Attempt 5.1 Introduction This analysis will use regression methods to attemp to predict star ratings from the text and/or titles of the reviews in our Yelp, Goodreads, and MEC datasets. My methods will closely follow those given in Chapter 6 of Supervised Machine Learning for Text Analysis in R (SMLTAR) by Silge and Hvitfeldt (2020). In the first case I will work through an example in detail to describe the steps (and to learn them!!), and in later sections I will move more quickly to try some different variations on the analysis. I’m going to use the tidymodels framework as much as possible, both because it’s the approach used in SMLTAR and because I’m a fan of the Tidyverse approach to software design and analysis. 5.2 A First Regression: Yelp Data I will begin with the Yelp data because we have a lot of it, and because based on our EDA it seemed to be “cleaner” than the Goodreads data which had a lot of duplicate posts, spam posts, plot summaries, etc. reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% mutate(date = lubridate::mdy(date)) %&gt;% rename(text = comment, rating_num = rating) reviews_yelp %&gt;% head(10) %&gt;% mutate(text = stringr::str_trunc(text, 100)) %&gt;% knitr::kable() business name date text rating_num url La Squadra Alain G. 2017-08-21 Confession: I am a foodie and I am a restaurant trained amateur Chef.&lt;br&amp;gt;Been wanting to try t… 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Amelia J. 2018-12-19 I came here for a Christmas lunch with coworkers and we tried the set Christmas menu (appetizer +… 4 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Michael C. 2017-07-03 Beautiful venue, great service and incredible food. Try Squadra pasta and pizza…and the Aranci… 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Cee Y. 2019-03-30 My husband and I stopped into this place as per a recommendation by Yelp. We had a fantastic time… 5 http://www.yelp.ca/biz/la-squadra-gatineau La Squadra Luc S. 2018-03-04 Best Italian restaurant in Gatineau ! The food is authentic and fresh with good wine recommendati… 5 http://www.yelp.ca/biz/la-squadra-gatineau Kallisto Greek Restaurant Amster S. 2020-04-18 I&amp;amp;#39;ve been here twice. Once with my work friends and second with my family. I will come ba… 5 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Reema D. 2020-01-12 Waitress was pretty slow. Didn&amp;amp;#39;t take our dinner orders until after we finished apps and … 4 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Jennifer P. 2018-05-21 My husband and I had dinner here recently and overall it was very good 3.75 stars, rounded up to … 4 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Teena D. 2018-02-07 I had lunch today at Kallisto Greek Restaurant.&lt;br&amp;gt;&lt;br&amp;gt;I love chicken souvlaki and that&amp;amp… 2 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa Kallisto Greek Restaurant Janie M. 2019-06-26 Find there&amp;amp;#39;s always warm and friendly service. Best Greek food in Ottawa! My son, daught… 5 http://www.yelp.ca/biz/kallisto-greek-restaurant-ottawa 5.2.1 Splitting the data First we will split our data into a training set and a testing set. This is a standard practice, wherein we build a model using the training data but set aside some other data so we can test it later. Otherwise we might have concerns about overfitting or model validity. I’m setting the value strata = \"rating_num\" to ensure that our random sampling has about the same distribution of star ratings as our full population–see the documentation for initial_split(). set.seed(1234) yelp_split &lt;- reviews_yelp %&gt;% initial_split(strata = &quot;rating_num&quot;) yelp_train &lt;- yelp_split %&gt;% training() yelp_test &lt;- yelp_split %&gt;% testing() The next step is to define our preprocessing steps: the stuff we’ll do to the text before we put it into a regression model. In the tidymodels approach we do this by creating a “recipe” objects and then adding a number of steps to it. We modify the object by using the pipe operator to add a bunch of steps to it using verb functions. This makes it easy to read the step-by-step process and understand what’s going on. I’ll note, though, that when I follow SMLTAR’s guide the recipe still includes explicit references to the dataset we’re analyzing, so it’s not a completely generic object that could be applied to other datasets: we would need to make other recipes for MEC and Goodreads. There may be more advanced ways to create generic recipes that can be reused. Here, following SMLTAR, we will use a recipe with the following steps: Tokenizing the text, which means breaking it down into constituent bits (words here), Filtering the tokens based on frequency, taking only the 250 most-common tokens, (NOTE this is not many tokens!!) TFIDF, or “term frequency inverse document frequency,” which weights each token based on both how frequent it is and on how common it is across documents (see step_tfidf()’s help page for details), and then Normalizing so our lasso regression will work properly. num_tokens &lt;- 250 yelp_rec &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) yelp_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Tokenization for text ## Text filtering for text ## Term frequency-inverse document frequency with text ## Centering and scaling for all_predictors() Next, Silge and Hvitfeldt (2020) suggest we create a workflow() object that combines preprocessing steps and models. yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) yelp_wf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: None ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_tokenfilter() ## * step_tfidf() ## * step_normalize() We now define a lasso regression model using parsnip. My understanding is that this acts as a “tidy wrapper” around other functions/packages, in this case glmnet, that lets you use them in a tidy way. I believe it can also make it easier to swap out models or parameters without having to completely rewrite your codebase. Note that penalty = 0.1 is arbitrary and we’ll look into that parameter more closely later. lasso_model &lt;- parsnip::linear_reg(penalty = 0.1, mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) lasso_model ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet Now we add the lasso model to the workflow and run the model. This takes about 9 seconds on my machine using only 250 tokens. (I expect we’ll need to use more to get a good result.) tic() lasso_fit &lt;- yelp_wf %&gt;% add_model(lasso_model) %&gt;% fit(data = yelp_train) toc() ## 8.82 sec elapsed We can look at the terms with the highest coefficients in the model: lasso_fit %&gt;% pull_workflow_fit() %&gt;% tidy() %&gt;% arrange(-estimate) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Loaded glmnet 4.0-2 ## # A tibble: 251 x 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.16 0.1 ## 2 tfidf_text_amazing 0.0490 0.1 ## 3 tfidf_text_delicious 0.0395 0.1 ## 4 tfidf_text_great 0.0379 0.1 ## 5 tfidf_text_best 0.0307 0.1 ## 6 tfidf_text_and 0.00624 0.1 ## 7 tfidf_text_2 0 0.1 ## 8 tfidf_text_3 0 0.1 ## 9 tfidf_text_34 0 0.1 ## 10 tfidf_text_39 0 0.1 ## # ... with 241 more rows This already doesn’t look too promising; only 5 terms have positive coefficients, and the intercept is 4.16. But let’s see how it goes. 5.2.2 Evaluating the first model Following Silge and Hvitfeldt (2020), we’ll evaluate the model using cross-fold validation, which is a way of trying to squeeze as much validation as you can out of a finite dataset. We will resample our training dataset to create 10 new datasets, and in each one we’ll use 90% for training and 10% for assessment. set.seed(1234) yelp_folds &lt;- vfold_cv(yelp_train) lasso_rs &lt;- fit_resamples( yelp_wf %&gt;% add_model(lasso_model), yelp_folds, control = control_resamples(save_pred = TRUE) ) tic() lasso_rs ## # Resampling results ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [6.3K/706]&gt; Fold01 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 3]&gt; ## 2 &lt;split [6.3K/706]&gt; Fold02 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 3]&gt; ## 3 &lt;split [6.3K/706]&gt; Fold03 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [706 x 3]&gt; ## 4 &lt;split [6.3K/705]&gt; Fold04 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 5 &lt;split [6.3K/705]&gt; Fold05 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 6 &lt;split [6.3K/705]&gt; Fold06 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 7 &lt;split [6.3K/705]&gt; Fold07 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 8 &lt;split [6.3K/705]&gt; Fold08 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 9 &lt;split [6.3K/705]&gt; Fold09 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; ## 10 &lt;split [6.3K/705]&gt; Fold10 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [705 x 3]&gt; toc() ## 0.02 sec elapsed Our \\(R^2\\) and RMSEs look really quite terrible: lasso_rs %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 rmse standard 0.985 10 0.00732 ## 2 rsq standard 0.147 10 0.0122 And when we plot predictions vs. true values, that also looks quite terrible: lasso_rs %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred, color = id)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Each cross-validation fold is shown in a different color&quot; ) The model generally predicts that everything will have a star rating of between 3 and 5, and is especially poor at predicting lower values. We’re now operating without much of a map, since the example in Silge and Hvitfeldt (2020) worked beautifully (predicting the year a USA Supreme Court decision was written based on its text). However, we can follow one of their last steps by tuning our lasso hyperparameters. 5.2.3 Tuning model parameters We can repeat the process but use model tuning to set the paramters in our lasso regression. Now instead of choosing a random lasso penalty of 0.1, we’re going to use the tune() function to figure out which penalty gives the best results on our training data. tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) tune_model ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet We create a grid of values to try: lambda_grid &lt;- grid_regular(penalty(), levels = 30) And now we use the function tune_grid() to fit our model at many different parameter values to see how they fare on our cross-fold validation set. Note: this takes a long time, 81.5 seconds for the 250-token model on my machine. set.seed(1234) tic() tune_rs &lt;- tune_grid( yelp_wf %&gt;% add_model(tune_model), yelp_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) ## ! Fold01: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold02: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold03: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold04: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold05: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold06: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold07: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold08: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold09: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold10: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. toc() ## 80.04 sec elapsed tune_rs ## Warning: This tuning result has notes. Example notes on model fitting include: ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [6.3K/706]&gt; Fold01 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 2 &lt;split [6.3K/706]&gt; Fold02 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 3 &lt;split [6.3K/706]&gt; Fold03 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,180 x 5]&gt; ## 4 &lt;split [6.3K/705]&gt; Fold04 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 5 &lt;split [6.3K/705]&gt; Fold05 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 6 &lt;split [6.3K/705]&gt; Fold06 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 7 &lt;split [6.3K/705]&gt; Fold07 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 8 &lt;split [6.3K/705]&gt; Fold08 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 9 &lt;split [6.3K/705]&gt; Fold09 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; ## 10 &lt;split [6.3K/705]&gt; Fold10 &lt;tibble [60 x 5]&gt; &lt;tibble [1 x 1]&gt; &lt;tibble [21,150 x 5]&gt; We can visualize our lasso model’s performance for each parameter value: tune_rs %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err ), alpha = 0.5 ) + geom_line(size = 1.5) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10() + theme(legend.position = &quot;none&quot;) + labs( title = &quot;Lasso model performance across regularization penalties&quot;, subtitle = &quot;Performance metrics can be used to identity the best penalty&quot; ) ## Warning: Removed 2 row(s) containing missing values (geom_path). Since we want the best model performance possible, we’ll follow Silge and Hvitfeldt (2020) and choose the value that minimizes our RMSE. tune_rs %&gt;% show_best(&quot;rmse&quot;) ## # A tibble: 5 x 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00853 rmse standard 0.903 10 0.0129 Model24 ## 2 0.00386 rmse standard 0.904 10 0.0133 Model23 ## 3 0.00174 rmse standard 0.905 10 0.0135 Model22 ## 4 0.000788 rmse standard 0.906 10 0.0136 Model21 ## 5 0.000356 rmse standard 0.907 10 0.0136 Model20 And we can extract the penalty that gives us the lowest RMSE using the select_best() function as follows: lowest_rmse &lt;- tune_rs %&gt;% select_best(&quot;rmse&quot;) And we can put it all together into a final workflow: final_lasso &lt;- finalize_workflow( yelp_wf %&gt;% add_model(tune_model), lowest_rmse ) We can then do a final fit by testing our model’s predictions against our testing data using the following command. lasso_fit &lt;- final_lasso %&gt;% last_fit(split = yelp_split) And then we can extract its predictions and plot them against the true values to see how it looks. lasso_fit %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot; ) This model looks better in some ways but worse in others. It’s better in that it gives lower predictions for in-truth lower reviews; it’s worse in that it predicts ratings over 5, and even over 6.5. The spread of predictions is also still quite large, but that may be to be expected with an \\(R^2\\) of only about 0.25. 5.3 Trying lasso again 5.3.1 With 1000 tokens Here is the whole process again in a single code block using 1000 tokens. num_tokens &lt;- 1000 set.seed(1234) # do initial split yelp_split &lt;- reviews_yelp %&gt;% initial_split(strata = &quot;rating_num&quot;) yelp_train &lt;- yelp_split %&gt;% training() yelp_test &lt;- yelp_split %&gt;% testing() # set up recipe yelp_rec &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) # set up our lasso model using tuning parameters tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) # create a grid of tuning parameters lambda_grid &lt;- grid_regular(penalty(), levels = 30) # create cross-validation folds set.seed(1234) yelp_folds &lt;- vfold_cv(yelp_train) # fit our model at many different parameter values using the cross-fold validation set set.seed(1234) tic() tune_rs &lt;- tune_grid( yelp_wf %&gt;% add_model(tune_model), yelp_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) toc() ## 182.2 sec elapsed # extract penalty that gives us the lowest RMSE lowest_rmse &lt;- tune_rs %&gt;% select_best(&quot;rmse&quot;) # put it into a final workflow final_lasso &lt;- finalize_workflow( yelp_wf %&gt;% add_model(tune_model), lowest_rmse ) # do a last fit lasso_fit &lt;- final_lasso %&gt;% last_fit(split = yelp_split) # see the metrics lasso_fit %&gt;% collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.868 ## 2 rsq standard 0.373 # and plot it lasso_fit %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;All reviews, 1000 tokens&quot; ) This has an \\(R^2\\) of 0.37, which is a big improvement over the 250-token model, but it’s still nowhere near good enough to use in practice. 5.3.2 Short reviews only: &lt;125 words Let’s try only using reviews under 125 words. It’s possible that shorter reviews are “denser” and more to the point, and that longer reviews contain too much “noise.” This leaves us with 6,323 reviews. To begin with, I’m going to define a function to run the lasso regression with different inputs. run_lasso &lt;- function(dataset, num_tokens){ set.seed(1234) data_split &lt;- dataset %&gt;% initial_split(strata = &quot;rating_num&quot;) data_train &lt;- data_split %&gt;% training() data_test &lt;- data_split %&gt;% testing() data_rec &lt;- recipe(rating_num ~ text, data = data_train) %&gt;% step_tokenize(text) %&gt;% step_tokenfilter(text, max_tokens = num_tokens) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) rm(num_tokens) data_wf &lt;- workflow() %&gt;% add_recipe(data_rec) # set up our lasso model using tuning parameters tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) # create a grid of tuning parameters lambda_grid &lt;- grid_regular(penalty(), levels = 30) # create cross-validation folds set.seed(1234) data_folds &lt;- vfold_cv(data_train) # fit our model at many different parameter values using the cross-fold validation set set.seed(1234) tic() tune_rs &lt;- tune_grid( data_wf %&gt;% add_model(tune_model), data_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE) ) toc() # extract penalty that gives us the lowest RMSE lowest_rmse &lt;- tune_rs %&gt;% select_best(&quot;rmse&quot;) # put it into a final workflow final_lasso &lt;- finalize_workflow( data_wf %&gt;% add_model(tune_model), lowest_rmse ) # do a last fit lasso_fit &lt;- final_lasso %&gt;% last_fit(split = data_split) return(lasso_fit) } Then we can use this function to easily run lasso regressions on different datasets. max_length &lt;- 125 min_length &lt;- 1 wordcounts_yelp &lt;- reviews_yelp %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% left_join (reviews_yelp %&gt;% rowid_to_column(), by =&quot;rowid&quot;) %&gt;% select(-rowid) reviews_yelp_short &lt;- wordcounts_yelp %&gt;% filter(n &lt;= max_length &amp; n &gt;= min_length ) lasso_results_short &lt;- run_lasso(dataset = reviews_yelp_short, num_tokens = 1000) ## 89.07 sec elapsed # see the metrics lasso_results_short %&gt;% collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.821 ## 2 rsq standard 0.391 # and plot it lasso_results_short %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Short Reviews &lt; 125 Words, 1000 Tokens&quot; ) This gives us an \\(R^2\\) of 0.39, slightly better than our full dataset. But looking at the chart, we can see that this won’t be useful in practice either. 5.3.3 Longer reviews &gt; 125 words For completeness, we’ll also try only using the long reviews &gt; 125 words. It’s possible that these reviews contain more useful information due to their length. This leaves us with 3,104 reviews. max_length &lt;- 10000 min_length &lt;- 125 wordcounts_yelp &lt;- reviews_yelp %&gt;% select(text) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% group_by(rowid) %&gt;% summarise(n = n()) %&gt;% left_join (reviews_yelp %&gt;% rowid_to_column(), by =&quot;rowid&quot;) %&gt;% select(-rowid) reviews_yelp_long &lt;- wordcounts_yelp %&gt;% filter(n &lt;= max_length &amp; n &gt;= min_length ) lasso_results_long &lt;- run_lasso(dataset = reviews_yelp_long, num_tokens = 1000) ## 72.61 sec elapsed # see the metrics lasso_results_long %&gt;% collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.824 ## 2 rsq standard 0.429 # and plot it lasso_results_long %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(slope=1, intercept = 0,color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted Rating&quot;, color = NULL, title = &quot;Final lasso model: Predicted and true star ratings for Yelp reviews&quot;, subtitle = &quot;Long Reviews &gt; 125 Words, 1000 Tokens&quot; ) Now our \\(R^2\\) has gone up to 0.43, so it is possible that the longer reviews do in fact contain more information. And looking at the chart, the “cloud” of points does creep measurably higher for each true star rating. However, I’m still skeptical that this would be useful for predicting anything in practice. 5.4 Removing stop words Stop words are common words that contain little information on their own, like “the” and “to.” If using a bag-of-words approach, where you’re not looking at the input text in a way that considers syntax (or, really, sentence-wise semantics) then it can be helpful to remove stop words. Here I will follow Silge and Hvitfeldt (2020) ’s SMLTAR s6.6 to try using three different sets of stopwords, to see which performs best on this dataset. First, they build a wrapper function to make it easy to build recipes with different stopword sets. stopword_rec &lt;- function(stopword_name) { recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text, stopword_source = stopword_name) %&gt;% step_tokenfilter(text, max_tokens = 1000) %&gt;% step_tfidf(text) } Next we set up a workflow that only has a model, using our tunable regularized regression model from before: tunable_wf &lt;- workflow() %&gt;% add_model(tune_model) tunable_wf ## == Workflow ==================================================================== ## Preprocessor: None ## Model: linear_reg() ## ## -- Model ----------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Now we will combine our functionized preprocessor with this tunable model and try three different stopword sets: snowball, smart, and stopwords-iso. This takes about 8 minutes on my machine. set.seed(1234) tic() snowball_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;snowball&quot;)), yelp_folds, grid = lambda_grid ) toc() ## 176.97 sec elapsed set.seed(1234) tic() smart_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;smart&quot;)), yelp_folds, grid = lambda_grid ) toc() ## 160.64 sec elapsed set.seed(1234) tic() stopwords_iso_rs &lt;- tune_grid( tunable_wf %&gt;% add_recipe(stopword_rec(&quot;stopwords-iso&quot;)), yelp_folds, grid = lambda_grid ) toc() ## 93.12 sec elapsed And we plot their performance, using code straight from SMLTAR: word_counts &lt;- tibble(name = c(&quot;snowball&quot;, &quot;smart&quot;, &quot;stopwords-iso&quot;)) %&gt;% mutate(words = map_int(name, ~ length(stopwords::stopwords(source = .)))) list( snowball = snowball_rs, smart = smart_rs, `stopwords-iso` = stopwords_iso_rs ) %&gt;% map_dfr(show_best, &quot;rmse&quot;, .id = &quot;name&quot;) %&gt;% left_join(word_counts) %&gt;% mutate(name = paste0(name, &quot; (&quot;, words, &quot; words)&quot;)) %&gt;% ggplot(aes(fct_reorder(name, words), mean, color = name)) + geom_point(size = 3, alpha = 0.8, show.legend = FALSE) + labs( x = NULL, y = &quot;mean RMSE for five best models&quot;, title = &quot;Model performance for three stop word lexicons&quot;, subtitle = &quot;For this dataset, the Snowball lexicon performed best&quot; ) ## Joining, by = &quot;name&quot; The RMSE is marginally better using the snowball set of stopwords, but is still quite terrible! 5.5 Adjusting n-grams When tokenizing, we can in general consider text strings of any length. So far we have been considering one-word strings, which we could call “unigrams.” We could also consider two-word strings and three-word strings, called “bigrams” and “trigrams” respectively. We might expect using n-grams, where n&gt;1, to increase our accuracy because it will let us capture more of the syntactic information in our text. For example, if we only consider 1-grams then the short phrase “Not bad!” becomes “not” and “bad,” and our model has no way to differentiate between cases where they occur alone (which might be negative) and together (which might be positive). But if we also consider “not bad,” then the model might learn that that phrase is associated with positive reviews. As before, we follow SMLTAR s6.7 and set up a wrapper function that will let us easily change our model recipe to use different n-grams: ngram_rec &lt;- function(ngram_options) { recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text, token = &quot;ngrams&quot;, options = ngram_options) %&gt;% step_tokenfilter(text, max_tokens = 1e3) %&gt;% step_tfidf(text) } step_tokenize() takes two arguments, n for the highest-n n-grams to consider, and n_min for the lowest-n ngrams to consider. We will pass these values in the variable ngram_options. We then out these all together into a wrapper function that will let us run many different models easily: tune_ngram &lt;- function(ngram_options) { tune_grid( tunable_wf %&gt;% add_recipe(ngram_rec(ngram_options)), yelp_folds, grid = lambda_grid ) } We will try three cases, using n-grams where n=1, n=1,2, and n=1,2,3. I’ve added tic()/toc() calls for loose benchmarking. The processing time goes up with each additional n-gram: 1-grams: 186s 2-grams: 267s 3-grams: 495s set.seed(123) tic() unigram_rs &lt;- tune_ngram(list(n = 1)) ## ! Fold01: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold02: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold03: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold04: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold05: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold06: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold07: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold08: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold09: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold10: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. toc() ## 174.64 sec elapsed tic() set.seed(234) bigram_rs &lt;- tune_ngram(list(n = 2, n_min = 1)) ## ! Fold01: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold02: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold03: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold04: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold05: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold06: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold07: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold08: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold09: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold10: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. toc() ## 254.42 sec elapsed tic() set.seed(345) trigram_rs &lt;- tune_ngram(list(n = 3, n_min = 1)) ## ! Fold01: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold02: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold03: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold04: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold05: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold06: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold07: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold08: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold09: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## ! Fold10: internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. toc() ## 452.24 sec elapsed And we can plot the results using a dot-plot, as per SMLTAR: list( `1` = unigram_rs, `1 and 2` = bigram_rs, `1, 2, and 3` = trigram_rs ) %&gt;% map_dfr(collect_metrics, .id = &quot;name&quot;) %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% ggplot(aes(name, mean, fill = name)) + geom_dotplot( binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, binpositions = &quot;all&quot;, show.legend = FALSE ) + labs( x = &quot;Degree of n-grams&quot;, y = &quot;mean RMSE&quot;, title = &quot;Model performance for different degrees of n-gram tokenization&quot;, subtitle = &quot;For the same number of tokens, unigrams alone performed best&quot; ) Amusingly, the fastest &amp; simplest approach of using only 1-grams worked best. 5.6 Full final regression After working through each piece of the regression preprocessing and recipe, we’ll now followed SMLTAR s6.10’s lead and put it all together. We will: Train on the cross-validation resamples; Tune both the lasso regularization parameter and the number of tokens used in the model; Only include unigrams; Remove the snowball stop words; And evaluate on the testing set. Here is our final recipe. note that we are using tune() as our max_tokens value. This will let us fit the model to a grid of values and see which one performs best. final_rec &lt;- recipe(rating_num ~ text, data = yelp_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text, stopword_source = &quot;snowball&quot;) %&gt;% step_tokenfilter(text, max_tokens = tune()) %&gt;% step_tfidf(text) final_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Tokenization for text ## Stop word removal for text ## Text filtering for text ## Term frequency-inverse document frequency with text Then we specify our model again: tune_model &lt;- linear_reg( penalty = tune(), mixture = 1) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) tune_model ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Then we set up our workflow: tune_wf &lt;- workflow() %&gt;% add_recipe(final_rec) %&gt;% add_model(tune_model) tune_wf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ----------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet Next we’ll tune the model. To do so, we need to choose the set of parameter values for the penalty and number of tokens we’ll test. We do this by setting up a “grid” of the value combinations using grid_regular(). With 20 steps for the penalty and with 6 steps for the tokens, we’ll have 120 combinations to test in total. This took 2180s on my machine. final_grid &lt;- grid_regular( penalty(range = c(-4,0)), max_tokens(range = c(1e3, 6e3)), levels = c(penalty = 20, max_tokens = 6) ) final_grid %&gt;% head(10) ## # A tibble: 10 x 2 ## penalty max_tokens ## &lt;dbl&gt; &lt;int&gt; ## 1 0.0001 1000 ## 2 0.000162 1000 ## 3 0.000264 1000 ## 4 0.000428 1000 ## 5 0.000695 1000 ## 6 0.00113 1000 ## 7 0.00183 1000 ## 8 0.00298 1000 ## 9 0.00483 1000 ## 10 0.00785 1000 Next we train our models using the tuning grid: tic() final_rs &lt;- tune_grid( tune_wf, yelp_folds, grid = final_grid, metrics = metric_set(rmse, mae, mape) ) toc() ## 2063.39 sec elapsed Now we can plot each model’s performance for the different numbers of tokens and regularization penalties. We see the familiar dip-shaped graph we expect in lasso regularization but the dips are much more pronounced for larger token numbers, suggesting that regularization is much more important as we use more tokens. Also note that the best performance happens with an intermediate number of tokens: for some reason, model performace gets worse on this dataset if you use more than 3000 tokens. final_rs %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = as.factor(max_tokens))) + geom_line(size = 1.5, alpha = 0.5) + geom_point(size = 2, alpha = 0.9) + facet_wrap(~.metric, scales = &quot;free_y&quot;) + scale_x_log10() + labs( color = &quot;Number of tokens&quot;, title = &quot;Lasso model performance across regularization penalties and number of tokens&quot;, subtitle = &quot;The best model includes a high number of tokens but also significant regularization&quot; ) We can extract the lowest MAE value from our models: lowest_mae &lt;- final_rs %&gt;% select_best(&quot;mae&quot;) lowest_mae ## # A tibble: 1 x 3 ## penalty max_tokens .config ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.0127 3000 Recipe3_Model11 And then we can use this value to set a final workflow: final_wf &lt;- finalize_workflow( tune_wf, lowest_mae ) final_wf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_tokenize() ## * step_stopwords() ## * step_tokenfilter() ## * step_tfidf() ## ## -- Model ----------------------------------------------------------------------- ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.0127427498570313 ## mixture = 1 ## ## Computational engine: glmnet Which we can then use to do one last final fit and view its metrics: tic() final_fitted &lt;- last_fit(final_wf, yelp_split) toc() ## 21.69 sec elapsed collect_metrics(final_fitted) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.845 ## 2 rsq standard 0.406 This plot uses the vip package to extract the most important positive and negative terms, so we can see what our lasso regression is picking up on. Overall, the terms look kind of random. I would have expected words like “delicious,” “great,” and “awesome” to have been strongly correlated with positive reviews, and so I’m not what to make of the fact that “talked,” “sounded,” and “dipped” are the top three most-associated-with-positive-review words. The negative words look a bit better–“unfortunate” is #1 and “worst” is #3–but there are still some head-scratchers, like “2.50” and “striploin.” (Although if you spend $2.50 on a striploin you have no one to blame but yourself.) library(vip) scotus_imp &lt;- pull_workflow_fit(final_fitted$.workflow[[1]]) %&gt;% vi(lambda = lowest_mae$penalty) scotus_imp %&gt;% mutate( Sign = case_when( Sign == &quot;POS&quot; ~ &quot;Better&quot;, Sign == &quot;NEG&quot; ~ &quot;Worse&quot;, ), Importance = abs(Importance), Variable = str_remove_all(Variable, &quot;tfidf_text_&quot;) ) %&gt;% group_by(Sign) %&gt;% top_n(20, Importance) %&gt;% ungroup() %&gt;% ggplot(aes( x = Importance, y = fct_reorder(Variable, Importance), fill = Sign )) + geom_col(show.legend = FALSE) + scale_x_continuous(expand = c(0, 0)) + facet_wrap(~Sign, scales = &quot;free&quot;) + labs( y = NULL, title = &quot;Variable importance for predicting Yelp review star ratings&quot; ) Finally, we can again plot our final lasso model’s predicted ratings vs. the actual ratings to see how they compare. There is a definite improvement from the first model, but the results ultimately still aren’t workable. The range of predictions is still much too wide, and true lower reviews are still predicted as much too high. final_fitted %&gt;% collect_predictions() %&gt;% ggplot(aes(rating_num, .pred)) + geom_abline(lty = 2, color = &quot;gray80&quot;, size = 1.5) + geom_point(alpha = 0.3) + labs( x = &quot;Truth&quot;, y = &quot;Predicted year&quot;, title = &quot;Predicted and true ratings for Yelp Reviews&quot; ) 5.7 Conclusion In this section I followed Silge and Hvitfeldt (2020) ‘s recipe and tried to predict a Yelp review’s star rating from its text using a lasso regression model. I varied a number of parameters, including the lasso regularization penalty, the number of tokens used in the model, the number and type of n-grams, and the lengths of the reviews. Although the models’ accuracy did improve as I refined them, none of the models were especially effective and none come close to being workable in practice. There are at least two possibilities: The problem might be with the dataset. The dataset may be too small, or too imbalanced (there are far fewer negative reviews than positive reviews), or have some other deficiency that makes it unsuitable for lasso regression. Linear regression may not be the right tool for the job. Given the relatively small number of discrete rating categories, this might be better modeled as a classification problem. We will look at both of these possibilities in subsequent entries. References "],
["yelp-classification-and-sentiment-test.html", "Chapter 6 Yelp Classification and Sentiment Test 6.1 Yelp Dataset 6.2 Kaggle Yelp dataset 6.3 NEXT STEPS 6.4 SessionInfo", " Chapter 6 Yelp Classification and Sentiment Test This notebook outlines my efforts to build a classification model that can predict Yelp star ratings based on Yelp review text. My previous attempts used linear regression to predict star rating as a real-valued function of input text. In this notebook, I will instead approach prediction as a classification problem and try to predict star ratings as discrete factors. Intead of trying to predict exact star ratings, I will follow standard practice and divide ratings into positive (“POS”) and negative (“NEG”) reviews. As Liu (2015) notes, “Sentiment classification is usually formulated as a two-class classification problem: positive and negative …A review with 4 or 5 stars is considered a positive review, and a review with 1 to 2 stars is considered a negative review. Most research papers do not use the neutral class (3-star ratings) to make the classification problem easier” (49). But if the results are good, we can always experiment with three- or five-class problems. A note on sourcing: My analysis here will closely follow the examples in Silge and Hvitfeldt (2020) (which I will often refer to as “SMLTAR,” for “Supervised Machine Learning and Text Analysis in R”) and Silge and Robinson (2020). In some cases I have used examples or hints from websites like Stack Overflow, and I’ve noted that where applicable. A note on aesthetics: in the interest of time I haven’t piped my outputs through kable(). Most outputs are straight console printouts. 6.1 Yelp Dataset Let’s begin with the Yelp dataset I collected. As a reminder, this dataset was collected in October 2020 and has 9,402 reviews for restaurants in Ottawa. Reviews were overwhelmingly positive, as can be seen in the following histogram. reviews_gr &lt;- read_csv(&quot;../tests/data/goodreads_all.csv&quot;) reviews_mec &lt;- read_csv(&quot;../tests/data/mec-reviews.csv&quot;) reviews_yelp &lt;- read_csv(&quot;../tests/data/ottawa_yelp_reviews.csv&quot;) %&gt;% rename(rating_num = rating) reviews_yelp %&gt;% ggplot(aes(x=rating_num)) + geom_histogram(bins=5) + labs(title = &quot;Small Yelp Dataset: Histogram of Star Ratings (n=9,402)&quot;, x = &quot;Star Rating&quot;, y = &quot;Count&quot;) The dataset is quite imbalanced: nearly 79% of reviews give 4 or 5 stars, our only about 9% give 1 or 2 stars. As we will see, this will create problems for our modeling. reviews_yelp %&gt;% group_by(rating_num) %&gt;% summarise(n = n()) %&gt;% mutate(pct = n/sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## rating_num n pct ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 343 0.0365 ## 2 2 510 0.0542 ## 3 3 1077 0.115 ## 4 4 2958 0.315 ## 5 5 4514 0.480 6.1.1 AFINN AFINN is a dictionary-based one-dimensional sentiment model that gives texts an integer score for how positive or negative they are. It treats texts as a “bag of words,” which means it does not consider any syntax or semantics beyond the values given in its dictionary. Each word in a text is given a pre-determined positive or negative score, and those scores are summed to give an overall rating for a text. For example, here are the AFINN scores for the top 5 positive words. Strongly negative words are generally NSFW and so I won’t print them here. afinn %&gt;% arrange(desc(value)) %&gt;% head(5) ## # A tibble: 5 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 breathtaking 5 ## 2 hurrah 5 ## 3 outstanding 5 ## 4 superb 5 ## 5 thrilled 5 Following the Tidytext method from Silge &amp; Robinson, we get an AFINN score for each Yelp review: afinn_yelp &lt;- reviews_yelp %&gt;% select(comment, rating_num) %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, comment) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T), rating_num = mean(rating_num)) We can make a boxplot to show the distribution of AFINN scores for reviews grouped by star rating. This actually looks moderately promising, since we can see that higher star ratings seem to be associated with somewhat higher AFINN scores. afinn_yelp %&gt;% mutate(rating_num = as.factor(rating_num)) %&gt;% ggplot(aes(x = rating_num, y=afinn_sent)) + geom_boxplot() + geom_smooth(method=&quot;lm&quot;) + labs( title = &quot;AFINN Scores by Star Rating&quot;, subtitle = &quot;Small Yelp dataset (n=9402)&quot;, x = &quot;Star Rating&quot;, y = &quot;AFINN Sentiment Score&quot; ) 6.1.2 Classification: Naive Bayes Classifier To approach this as classification problem, we will divide reviews into two groups: positive (&gt;3 stars) and negative (&lt;3 stars). factor_yelp &lt;- reviews_yelp %&gt;% bind_cols(afinn_yelp %&gt;% select(afinn_sent)) %&gt;% filter(rating_num != 3) %&gt;% mutate(rating_factor = case_when( rating_num &lt;3 ~ &quot;NEG&quot;, rating_num &gt;3 ~ &quot;POS&quot;), rating_factor = as.factor(rating_factor)) #factor_yelp Here we’ll follow SMLTAR Ch 7 very closely and set up a naive Bayes classifier that takes AFINN sentiment as its only input and predicts positive or negative sentiment as its only output. The code here follows SMLTAR very closely except where otherwise specified. Note that SMLTAR actually uses the text itself, and not a real-valued variable like AFINN sentiment; we can try this next. First we set up testing and training split: set.seed(1234) yelp_split &lt;- initial_split(factor_yelp, strata = rating_factor) yelp_test &lt;- testing(yelp_split) yelp_train &lt;- training(yelp_split) Then we set up a recipe, set up a workflow, specify a naive Bayes model, and fit this model to our training data: yelp_rec &lt;- recipe(rating_factor ~ afinn_sent, data = yelp_train) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) nb_spec &lt;- naive_Bayes() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;naivebayes&quot;) nb_fit &lt;- yelp_wf %&gt;% add_model(nb_spec) %&gt;% fit(data = yelp_train) nb_fit ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: naive_Bayes() ## ## -- Preprocessor ---------------------------------------------------------------- ## 0 Recipe Steps ## ## -- Model ----------------------------------------------------------------------- ## ## ======================================================================================== Naive Bayes ========================================================================================= ## ## Call: ## naive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE) ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Laplace smoothing: 0 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## A priori probabilities: ## ## NEG POS ## 0.1024984 0.8975016 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Tables: ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::NEG (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (640 obs.); Bandwidth &#39;bw&#39; = 1.66 ## ## x y ## Min. :-32.98 Min. :0.0000012 ## 1st Qu.:-11.74 1st Qu.:0.0003679 ## Median : 9.50 Median :0.0035701 ## Mean : 9.50 Mean :0.0117586 ## 3rd Qu.: 30.74 3rd Qu.:0.0137708 ## Max. : 51.98 Max. :0.0584066 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::POS (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (5604 obs.); Bandwidth &#39;bw&#39; = 1.195 ## ## x y ## Min. :-11.59 Min. :7.100e-07 ## 1st Qu.: 17.96 1st Qu.:1.262e-04 ## ## ... ## and 7 more lines. We will use resampling to evaluate the model, again with 10 cross-fold validation sets. yelp_folds &lt;- vfold_cv(yelp_train) nb_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) %&gt;% add_model(nb_spec) nb_rs &lt;- fit_resamples( nb_wf, yelp_folds, control = control_resamples(save_pred = TRUE) ) nb_rs_metrics &lt;- collect_metrics(nb_rs) nb_rs_predictions &lt;- collect_predictions(nb_rs) Let’s see the fit metrics: nb_rs_metrics ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.914 10 0.00283 ## 2 roc_auc binary 0.784 10 0.0121 We can also plot an ROC curve, which is supposed to show a model’s accuracy and how well a model trades off false positives and false negatives. Better models are associated with curves that bend farther away from the line y=x (citation needed). According to the standard story about ROC curves, this looks okay. nb_rs_predictions %&gt;% group_by(id) %&gt;% roc_curve(truth = rating_factor, .pred_NEG) %&gt;% autoplot() + labs( color = NULL, title = &quot;Receiver operator curve for small Yelp dataset&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) We can also look at a heat map and a confusion matrix to see how often the model was correct and incorrect. nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) ## Truth ## Prediction NEG POS ## NEG 18 2 ## POS 42 563 But looking at the confusion matrix shows a problem: there are so many fewer true NEG cases that our model’s performance doesn’t mean much. The Bayes classifier achieved ~91.4% accuracy, but since ~89.7% of the data is classified as POS we could get nearly as much accuracy by just guessing “POS” in each case. The data is heavily unbalanced. factor_yelp %&gt;% group_by(rating_factor) %&gt;% summarise(n = n()) %&gt;% mutate(pct = n/sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## rating_factor n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NEG 853 0.102 ## 2 POS 7472 0.898 We need to balance our dataset so that there is a roughly equal number of positive and negative reviews. The easiest way is by downsampling, where you remove items from the larger set until you have two sets of about the same size. But to get a balanced dataset we would need to throw away nearly 80% of our data, and since our dataset is somewhat small we might not have enough to work with. TODO cite SMLTAR or Text Mining with R. There are more sophisticated balancing approaches that are out of scope here, but the easiest approach for our puposes is to find a much larger public dataset to work with. 6.2 Kaggle Yelp dataset Yelp makes a huge dataset available for teaching and research at this link through Kaggle. A larger dataset will probably help us build a better model, especially if we need to balance our datasets to have roughly equal numbers of positive and negative reviews. The dataset is enormous: it has around 6 gigabytes of review text and around 5 million reviews. This is too big to load using conventional methods on my machine. After a few failures, I found a discussion on StackOverflow that helped me read just the first n lines from the jsonLine file and parse them. For the present, we’ll read the first 100k reviews: # figure out how to do it reading between the lines of this stackoverflow: # https://stackoverflow.com/questions/53277351/read-first-1000-lines-from-very-big-json-lines-file-r yelp_big &lt;- readLines(&quot;../tests/data/yelp_academic_dataset_review.json&quot;, n = 100000) %&gt;% textConnection() %&gt;% jsonlite::stream_in(verbose=FALSE) yelp_big &lt;- yelp_big %&gt;% select(stars, text) And plot a histogram of the star distributions. The star distributions look very similar to the data I collected manually, but with a slight spike at 1 that we didn’t find in my Yelp data. We did find this 1-spike in the MEC data, so there may be a common review phenomenon here. yelp_big %&gt;% ggplot(aes(x=stars)) + geom_histogram(bins=5) + labs(title = &quot;Large Yelp Dataset: Histogram of Star Ratings (n=100,000)&quot;) Let’s classify the reviews into NEG and POS again, once more classifying reviews with fewer than 3 stars as negative, more than 3 stars as positive, and discarding reviews with 3 stars. yelp_big_factor &lt;- yelp_big %&gt;% mutate(rating_factor = case_when( stars &lt; 3 ~ &quot;NEG&quot;, stars &gt; 3 ~ &quot;POS&quot;) %&gt;% as.factor() ) %&gt;% select(-stars) %&gt;% drop_na() yelp_big_factor %&gt;% summary() ## text rating_factor ## Length:88821 NEG:21928 ## Class :character POS:66893 ## Mode :character This dataset is quite imbalanced: there are ~67k positive reviews and ~22 negative reviews. Since classification engines can have trouble with unbalanced sets, we will downsample our dataset by randomly removing some positive reviews so that we have around the same number of negatvie and positive reviews. This new balanced dataset will have ~22k positive and negative reviews, still far more than we had in the dataset I collected myself. set.seed(1234) yelp_balanced &lt;- yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% bind_rows(yelp_big_factor%&gt;% filter(rating_factor == &quot;POS&quot;) %&gt;% slice_sample(n=yelp_big_factor %&gt;% filter(rating_factor == &quot;NEG&quot;) %&gt;% nrow() )) yelp_balanced %&gt;% summary() ## text rating_factor ## Length:43856 NEG:21928 ## Class :character POS:21928 ## Mode :character Let’s try AFINN again on the balanced set. First we’ll get the AFINN sentiments for all our reviews. tic() afinn_yelp_big &lt;- yelp_balanced %&gt;% rowid_to_column() %&gt;% tidytext::unnest_tokens(word, text) %&gt;% left_join(afinn) %&gt;% group_by(rowid) %&gt;% summarise(afinn_sent = sum(value, na.rm = T)) toc() ## 9.7 sec elapsed yelp_big_bal_afinn &lt;- afinn_yelp_big %&gt;% left_join(yelp_balanced %&gt;% rowid_to_column()) %&gt;% select(-rowid) And we can make a boxplot of the AFINN distributions for POS and NEG reviews. There is enough difference between the POS and NEG reviews that this looks like it might plausibly work. yelp_big_bal_afinn %&gt;% ggplot(aes(x=rating_factor,y=afinn_sent)) + geom_boxplot() + labs( title = &quot;AFINN Scores by Star Rating&quot;, subtitle = paste0(&quot;Big Yelp dataset (n=&quot;,nrow(yelp_big_bal_afinn),&quot;)&quot;), x = &quot;Star Rating&quot;, y = &quot;AFINN Sentiment Score&quot; ) And for another view, here’s a density plot: yelp_big_bal_afinn %&gt;% ggplot(aes(x=afinn_sent, fill=rating_factor)) + geom_density(alpha=0.5) + labs(title = &quot;Density Distributions of AFINN Sentiment for POS and NEG Reviews&quot;, subtitle = &quot;Large Balanced Yelp Dataset, n=43,855&quot;, x = &quot;AFINN Sentiment&quot;, y =&quot;Density&quot;) 6.2.1 Naive Bayes Classifier We will again go through the tidymodels process of setting up a naive Bayes classifier. First we do a test/train split of our large balanced dataset. set.seed(1234) yelp_split &lt;- initial_split(yelp_big_bal_afinn, strata = rating_factor) yelp_test &lt;- testing(yelp_split) yelp_train &lt;- training(yelp_split) Then we set up a recipe, a naive Bayes model, and a workflow, and then fit our model to our training data. yelp_rec &lt;- recipe(rating_factor ~ afinn_sent, data = yelp_train) yelp_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) nb_spec &lt;- naive_Bayes() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;naivebayes&quot;) nb_fit &lt;- yelp_wf %&gt;% add_model(nb_spec) %&gt;% fit(data = yelp_train) nb_fit ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: naive_Bayes() ## ## -- Preprocessor ---------------------------------------------------------------- ## 0 Recipe Steps ## ## -- Model ----------------------------------------------------------------------- ## ## ======================================================================================== Naive Bayes ========================================================================================= ## ## Call: ## naive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE) ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Laplace smoothing: 0 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## A priori probabilities: ## ## NEG POS ## 0.5 0.5 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Tables: ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::NEG (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (16446 obs.); Bandwidth &#39;bw&#39; = 0.7709 ## ## x y ## Min. :-64.31 Min. :0.000e+00 ## 1st Qu.:-29.41 1st Qu.:2.549e-05 ## Median : 5.50 Median :2.295e-04 ## Mean : 5.50 Mean :7.155e-03 ## 3rd Qu.: 40.41 3rd Qu.:3.387e-03 ## Max. : 75.31 Max. :7.088e-02 ## ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ::: afinn_sent::POS (KDE) ## ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## ## Call: ## density.default(x = x, na.rm = TRUE) ## ## Data: x (16446 obs.); Bandwidth &#39;bw&#39; = 0.9637 ## ## x y ## Min. :-33.891 Min. :0.0000000 ## 1st Qu.: 2.054 1st Qu.:0.0000199 ## ## ... ## and 7 more lines. Then we use resampling to evaluate the model, again with 10 cross-fold validation sets. yelp_folds &lt;- vfold_cv(yelp_train) nb_wf &lt;- workflow() %&gt;% add_recipe(yelp_rec) %&gt;% add_model(nb_spec) nb_rs &lt;- fit_resamples( nb_wf, yelp_folds, control = control_resamples(save_pred = TRUE) ) nb_rs_metrics &lt;- collect_metrics(nb_rs) nb_rs_predictions &lt;- collect_predictions(nb_rs) Let’s see the fit metrics. Our accuracy is ~78.7%, which is quite a bit better than chance so there is good evidence that the model is getting something right. # create a character a vector with the accuracy % that we can use in the text later nb_acc &lt;- nb_rs_metrics %&gt;% pull(mean) %&gt;% head(1) %&gt;% round(3) %&gt;% `*`(100) %&gt;% paste0(&quot;%&quot;,.) # print out the metrics nb_rs_metrics ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.787 10 0.00188 ## 2 roc_auc binary 0.854 10 0.00167 We can also look at the ROC curve, which again shows some good performance: nb_rs_predictions %&gt;% group_by(id) %&gt;% roc_curve(truth = rating_factor, .pred_NEG) %&gt;% autoplot() + labs( color = NULL, title = &quot;Receiver operator curve for big balanced Yelp dataset, AFINN sentiment&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) And a confusion matrix: nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) nb_rs_predictions %&gt;% filter(id == &quot;Fold01&quot;) %&gt;% conf_mat(rating_factor, .pred_class) ## Truth ## Prediction NEG POS ## NEG 1195 230 ## POS 448 1417 Our naive Bayes classifier did quite a bit better than chance on our balanced dataset. We would have expected about 50% accuracy by chance, and it was accurate %78.7 of the time on our training data. 6.2.2 Logistic Regression It’s also worth trying a logistic regression, for at least two reasons: It’s good practice; and It’s a simple and common model. For the code here, I referred to this website to remind me of the basics of doing logistic regression in R. I elected not to do it in a tidymodels framework. We’ll use the same big balanced dataset. First we’ll split our data into testing and training: index &lt;- sample(c(T,F), size = nrow(yelp_big_bal_afinn), replace = T, prob=c(0.75,0.25)) train &lt;- yelp_big_bal_afinn[index,] test &lt;- yelp_big_bal_afinn[!index,] Then we’ll use glm() to run a simple logistic regression, predicting the rating factor based on the AFINN sentiment score. Here is the model output: logit &lt;- glm(data= train, formula= rating_factor ~ afinn_sent, family=&quot;binomial&quot;) summary(logit) ## ## Call: ## glm(formula = rating_factor ~ afinn_sent, family = &quot;binomial&quot;, ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6936 -0.7363 0.0076 0.8166 3.7598 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.166717 0.018845 -61.91 &lt;2e-16 *** ## afinn_sent 0.190342 0.002265 84.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45641 on 32922 degrees of freedom ## Residual deviance: 32490 on 32921 degrees of freedom ## AIC: 32494 ## ## Number of Fisher Scoring iterations: 5 Our results are strongly significant, so we have some reason to take this model seriously. Referring to this website for more pointers, we can use our logistic regression results to predict rating scores for our test dataset. The simplest way to do this is to say that we predict whichever outcome the model says is more likely. In other words, if a review has a predicted probability &gt;0.5 of being positive, then we predict it’s positive. How accurate would we be? pred &lt;- predict(logit, newdata = test, type=&quot;response&quot;) test_results &lt;- test %&gt;% bind_cols(tibble(pred = pred)) %&gt;% mutate(pred = if_else(pred &gt; 0.5, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% mutate(correct = if_else (pred == rating_factor, T, F)) %&gt;% summarise(accuracy = sum(correct) / nrow(.)) logit_acc &lt;- test_results %&gt;% `*`(100) %&gt;% round(3) %&gt;% paste0(&quot;%&quot;,.) For this data, a simple logistic regression was only a little bit less accurate than the naive Bayes classifier: %76.848, as opposed to %78.7. 6.3 NEXT STEPS Consider another sentiment-detection algorithm / dictionary. Naive Bayes classifier based on review text, intead of AFINN sentiment score. Consider review length as a tuning paramter. 6.4 SessionInfo sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] glmnet_4.0-2 Matrix_1.2-18 discrim_0.1.1 vip_0.2.2 tictoc_1.0 textrecipes_0.3.0 lubridate_1.7.9 yardstick_0.0.7 workflows_0.2.0 tune_0.1.1 ## [11] rsample_0.0.8 recipes_0.1.13 parsnip_0.1.4 modeldata_0.0.2 infer_0.5.3 dials_0.0.9 scales_1.1.1 broom_0.7.0 tidymodels_0.1.1 tidytext_0.2.5 ## [21] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.1 class_7.3-17 fs_1.5.0 rstudioapi_0.11 listenv_0.8.0 furrr_0.1.0 farver_2.0.3 SnowballC_0.7.0 ## [10] prodlim_2019.11.13 fansi_0.4.1 xml2_1.3.2 codetools_0.2-16 splines_4.0.2 knitr_1.29 jsonlite_1.7.0 pROC_1.16.2 packrat_0.5.0 ## [19] servr_0.18 dbplyr_1.4.4 compiler_4.0.2 httr_1.4.2 backports_1.1.7 assertthat_0.2.1 cli_2.0.2 later_1.1.0.1 htmltools_0.5.0 ## [28] tools_4.0.2 gtable_0.3.0 glue_1.4.1 naivebayes_0.9.7 rappdirs_0.3.1 Rcpp_1.0.5 cellranger_1.1.0 DiceDesign_1.8-1 vctrs_0.3.2 ## [37] iterators_1.0.12 timeDate_3043.102 gower_0.2.2 xfun_0.16 stopwords_2.0 globals_0.13.0 rvest_0.3.6 mime_0.9 lifecycle_0.2.0 ## [46] future_1.19.1 MASS_7.3-51.6 ipred_0.9-9 promises_1.1.1 hms_0.5.3 parallel_4.0.2 yaml_2.2.1 gridExtra_2.3 rpart_4.1-15 ## [55] stringi_1.4.6 highr_0.8 tokenizers_0.2.1 foreach_1.5.0 textdata_0.4.1 lhs_1.0.2 hardhat_0.1.4 shape_1.4.5 lava_1.6.8 ## [64] rlang_0.4.7 pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-41 labeling_0.3 tidyselect_1.1.0 plyr_1.8.6 magrittr_1.5 bookdown_0.20 ## [73] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.6 haven_2.3.1 withr_2.2.0 survival_3.1-12 nnet_7.3-14 janeaustenr_0.1.5 ## [82] modelr_0.1.8 crayon_1.3.4 utf8_1.1.4 rmarkdown_2.3 usethis_1.6.1 grid_4.0.2 readxl_1.3.1 blob_1.2.1 reprex_0.3.0 ## [91] digest_0.6.25 webshot_0.5.2 httpuv_1.5.4 munsell_0.5.0 GPfit_1.0-8 viridisLite_0.3.0 kableExtra_1.1.0 References "],
["references.html", "References", " References "]
]
